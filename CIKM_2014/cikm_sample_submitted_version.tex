%May 28 2014
%Updates on reviewers:
%Perhaps use term ``Mobius join''
% Main thing: show usefulness of having negated relationships.
%potential application: small contingency tables.
%complexity bounds: parametric complexity, measure operations over and above data accesses.

% Fixed the equation on page 3 to prevent line overflow. (AhmetSacan, Sep2012)
%Ideas for the future: separate computing time for counts from ct-table.
%Reorganize so we don't use the term "nodes" and introduce Bayes nets later.
%latest version with Anoop's comment, after submission, March 4th, 2014




%\documentclass{vldb}
\documentclass{acm_proc_article-sp}
\usepackage{array}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\usepackage{graphicx}
\input{preamble-stuff}
\newcommand{\ct}{\mathit{ct}}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)

\begin{document}

% ****************** TITLE ****************************************
%BayesBase: Managing Relational Database Schemata for Learning Graphical Models
%\title{BayesBase: Relational Learning with Relational Algebra}
%\title{BayesBase: Learning Bayes Nets with Relational Algebra: Need a new title}
%\title{On Computing Sufficient Statistics for Bayes Nets Learning with Relational Algebra}
\title{Computing Multi-Relational Sufficient Statistics for  Large Databases}
%\author{Zhensong Qian and Oliver Schulte \\
%\\ School of Computing Science\\ Simon Fraser University\\Vancouver-Burnaby, Canada }

%\author{
%Zhensong Qian, Oliver Schulte, Yan Sun\\
% Simon Fraser University, Canada\\
%\{zqian, oschulte, sunyans\}@sfu.ca
%}

\maketitle  

\begin{abstract} 
Utilizing the information in a relational database for statistical modelling and pattern mining requires fast access to multi-relational sufficient statistics, that combine information across database tables. 
Previously established techniques are available to compute multi-relational sufficient statistics for  conjunctive queries with positive relationships only. We present an efficient dynamic program that computes sufficient statistics for any combination of positive {\em and} negative relationships, starting with a set of statistics for positive relationships only.
Our dynamic program performs a virtual join operation, that counts the number of statistics in a table join without actually constructing the join. We show that the run time of the algorithm is $O(r \log r)$, where $r$ is the number of sufficient statistics to be computed.
The computed statistics are stored in contingency tables.
We introduce contingency table algebra, an extension of relational algebra, to elegantly describe and efficiently implement the dynamic program. 
Empirical evaluation on seven benchmark databases demonstrates the scalability of our algorithm; we compute sufficient statistics with positive and negative relationships in databases with over 1 million data records.  
Our experiments illustrate how access to sufficient statistics for both positive and negative relationships enhances feature selection, rule mining, and generative modelling.  
 \end{abstract}

%Fast access to sufficient statistics is a key factor for scalable data mining and machine learning. 
%Multi-relational database contain information about both the presence and the absence of relationships. 
% for both positive and negative relationships.
%In this paper we consider computing multi-relational statistics for conjunctive queries that specify the values of attributes, as well as the presence and/or {\em absence} of any number of relationships. 
%
%for computing relational sufficient statistics that combine information from multiple database tables. 
%As an application, we show how the precomputed sufficient statistics support learning a Bayes net model for the entire input database. 
%The scalability of Bayes net structure learning depends mainly on fast access to sufficient statistics; our algorithm is the first to accomplish structure learning for databases with over 1 million records.
%And we also discuss the possible sparse data structures to pruning our algorithm more efficient.

 
\section{Introduction} Relational databases contain information about attributes of entities, and which relationships do and do not hold among entities. To make this information accessible for knowledge discovery requires requires computing {\em sufficient statistics}. 
For relational statistical analysis to discover cross-table correlations, these sufficient statistics must be instantiation counts for conjunctive queries  that combine information from different database tables, and that may contain any number of {\em positive and negative} relationships. Negative relationships concern the nonexistence of a relationship. Such statistics are important for learning correlations between different relationship types (e.g., if user $u$ performs a web search for item $i$, is it likely that $u$ watches a video about $i$ ?). 

Whereas sufficient statistics with positive relationships only can be efficiently computed by SQL joins of existing database tables, a table join approach is not feasible for negative relationships. This is because we would have to enumerate all tuples of entities that are {\em not} related (consider the number of user pairs who are {\em not} friends on Facebook). The cost of the enumeration approach is close to materializing the Cartesian product of entity sets, which grows exponentially with the number of entity sets involved. It may therefore seem that sufficient statistics with negative relationships can be computed only for small databases. Indeed  such sufficient statistics have not been previously used in multi-relational data analysis, to our knowledge. We show that on the contrary, assuming that sufficient statistics with positive relationships are available, extending them to negative relationships can be achieved in a highly scalable manner, which does not depend on the size of the database.

\emph{Virtual Join Approach.} Our approach to this problem introduces a new virtual join operation. A virtual join algorithm computes sufficient statistics {\em without} materializing a cross product \cite{Yin2004}. Sufficient statistics can be represented in contingency tables \cite{Moore1998}. Our virtual join operation is a dynamic programming algorithm that successively builds up a large contingency table from smaller ones, {\em without a need to access the original data tables}.
%
%In many applications of machine learning to big datasets with discrete variables, the dominant computational cost is due to counting how many times a pattern is instantiated in the data. An especially common pattern is a conjuntive query.
%These counts are referred to as {\em sufficient statistics} because they summarize the information in the data that a machine learning algorithm requires \cite{Moore1998}. 
%%Machine learning researchers have developed several methods for computing sufficient statistics quickly. 
%One of the most effective approaches to gathering sufficient statistics is to precompute a cache  before executing the learning algorithm. This paper describes the first precounting approach for large {\em relational} datasets with sufficient statistics for queries 
%The precounting approach offers several advantages over on-demand counting. (1) Compute once, use often: Each sufficient statistic is computed only once, then looked up by the learning algorithm as required. Different learning algorithms for different tasks can access the same cache.
%(2) Dynamic programming: Computing a complete set of sufficient statistics at the same time facilitates the use of dynamic programming where  counts for more complex queries are built up from results for simpler queries. 
%
%We apply precounting to learning a Bayes net structure for an entire relational database, including cross-table dependencies. 
%Precounting makes it feasible to learn a Bayes net for tables with over 1 million records (drawn from the IMDB database). This is an order of magnitude larger than the databases for which graphical models  have been learned previously \cite{Schulte2012,Khot2011}.



%\emph{The Materialization Challenge for Relational Learning.}  
%In a single data table, sufficient statistics count how many table rows instantiate a query.
%A relational database contains multiple tables interrelated by foreign key constraints. 
%For relational learning to discover cross-table correlations, it must compute counts for cross-table queries whose instantiations do {\em not} define a subset of rows in any of the input tables. 
%Instead, they define a subset of rows in a {\em cross-product} of existing database tables. 
%%In terms of the Structured Query Language (SQL), the FROM clause defines a cross product of tables, and the WHERE clause defines the query condition. 
%%In nonrelational learning, the FROM clause refers only to the single input table, whereas in relational learning, the clause can list any number of tables in the database. 
%In principle, cross-table query counts can be computed by constructing or {\em materializing} the cross product and then applying single-table computations. 
%In practice this is often infeasible because the size of the cross product grows exponentially. We refer to this fact as the {\em materialization challenge}. 
%
%\emph{Virtual Join Approach.} 
%An algorithm for computing sufficient counts without materializing a cross product is called a Virtual Join \cite{Yin2004}. 
%We present the first Virtual Join algorithm that finds sufficient statistics that involve any number of negative relationships. 
%A materialization approach for negative relationships does not scale because
%% infeasible for realistic databases as 
%the unconstrained cross product of two or more different entity sets is far too large.
% (e.g., consider the number of $(u,i)$ pairs such that user $u$ has {\em not} watched a movie about $i$).
%We provide a subtraction method that  computes the sufficient statistics for any combination of positive and negative relationships. 

We introduce an extension of relational algebra with operations on contingency tables that generalize standard relational algebra operators. 
We establish a contingency table algebraic identity that reduces the computation of sufficient statistics with $k+1$ negative relationships to the computation of sufficient statistics with only $k$ negative relationships. 
A dynamic programming algorithm applies the identity to construct contingency tables that involve $1,2,\ldots,\ell$ relationships (positive and negative), until we obtain a joint contingency table for all tables in the database. We provide a theoretical upper bound of $O(r \log r)$ for the number of contingency table operations required by the algorithm, where $r$ is the number of sufficient statistics involving negative relationships. In other words, the number of table operations is nearly linear in the size of the required output. 

\emph{Evaluation.} We evaluate our Virtual Join algorithm by computing contingency tables for seven real-world databases. The computation times exhibit the near-linear growth predicted by our theoretical analysis. 
They range from two seconds on the simpler database schemas to just over two hours for the most complex schema with over 1 million tuples from the IMDB database.
%Once the sufficient statistics have been computed, learning a Bayes net model that represents correlations across the entire database is relatively fast, and takes less time than computing the sufficient statistics. 

Given that computing sufficient statistics for negative relationships is {\em feasible}, the remainder of our experiments evaluate their {\em usefulness}. These sufficient statistics allow statistical analysis to utilize the absence or presence of a relationship as a feature. 
Our benchmark datasets provide evidence that the relationship indicator features enhance different types of statistical analysis, as follows. (1) Feature selection: We learn two different feature sets for each of our databases and a given target class label. A standard feature selection method selects different features for classification when provided with statistics for negative and positive relationships, from those that it selects when given positive relationship statistics only. (2) Association Rule Mining: A standard association rule learning method includes many association rules with relationship conditions in its top 20 list. 
%On all data sets, relationship conditions are included in the majority of rules. 
(3) Bayesian network learning. A Bayesian network provides a graphical summary of the probabilistic dependencies among relationships and attributes in a database. We learn two Bayesian network structures on each of our database, one whose input is a contingency table with only positive relationships, one whose input is a contingency table with both positive and negative relationships. Our largest database is an order of magnitude larger than the databases for which graphical models  have been learned previously \cite{Schulte2012}. The two Bayes net structures learned are different for all but one database, and make different trade-offs between model likelihood and model complexity. On the two datasets with the most complex schemas, enhanced sufficient statistics lead to a clearly superior model (better data fit with fewer parameters).

\emph{Contributions.} Our main contributions 
%may be summarized 
are as follows.
\begin{enumerate}
\item A dynamic program to compute a joint contingency table for sufficient statistics that combine several tables, and that may involve any number of {\em positive and negative }relationships.
\item An extension of relational algebra for contingency tables that supports the dynamic program conceptually and computationally.
%\item An application to learning a Bayes net structure representing cross-table dependencies across the entire database.
%\item Empirical studies for several machine learning tasks of the entire database enhanced by sufficient statistics.
\end{enumerate}

\emph{Paper Organization.} 
We review background for relational databases and statistical concepts. 
%One of the inputs to the Virtual Join algorithm is a set of random variables for which sufficient statistics are required. 
%We discuss how a default set of random variables can be generated using the schema information in the system catalog. 
The main part of the paper describes the dynamic programming algorithm for computing a joint contingency table for all random variables. 
We describe the contingency table algebra for dealing with negative relationships. 
A complexity analysis establishes feasible upper bounds on the number of contingency table operations required by the Virtual Join algorithm. 
We also investigate the scalability of the algorithm empirically. 
The final set of experiments examines how the cached sufficient statistics support the analysis of cross-table dependencies for different learning and data mining tasks.


%As an application, Tuffy supports parameter learning for a Markov Logic network.
 
\section{Background and Notation}

%\subsection{Relational Databases }
% \begin{table}[tbp] \centering
%\begin{tabular}
%[c]{|l|}\hline\\
%$\student$(\underline{$student\_id$}, $\intelligence$, $ranking$)\\
%$\course$(\underline{$course\_id$}, $\diff$, $rating$)\\ 
%$\prof$ (\underline{$professor\_id$}, $teaching\_ability$, $popularity$)\\
%$\reg$ (\underline{$student\_id$, $course\_id$}, $grade$, $satisfaction$)\\
%$\ra$ (\underline{$student\_id$, $prof\_id$}, $salary$, $capability$)\\
%$\it{Teaches} $(\underline{$professor\_id$, $course\_id$})\\
%\\
%\hline
%\end{tabular}
%\caption{Replace by a ER Diagram: A relational schema for a university domain. Key fields are underlined. An instance for this schema is given in Figure \ref{fig:university-tables}.
%\label{table:university-schema}} 
%\end{table}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
 \centering
\resizebox{0.4\textwidth}{!}{
 \includegraphics[width=0.5\textwidth]{figures/university-schema.png}
% \includegraphics[width=0.5\textwidth]{figures/university-tables.png}  
} 
\caption{A relational ER Design.}
 %for a university domain.}
 \label{fig:university-schema}
\end{figure}
 We assume a standard \textbf{relational schema} containing a set of tables, each with key fields, %typically
descriptive attributes, and possibly foreign key pointers. 
A \textbf{database instance} specifies the tuples contained in the tables of a given database schema. 
We assume that tables in the relational schema can be divided into {\em entity tables} and {\em relationship tables.} 
This is the case whenever a relational schema is derived from an entity-relationship model (ER model) \cite[Ch.2.2]{Ullman1982}. A \textbf{table join} of two or more tables contains the rows in the Cartesian products of the tables whose values match on common fields.

%The functor formalism is rich enough to represent the constraints of an ER schema by the following translation: Entity sets correspond to types, descriptive attributes to functions, relationship tables to predicates, and foreign key constraints to type constraints on the arguments of relationship predicates.  
%Assuming an ER design, a relational structure can be visualized as a complex network \cite[Ch.8.2.1]{Russell2010}: individuals are nodes, attributes of individuals are node labels, relationships correspond to (hyper)edges, and attributes of relationships are edge labels. Conversely, a complex  network can be represented using a relational database schema.

\subsection{Relational Random Variables} \label{sec:variables}
We adopt function-based notation from logic for combining statistical and relational concepts \cite{Russell2010}.
A domain or \textbf{population} is a set of individuals.
Individuals are denoted by lower case expressions (e.g., $\it{bob}$). 
%A \textbf{first-order variable} is capitalized. 
A \textbf{functor} represents a mapping
$\functor: \population_{1},\ldots,\population_{a} \rightarrow \outdomain_{\functor}$
where $\functor$ is the name of the functor, each $\population_{i}$ is a population, and $\outdomain_{\functor}$ is the output type or \textbf{range} of the functor. 
In this paper we consider only functors with a finite range, disjoint from all populations.  If $\outdomain_{\functor} = \{\true,\false\}$, the functor $\functor$ is a (Boolean) \textbf{predicate}. A predicate with more than one argument is called a \textbf{relationship}; other functors are called \textbf{attributes}. We use uppercase for predicates and lowercase for other functors. Throughout this paper we assume that all relationships are binary, though this is not essential for our algorithm.
%the populations associated with 
 %the variables are of the appropriate type for the functor.

A \textbf{(Parametrized) random variable} (PRV) is of the form $\functor(\X_{1},\ldots,\X_{a})$, where each $\X_{i}$ is a first-order variable \cite{Poole2003}. 
Each first-order variable is associated with a population/type. 
%We refer to PRVs ususally just as random variables.
%We refer to the first-order variables as \textbf{first-order variables} to distinguish them from the parametrized random variables. %that appear in a Bayes net model. 
%Figure \ref{fig:university-tables} displays a small database instance for this schema.% together with a Parametrized Bayes Net (only considering the $\ra$ relationship for simplicity.) 
%To keep the schema simple, we introduce only a limited number of attributes for each entity class.  
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
 \centering
\resizebox{0.45\textwidth}{!}{
 \includegraphics[width=0.5\textwidth]{figures/university-tables.png} 
}
\caption{Database Instance based on Figure~\ref{fig:university-schema}.%: (a) $\student$, (b) $\prof$, (c) $\ra$.  (d) A sample Parametrized Bayes Net based on the university schema.  % The $\cttable(\reg)$ table  is the contingency table for $\reg$ which we will introduce in Section~\ref {sec:mobius}. (e) 
}
 \label{fig:university-tables}
\end{figure}


The functor formalism is rich enough to represent the constraints of an entity-relationship schema via the following translation: Entity sets correspond to populations, descriptive attributes to functions, relationship tables to relationships, and foreign key constraints to type constraints on the arguments of relationship predicates. Table~\ref{table:translation} illustrates this translation, distinguishing attributes of entities ($\eatts$) and attributes of relationships ($\ratts$). 

\begin{table}[btp] \centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}[c]
{|l|c|l|l|}\hline
  \begin{tabular}{l}ER \\Diagram \end{tabular}&Type & Functor &Random Variable \\\hline
   % \begin{tabular}{l}Entity \\Tables\end{tabular}&\begin{tabular}{l}Population \\Variables \end{tabular} & Student, Course & S, C \\\hline
    \begin{tabular}{l}Relation \\Tables \end{tabular}&RVars &RA & $RA(\P,\S)$ \\\hline
   \begin{tabular}{l}Entity \\Attributes \end{tabular}&$\eatts$ & intelligence, ranking &\begin{tabular}{l} 
  $\{intelligence(\S), ranking(\S)\}$  \\$=\eatts(\S) $\end{tabular} \\\hline
  \begin{tabular}{l} Relationship \\Attributes \end{tabular}&$\ratts$ & teaching-ability, salary &\begin{tabular}{l}   $\{teaching-ability(\P,\S), salary(\P,\S)\} $ \\= $\ratts(RA(\P,\S))$\end{tabular}\\\hline
   
\end{tabular}
}
 % end scalebox
\caption{Translation from ER Diagram to %Parametrized 
Random Variables. 
%Population variables are derived from entity tables as described in the text.
 \label{table:translation}}
\end{table}

\subsection{Contingency Tables}
%Our starting point is the observation that a statistical learning algorithm like a Bayes net learner does not require an enumeration of individuals tuples, but only {\em sufficient statistics} \cite{Heckerman1995,Schulte2011}. 
%It is well-known in machine learning that a statistical learning algorithm like a Bayes net learner does not require an enumeration of individuals tuples, but only {\em sufficient statistics} \cite{Heckerman1995,Schulte2011}. 
Sufficient statistics can be represented in {\em contingency tables} as follows \cite{Moore1998}. 
%
 %A contingency table is defined as follows.
%The Bayes net is learned from the contingency table. 
%
%The count column in the $\ct$-table represents the number of instantiations for a given tuples of values in a row in the input database. 
%
Consider a fixed list of  random variables.
%$\R_{1}, R_{2},\ldots,R_{m}$, and attribute nodes $\functor_{1},\ldots,\functor_{j}$. 
A \textbf{query} is a set of $(variable = value)$ pairs where each value is of a valid type for the random variable. 
The \textbf{result set} of a query in a database $\D$ is the set of instantiations of the first-order variables such that the query evaluates as true in $\D$.
For example, in the database of Figure~\ref{fig:university-tables} the result set for the query 
%$(\it{intelligence}(\S) = 2$, $\it{rank}(\S) = 1$, $\it{rating}(\C) = 3$, $\it{diff}(\C) = 1$, $\reg(\S,\C) = F)$
$(\it{intelligence}(\S) = 2$, $\it{rank}(\S) = 1$, $\it{popularity}(\P) = 3$, $\it{teaching-ability}(\P) = 1$, $\ra(\P,\S) = T)$ is the singleton $\{\langle \it{kim}, \it{oliver}\rangle\}$. 
% $\{\langle \it{kim}, \it{101}\rangle\}$
The \textbf{count} of a query is the cardinality of its result set. 

For every set of variables $\set{V} = \{\V_{1}$,$\ldots,\V_{n} \}$ there is a \textbf{contingency table} $\ct(\set{V})$. %$CT(\set{V})$. 
This is a table with a row for each of the possible assignments of values to the variables in $\set{V}$, and a special integer column called $\qcount$. 
The value of the $\qcount$ column in a row 
corresponding to $V_{1} = v_{1},\ldots,V_{n} = v_{n}$ records the count of the 
corresponding query. 
Figure~\ref{fig:ct} shows the contingency table for the university database. 
The value of a relationship attribute is undefined for entities that are not related.
Following \cite{Russell2010}, %\cite{BLOG}, 
%$\it{capability(\P,\S)} = n/a $
we indicate this by writing 
%$\it{grade}(\s,\c) = n/a$ 
$\it{capability(\P,\S)} = n/a $ for a reserved constant $\it{n/a}$. 
The assertion $\it{capability(\P,\S)}$ = n/a is therefore equivalent to the assertion that $\ra(\P,\S) = \false$.
%For example, if student $\s$ is not registered in course $\c$, the value of $\it{grade}(\s,\c)$ is undefined. 
\begin{figure}[htbp]
\begin{center}
\resizebox{0.5\textwidth}{!}{
%%!%\includegraphics[width=1\textwidth]{pbn}
%\includegraphics{figures/ct-table.pdf}
\includegraphics[width=0.5\textwidth]{figures/uni-ct-table.JPG}
}
\caption{Excerpt from the joint contingency table for the university database of Figure~\ref{fig:university-tables}. % for the attribute-relation table of Figure~\ref{fig:university-tables}
%Each row shows a query and its instantiation count in the database.
%one possible assignment of such nodes with its count. 
% where for illustration we have added counts for another student like Jack and another course like 103.
%use partial real table
\label{fig:ct}}
\end{center}
\end{figure}
A \textbf{conditional contingency table}, written $$\ct(V_{1},\ldots,V_{k}|V_{k+1} = v_{k+1},\ldots, V_{k+m} = v_{k+m})$$
is the contingency table whose column headers are $V_{1},\ldots,V_{k}$ and whose counts are  defined by subset of instantiations that match the condition to the right of the $\vert$ symbol.  %  \textbar 
We assume that contingency tables omit rows with count 0.
%
%\subsection{Bayes Nets for Relational Data}
%Poole introduced the Parametrized Bayes net (PBN) formalism that combines Bayes nets with logical syntax for expressing relational concepts \cite{Poole2003}. A {\bf Bayes Net (BN)} is a directed acyclic graph (DAG) whose nodes comprise a set of random variables and conditional probability parameters.
%For each assignment of values to the nodes, the joint probability 
%is specified by the product of the conditional probabilities, $P(\it{child}|\it{parent\_values}$). A \textbf{Parametrized Bayes Net} (PBN) is a Bayes net whose nodes are Parametrized random variables \cite{Poole2003}. 
%Since in our machine learning application, PRVs appears in Bayes nets, we often refer to PRVs simply as \textbf{nodes}. Figure \ref{fig:bn-example} shows a PBN for the University database.
%\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
% \centering
%\resizebox{0.3\textwidth}{!}{
% \includegraphics[width=0.3\textwidth]{figures/bn-example.png} 
%}
%\caption{ A sample Parametrized Bayes Net based on the university schema. 
%}\label{fig:bn-example}
%\end{figure}

%\section{Specification of Random Variables}
%The Virtual Join algorithm takes as input a relational database and a set of PRVs or nodes and returns a contingency table for the set. In this section we discuss how to represent a set of PRVs.
%Bayes net nodes.
%parametrized random variables. 
%This set can be defined by the user, or generated based on the database schema information. 
%We define a default set of parameterized random variables for  a target relational database. 
%We show that this set and metainformation about the random variables can be computed from the database system catalog. % using SQL queries. 
%\subsection{The Random Variable Database}
%The information about the random variables is stored in the \textbf{random variable database}.  
%This database provides metainformation for machine learning analysis analogous to the way in which the system catalog provides metainformation for database queries. The definitions of the random variables are stored in tables in the random variable database, together with useful metainformation about the random variables, such as their domains and the sizes of their domains. 
%
%The Virtual Join algorithm distinguishes three types of PRVs: 1Atts, RNodes, and 2Atts. 
%For each attribute of an entity, there is a corresponding attribute node called a \textbf{1Node} for short. The possible values for a 1Node are the same as for the corresponding attribute.  There is a Boolean random variable for each relationship set, called a \textbf{relationship variable} or \textbf{RNode} for short.
%Throughout this paper we assume that all relationships are binary, though this is not essential for our algorithm. 
%For each attribute of an relationship, there is a corresponding attribute node called a \textbf{2Node} for short. The possible values for a 2Node are the same as for the corresponding attribute. 
%%In logical terms, nodes are interpreted as functions that return a value for a given input entity tuple \cite{Milch2007}.
%Table~\ref{table:translation} illustrates %the mapping from the ER design to 
%the logical functor notation and its relationship to the database ER diagram. 
%%\begin{figure}[htbp]
%%\begin{center}
%%\resizebox{0.5\textwidth}{!}{
%%\includegraphics[width=0.5\textwidth]{figures/translation.png}
%%}
%%\caption{Translation from ER Diagram of Relational Database to Bayes Nets.
%%\label{fig:translation}}
%%\end{center}
%%\end{figure}
%
%%\begin{table}[btp] \centering
%%\resizebox{0.5\textwidth}{!}{
%%\begin{tabular}[c]
%%{|l|l|l|l|}\hline
%% ER Design &Bayes Net & Example &Term Notation\\\hline
%%    Entity Tables&Population Variables  & Student, Course & S, C \\\hline
%%    Relation Tables &RNodes &Registration & Registration(C,S) \\\hline
%%   Entity Attributes &1Atts & intelligence, ranking & \{intelligence(S), ranking(S)\} = 1Atts(S) \\\hline
%%   Relationship Attributes &2Atts & satisfaction, grade & \{satisfaction(C, S), grade(C,S)\} = 2Atts(Registration(C,S)) \\\hline
%%   
%%\end{tabular}
%%}
%% % end scalebox
%%\caption{Translation from ER Design of Relational Database to Bayes Nets.
%% \label{table:translation}}
%%\end{table}
%
%%{\em Database Representation.} 
%
%
%The definitions of the three types of PRVs are listed in three tables in the
% Random Variable Database. 
%This database also provides information about the random variables that is required for learning, such as their domains and the sizes of their domains. We refer to this information about the random variables as \textbf{metainformation}. 
%%Utilizing the metainformation allows us to develop general schema-independent learning algorithms. 

%\subsection{Generating Default PRVs} Users may lack the expertise for translating domain knowledge into the functor representation and metainformation.
%% terms of understanding the functor representation and extracting metainformation. 
%An alternative is to exploit the database schema catalog to generate default random variable database.
%We use this default in the experiments reported below.  Because the system metadata is always available for any relational database,  by exploiting it, default random variables can be generated in a completely general way. This means that the Random Variable Database can be set up with no information from the user beyond  the SQL system catalog. However, a user or an application can change the random variable metainformation, by editing the tables in the Random Variable Database.
%
%Given an ER-diagram, it is straightforward to generate functors as illustrated in Table~\ref{table:translation}.
% Given a database information schema, we can compute an implicit ER diagram by reversing the standard translation from ER-diagrams to SQL \cite{Ullman1982}.
%For instance, if a table contains a single primary key column and no foreign key pointers, we treat it as representing an entity set and associate a first-order variable with it. 
%Since the database information schema is itself stored in table form, the default random variable database can be created using an SQL script. 
%We omit further details due to space constraints.
%
%The main complication arises when the database contains a self-relationship \cite{Heckerman+al:SRL07} that relates two entities of the same type. If the schema contains a self-relationship, we add one first-order variable for each role in the relationship, and one set of 1Atts for each first-order variable. 
%%, when we need to introduce more than one relationship variable for the relationship functor. 
%For example, the Mondial database contains a self-relationship $\it{Borders}$ that relates two countries. 
%In the ER diagram, there are two lines from the $\it{Countries}$ entity set to the $\it{Borders}$ relationship set 
%The ring structure may be represented with two different first-order variables that each refer to the $\it{Countries}$ entity set. 
%The corresponding relationship variable is $\it{Borders}(\C_{1},\C_{2})$. The different positions of the first-order variables can represent different roles in the self-relationship.
% In the Mondial example, there are two 1Atts $\it{continent}(\C_{1})$ and $\it{continent}(\C_{2})$.

\section{Relational  Contingency Tables}
%Relational learning algorithms explore longer and longer chains of relationships that may link statistically dependent objects. 
Many relational learning algorithms take an iterative deepening approach: 
%\cite{Neville2007}: 
explore correlations along a single relationship, then along relationship chains of length 2, 3, etc. 
Chains of relationships form a natural lattice structure, where iterative deepening corresponds to moving from the bottom to the top. 
%
%Conceptually, the lattice view simplifies describing and developing learning algorithms. Computationally, 
The Virtual Join algorithm computes contingency tables by using the results for smaller relationships for larger relationship chains. 
%Another benefit of level-wise lattice learning is that if the learning algorithm requires sufficient statistics only up to a certain depth in the lattice, the Virtual Join algorithm can be terminated at this depth. %The lattice diagram facilitates the computation of contingency tables that summarize the sufficient database statistics for learning.
%
%\subsection{The Relationship Lattice}
%

A relationship variable set is a \textbf{chain} if it can be ordered as a list $[\Relation_{1}(\argterms_{1}),\ldots,\Relation_{k}(\argterms_{k})]$ 
%is a \textbf{chain} if 
such that each functor $\Relation_{i+1}(\argterms_{i+1})$ shares at least one first-order variable with the preceding terms $\Relation_{1}(\argterms_{1}),\ldots,\Relation_{i}(\argterms_{i})$.
%\footnote{Essentially the same concept is called a slot chain in PRM modelling \cite{Getoor2007c}.}
%A relationship set forms a chain if the corresponding list is a chain. 
All sets in the lattice are constrained to form a chain.
%
For instance, in the University schema of Figure~\ref{fig:university-schema}, a %relationship 
chain is formed by the two relationship variables
\[\reg(\S,\C),\ra(\P,\S).\]
%list \[[\it{RA}(\P,\S),\it{Registered}(\S,\C)].\] 
If relationship variable $\it{Teaches}(\P,\C)$ is added,
%to record which student is a TA for which course,
we may have a three-element chain \[\reg(\S,\C),\ra(\P,\S),\it{Teaches}(\P,\C).\] 
The subset ordering defines a lattice on relationship sets/chains. 
Figure~\ref{fig:big-lattice} illustrates the  lattice for the relationship variables in the university schema. 
For reasons that we explain below, entity tables are also included in the lattice and linked to relationships that involve the entity in question. 
\begin{figure}[htbp]
\begin{center}
\resizebox{0.5\textwidth}{!}{
\includegraphics[width=0.8\textwidth]{figures/uni-big-lattice.png}
}

\caption{A lattice of relationship sets for the university schema of Figure~\ref{fig:university-schema}.
% Links from entity tables to relationship tables correspond to foreign key pointers. 
%The list representation of the sets is determined by the functor ordering $\it{Registered} < \it{TA} < \it{Teaches}$. 
\label{fig:big-lattice}}
\end{center}
\end{figure}
%
With each relationship chain $\set{\Relation}$ (Rchain for short) is associated a $\ct$-table $\ct_{\set{\Relation}}$. 
The variables in the $\ct$-table  $\ct_{\set{\Relation}}$ %$\B_{\set{Relation}}$
 comprise the relationship variables  in $\set{\Relation}$, and the unary/binary descriptive attributes associated with each of the relationships. To describe these, we introduce the following notation (cf. Table~\ref{table:translation}).
%Let's first introduce the notations for all kinds of functor variables in order to extend relational algebra operators for manipulating $\qcount$ in contingency tables as follows.
%Let $R$ be a relationship variable and let $\set{R}$ be a set of relationship variables.

\begin{itemize}
\item  $\eatts(\A)$ denotes the attribute variables of a first-order variable $\A$ collectively (1 for unary).
\item $\eatts(\set{R})$ denotes the set of entity attribute variables for the first-order variables that are involved in the relationships in $\set{R}$. 
%\item $\eatts(\set{\R})$ is the union of the entity attributes for each relationship $\R \in \set{\R}$.
\item $\ratts(\set{R})$ denotes the set of relationship attribute variables for %the first-order variables involved in 
the relationships in $\set{R}$ (2 for binary).
%\item $\ratts(\set{R})$ is the union of the relationship attributes for each relationship $R \in \set{R}$.
\item $\atts(\set{R}) \equiv \eatts(\set{R}) \cup \ratts(\set{R})$ is the set of all attribute variables in the relationship chain $\set{R}$.
%, similarly for $\atts({\set{R}})$.

%\item $R_{i}$ is the Boolean relationship variable.
%\item $\etable$ denotes a entity table.
%\item $Column\_List({\etable})$ is the set of non-count columns in entity table $\etable$.
\end{itemize}

%We extend these concepts union-wise to sets of relationships, so that $\eatts(\set{\R})$ resp. $\ratts(\set{R})$ denotes the union of the entity resp. relationship attributes for each relationship $\R \in \set{\R}$.
In this notation, the variables in the $\ct$-table  $\ct_{\set{\Relation}}$  are denoted as $\set{\Relation} \cup \atts({\set{R}})$. 
%
%\emph{Implementation.} The Random Variable Database specifies the set of relationship variables for building the lattice. Building a lattice of relationship chains can be done using any standard technique, such as those developed for enumerating itemsets in association rule  mining \cite{Agrawal1994}. 
%We create a table {\em Lattice} in the Bayes net database with one row for each relationship chain. We also create two auxilliary tables: One that lists which relationship chains are children of which others.  
%For example, the relationship chain $\it{Registration}(\S,\C),\it{RA}(\S,\P)$ is a child of the chain $\it{Registration}(\S,\C)$. Another auxilliary table lists which relationship variables are members of which relationship chain. For example, the relationship chain $\it{Registration}(\S,\C),\it{RA}(\S,\P)$ has two members. The auxilliary tables are used during learning as described below.
%
The goal of the Virtual Join algorithm is to compute a contingency table for each chain $\set{\Relation}$. 
In the example of Figure~\ref{fig:big-lattice}, the algorithm computes 10 contingency tables. The $\ct$-table for the top element of the lattice is the \textbf{joint $\ct$-table} for the entire database. 
%It is useful to distinguish two types of rows within each $\ct$-table: rows where all the relationship variables are assigned the value $\true$---positive relationships only---and rows where one or more relationship variables are assigned the value $\false$---some negative relationships. 
%We begin with the case of positive relationships only.
%\section{Computing Relational Contingency Tables} \label{sec:cta}
%\section{Computing Contingency Tables For Positive Relationships} 
%The learning algorithms described in this paper rely on the  availability of the $\ct$-tables (see Figure~\ref{fig:university-tables}). 
%Computing the contingence tables raises two key challenges that we address in this paper. 

%\begin{normalsize}
If a conjunctive query involves only positive relationships, then it can be computed using SQL's count aggregate function applied to a table join. To illustrate, we show the SQL for computing the positive relationship part of the $\ct$-table for the $\ra(\P,\S)$ chain.
%\end{normalsize}
%
%\begin{small}
\begin{quote}
CREATE TABLE $\ct_{T}$  AS 
\\SELECT Count(*) as  count,  student.ranking, \\student.intelligence, professor.popularity,\\ professor.teachingability, RA.capability, RA.salary  \\
FROM professor, student, RA  \\
WHERE  \\RA.p\_id = professor.p\_id and RA.s\_id = student.s\_id  \\
GROUP BY student.ranking,  student.intelligence, professor.popularity,  professor.teachingability, RA.capability,  RA.salary
\end{quote}
%\end{small}
Even more efficient than SQL count queries is the Tuple ID propagation method, a Virtual Join method for computing query counts with positive relationships only \cite{Yin2004}. 
In the next section we assume that contingency tables for positive relationships only have been computed already, and consider how such tables can be extended to full contingency tables with both positive and negative relationships.

%The SQL count query represents the positive part of the $\ct$-table much like a view definition represents a relation. 
%To construct the SQL count query without advance knowledge of the database schema, we utilize the metainformation in a \textbf{metaquery}.
%: Using the meta-information in the random variable database, we construct an SQL query for each relationship chain. 
%The SQL query represents a table join followed by a sum aggregation. Executing this query constructs the positive relationship part of the contingency table. 

%In the next section we proposed a new approach for the second challenge.
%
%The second challenge is computing event counts that involve negative relationships. This is infeasible using standard table joins [explain: firends vs un-firends]. 
%We approach this problem in two steps. 
%First, we show that event counts using $m+1$ negative relationships can be computed from two event counts that each involve at most $m$ relationships. 
%We state this result as a relational algebra identity in a new extension of relational algebra that we term \textbf{contingency table algebra}. A dynamic programming algorithm applies the algebraic identify repeatedly to efficiently build up a complete contingency table from partial tables that involve fewer negative relationships. 
%
%\subsection{Metaqueries for Utilizing Schema Information} 
%%As the top level of  Figure~\ref{fig:flow} illustrates, 
%The required counts involving only true relationships can be computed using the standard SQL constructs COUNT(*) and GROUP BY. 
%The general form of these operations is the same for every input database. 
%The varying part is the list of columns to be included, which depends on the input database. 
%For a fixed database, the column lists can be hard-coded. 
%To achieve a general solution when the column list is not known in advance, we introduce a new approach that we refer to as an SQL \textbf{meta query}. 
%
%An SQL metaquery is an SQL query that takes as input schema information as recorded in the random variable database,
%and produces four kinds of tables: the Select, From, Where and Group By tables. The Select table lists the entries in the Select clause of the target query, the From table lists the entries in the From clause, and similar for Where and GROUP BY tables. %\footnote{basically the Group BY table is same as Select table without $\qcount$ column}. 
%Thus an SQL meta query maps schema information to the components of another SQL query. 
%Given the four query tables, the corresponding SQL query can be easily executed in an application or stored procedure to produce the $\ct$-table entries.
%Figure~\ref{fig:meta-query} shows an example of metaqueries for the university database. The metaquery accesses tables in the Random Variable Database $RV$. The entries of the Group By table (not shown) are the same as the Select table without the $\qcount$ column.
%The $\it{@database@}$ is a string placeholder for the input database name. We omit further details due to space constraints.
%
%\begin{figure}[htb]
%\begin{center}
%\resizebox{0.45\textwidth}{!}{
%\includegraphics[width=0.5\textwidth]{figures/meta-query.png}
%}
%\caption{Example of metaqueries and metaquery results for the relationship table $\ra$  based on university database. {\em
%zqian: explain rnid, pvid ?? }
%~\label{fig:meta-query} }
%\end{center}
%\end{figure}


\section{Computing Contingency Tables For Negative Relationships} 
%Computing sufficient statistics that involve negative relationships is infeasible using standard table joins. 
%%and can be carried out by regular table joins or optimized virtual joins~\cite{Yin2004}. 
%%
%%Computing joint probabilities for a family containing one or more negative relationships is harder. 
%Standard join tables materialize new data tables that enumerate tuples of objects that are {\em not} related. %(see Figure~\ref{fig:university-tables}).
%%and then apply existing counting methods to the new tables. 
%However, the number of unrelated tuples is too large to make this scalable.
% (consider the number of user pairs who are {\em not} friends on Facebook). 
%A numerical example will illustrate why this is not feasible. Consider a university database with 20,000 Students, 1,000 Courses and 2,000 TAs. If each student is registered in 10 courses, the size of a $\it{Registered}$ table is 200,000. So the number of complementary student-course pairs is $2 \times 10^{7}-2 \times 10^{5}$, which is too big for most database systems. 
%If we consider joins, complemented tables are even more difficult to deal with: suppose that each course has at most 3 TAs. Then  the number of satisfying instantiations of a positive relationship only formula such as $\it{Registered}(\S,\C) = \true,\it{TA}(\T,\C) = \true)$ is less than $6 \times 10^{5}$, whereas with negations the number of instantiations of the expression $\it{Registered}(\S,\it{course}) = \false, \it{TA}(\T,\it{course}) = \false)$ is on the order of $4 \times 10^{10}$. 
%\subsection{The M\"obius parametrization.} 
%To compute frequencies involving negative relationships, we would like to use the optimized algorithms for table join frequencies as an oracle/black box. 
%Can we instead reduce the computation of sufficient statistics that involve negative relationships to the computation of sufficient statistics that involve existing (positive) relationships only? 
%In their work on learning Probabilistic Relational Models with existence  uncertainty, Getoor et al. 
%provided a subtraction method for the special case of estimating counts with only a single negative relationship \cite[Sec.5.8.4.2]{Getoor2007c}. 
%They did not treat contingency tables with multiple negative relationships, which we consider next.
We describe a Virtual Join algorithm that computes the required sufficient statistics without the materializing a Cartesian product of entity sets. Our experiments below compare the virtual joins with materializing table joins. 
%We approach this problem in two steps. 
First, we introduce an  extension of relational algebra that we term \textbf{contingency table algebra}. The purpose of this extension is to 
show that query counts using $k+1$ negative relationships can be computed from two query counts that each involve at most $k$ relationships. 
%We state this result as a relational algebra identity in a new extension of relational algebra that we term \textbf{contingency table algebra}. 
Second, a dynamic programming algorithm applies the algebraic identify repeatedly to build up a complete contingency table from partial tables.
% that involve fewer negative relationships. 

%Our first naive implementation constructs this tables using standard joins. 
%While this was sufficient for our experiments, the cross-products carry a \textbf{quadratic} costs for binary relations, and therefore do not scale to large datasets. 
%Moreover, the hierarchical search requires joins of the extended tables. 

\subsection{Contingency Table Algebra} \label{sec:cta}
We introduce relational algebra style operations defined on contingency tables.
%\subsection{Definition} 
%First we introduce some notation.
%If $\dtable$ denotes a generic database table, then $\ColumnList(\dtable)$ is a list of columns in table $\dtable$; the list order is irrelevant. For a CT-table $\ct$, the expression $\ColumnList(\ct)$ denotes a list of columns other than the count column.

%none $\qcount$ columns in table $\dtable$, 
%and column sets $\set{V}$, $\set{U}$ are the union of $ \qcount $ column with $\ColumnList({\dtable})$ for different given $\dtable$. 
%Suppose ${\dtable}_{1}$, ${\dtable}_{2}$ be two union-compatible %contingency 
%tables with the same column headers, and ${\dtable}_{3}$ is another table,
%then $\ColumnList({\dtable}_{1}) =V_{1}, \ldots,\ V_{k}$, $\ColumnList({\dtable}_{2}) =V_{1}, \ldots,\ V_{k}$
%and $\ColumnList({\dtable}_{3}) = U_{1}, \ldots,\ U_{m}$.
%We define $\ColumnList({\dtable}_{1}) = \ColumnList({\dtable}_{2})$ by 
%${\dtable}_{1}.V_{1} ={\dtable}_{2}.V_{1}, \ldots, {\dtable}_{1}.V_{k} = {\dtable}_{2}.V_{k}$.


\subsubsection{Unary Operators} \label{sec:unary}
\begin{description}
\item[Selection] $\sigma_{\selectcond}  \ct$ selects a subset of the rows in the  $\ct$-table  that satisfy condition $\selectcond$. This is the standard relational algebra operation except that the selection condition $\selectcond$ may not involve the $\qcount$ column.
\item[Projection]  %$\project_{\set{V}}  
$\project_{\V_{1},\ldots,\V_{k}} \ct$ selects a subset of the  columns in the  $\ct$-table, excluding the count column. 
The counts in the projected sub table are the sum of counts of rows that satisfy the query in the sub table. 
The  $\ct$-table projection  $\project_{\V_{1},\ldots,\V_{k}} \ct$ can be defined by the following SQL code template:
\begin{quote}
SELECT SUM(count) AS count, $V_{1}, \ldots,\ V_{k}$ \\
FROM $\ct$ \\
GROUP BY $V_{1}, \ldots,\ V_{k}$
\end{quote}
%\begin{quote}
%SELECT SUM($\qcount$) AS $\qcount$, $\ColumnList({\ct})$ \\
%FROM $\ct$ \\
%GROUP BY $\ColumnList({\ct})$
%\end{quote}
\item[Conditioning]  $\condition_{\selectcond}  \ct$ returns a conditional contingency table. Ordering the columns as $(V_{1},\ldots,V_{k}, \ldots,\V_{k+j}$),  suppose that the selection condition is a conjunction of values of the form $C = (V_{k+1} = v_{k+1},\ldots, V_{k+j} = v_{k+j})$.  Conditioning can be defined in terms of selection and projection by the equation:
\begin{equation}
\condition_{\selectcond}  \ct = \project_{\V_{1},\ldots,\V_{k}} (\select_{\selectcond}  \ct) \nonumber
\end{equation}
\end{description}

\subsubsection{Binary Operators} \label{sec:bin}
We use $\set{V}$, $\set{U}$ in SQL templates to denote a list of column names in arbitrary order. The notation $\ct_{1}.\set{V} = \ct_{2}.\set{V}$ indicates an equijoin condition: the contingency tables $\ct_{1}$ and $\ct_{2}$ have the same column set $\set{V}$ and matching columns from the different tables have the same values.
\begin{description}
\item[Cross Product]  %Let %$\ct_{1}(\V_{1}, \ldots,\ V_{m}),\ct_{3}(U_{1}, \ldots,\ U_{k})$
%$\ct_{1}(\set{V}),\ct_{3}(\set{U})$ 
% be two contingency tables %that do not share any papulation variable. 
The \textbf{cross-product} of $\ct_{1}(\set{U}),\ct_{2}(\set{V})$ is the Cartesian product of the rows, where the product counts are the products of count. The cross-product can be defined by the following SQL template:
% \begin{quote}
%SELECT \\($\ct_{1}.COUNT*\ct_{2}.COUNT$) AS COUNT,  $U_{1}, \ldots,\ U_{k}, \V_{1}, \ldots,\ V_{m}$ \\
%FROM  $\ct_{1},\ct_{2}$
%\end{quote}
\begin{quote}
SELECT \\($\ct_{1}.\qcount *\ct_{2}.\qcount$) AS $\qcount$,  $\set{U}$, $\set{V}$\\
FROM  $\ct_{1},\ct_{2}$
\end{quote}


\item[Addition] 
 The \textbf{count addition} $\ct_{1}(\set{V}) + \ct_{2}(\set{V})$ adds the counts of matching rows, as in the following SQL template.
\begin{quote}
SELECT % $\ct_{1}$.COUNT+$\ct_{2}$.COUNT 
$\ct_{1}.\qcount$+$\ct_{2}.\qcount$ AS $\qcount$, $\set{V}$ \\%$\ct_{1}.V_{1} , \ldots, \ct_{1}.V_{k} $ \\
FROM  $\ct_{1},\ct_{2}$\\
%WHERE $\ct_{1}.V_{1} = \ct_{2}.V_{1}, \ldots, \ct_{1}.V_{k} = \ct_{2}.V_{k}$
WHERE $\ct_{1}.\set{V} = \ct_{2}.\set{V}$
\end{quote}

If a row appears in one $\ct$-table but not the other, we include the row with the count of the table that contains the row. %Note that  $\ct_{1}(\set{V})$ and $\ct_{2}(\set{V})$ are union-compatible because they have the same  column headers.

\item[Subtraction] %Let $\ct_{1}(\set{V}),\ct_{2}(\set{V})$ be two union-compatible contingency tables with the same column headers. 
The \textbf{count difference} $\ct_{1}(\set{V}) - \ct_{2}(\set{V})$ equals $\ct_{1}(\set{V}) + (- \ct_{2}(\set{V}))$ where $- \ct_{2}(\set{V})$ is the same as $\ct_{2}(\set{V})$ where the counts are negative. 
Table subtraction is defined only if (i) without the $\qcount$ column, the rows in $\ct_{1}$ are a superset of those in $\ct_{2}$, and (ii) for each row that appears in both tables, the count in $\ct_{1}$ is at least as great as the count in $\ct_{2}$.
\end{description}


%\subsection{Implementing the Contingency Table Operators}\label{sec:imp}
%Everything true: database optimization. General tricks, e.g., omit 0 counts. Specific tricks, e.g. merge-sort.
\subsubsection{Implementation}\label{sec:imp}
%Our implementation is based on SQL queries, whose execution is well optimized by a RDBMS such as MySQL.   
%A RDBMS offers built-in options for improving query planning, such as using covering index  to speed up the conditioning.
%[give examples: e.g, indixes]. 
%For some operations where MySQL chooses a suboptimal query plan, we implemented our own method.

%\subsubsection{Unary Operators}
%Suppose we already know the required columns list $\ColumnList({\dtable})$ for each query clause. 
The selection operator can be implemented  using SQL as with standard relational algebra. 
Projection with $\ct$-tables requires use of the GROUP BY construct as shown in Section~\ref{sec:unary}. 
%The list of columns to be projected at a point in the algorithm can be found by a meta query. 
%\subsubsection{Binary Operators}
 % big * smaller, faster enough.
%The required column lists can be found by a meta query. 

%The most difficult operation to implement efficiently is 
For addition/subtraction, simply executing the SQL query shown above %shown in Section~\ref{sec:bin} 
may lead to a quadratic cost if a nested-loops join is used, and therefore does not scale to large datasets.
A more efficient algorithm is a sort-merge join \cite{Ullman1982}. 
Given two union-compatible $\ct$-tables, each row in one matches at most one other on the non-count columns. 
As is well known, with unique matches, the cost of a sort-merge join is $\it{size}(table1) + \it{size}(table2) +$ the cost of sorting both tables. 
Our experiments below are based on a sort-merge join (our own implementation in Java). 

The cross product is easily implemented in SQL as shown in Section~\ref{sec:bin}. The cross product size is quadratic in the size of the input tables, so a quadratic cost is unavoidable.
%A general trick to shrink the size $\ct$-table is remove the 0 counts in $\ct$-table.
%But for a general input database, usually the user can not know the column lists in advance. 
%So in next section, we propose a new method to compute these.

\subsection{Lattice Computation of Contingency Tables} \label{sec:mobius}
\begin{figure*}[tb]
\begin{center}
\resizebox{0.8\textwidth}{!}{
%%!%\includegraphics[width=1\textwidth]{pbn}
%\includegraphics{figures/subtraction-flow.pdf}
\includegraphics[width=0.9\textwidth]{figures/sub.jpg}
}

\caption{Computation of the $\ct$-table for the relationship $\ra(\P,\S)$ using Algorithm~\ref{Pivot_CT_Computation}. The operations are implemented using dynamic SQL queries as shown. Lists of column names are abbreviated as shown and as follows.
$\ColumnList({\ct_{*}}) = \ColumnList({temp})=\ColumnList({\ct_{F}})$, 
$\ColumnList({\ct}) =  \ColumnList(\ct_{\false}^{+})  = \ColumnList(\ct_{\true}^{+}) $.
\label{fig:flow}}
\end{center}
\end{figure*}

This section describes a method for computing the contingency tables level-wise in the relationship chain lattice. We start with a contingency table algebra equivalence that allows us to compute counts for rows with negative relationships from rows with positive relations.
Following \cite{Moore1998}, we use a ``don't care" value $*$ to indicate that a query does not specify the value of a node. For instance, the query $\Relation_{1} = \true, \Relation_{2} = *$ is equivalent to the query $\Relation_{1} = \true$. 
\begin{proposition}%[Pivot_CT]
\label{PivotCT}
Let $R$ be a relationship variable and let $\set{R}$ be a set of relationship variables. Let $\Nodes$ be a set of variables that %(1) must contain all $\eatts$ of $\R$, and may contain any other variables, as long as (2) $\Nodes$ 
does not contain $\R$ nor any of the $\ratts$ of $\R$. Let  $\X_{1},\ldots, \X_{l}$ be the first-order variables that appear in $\R$ but not in $\Nodes$, where ${l}$ is possibly zero. Then we have
\begin{flalign}
\label{eq:update}
&\ct(\Nodes \cup \eatts(R)|\set{R} = \true, R = F) = & \\ %\nonumber\\
& \ct(\Nodes|\set{R} = \true, R =*) \times \ct(\X_{1}) \times \cdots \times \ct(\X_{l}) \nonumber & \\
& -\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = T). \nonumber&
\end{flalign}
If $l = 0$, the equation holds without  the %$ \ct(\X_{1}) \times \cdots \times \ct(\X_{k})$ 
cross-product term.
\end{proposition}

We omit the proof due to space constraints. The core of the argument is that Equation~\ref{eq:update} can be treated as a relational algebra analog of the probabilistic equality
$P(\selectcond,\Relation = \false) = P(\selectcond) - P(\selectcond,\Relation = \true)$ that holds for any condition $\selectcond$.
%
%\begin{proof}
%The equation 
%\begin{align}
%&\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = *) = &\label{eq:update2}  \\ 
%&\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = T)  + & \nonumber \\ 
%&\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = F) & \nonumber
%\end{align}
%holds because the set $\Nodes \cup \eatts(R)$ contains all first-order variables in $R$.\footnote{We assume here that  for each first-order variable, there is at least one $\eatt$, i.e., descriptive attribute.} % $\it{1Node}$. 
%
%%Using table subtraction Equation~\eqref{eq:update} implies
% Equation~\eqref{eq:update2} implies
%\begin{align} 
%&\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = \false) =& \label{eq:table-subtract} \\ 
%&\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = *) -&\nonumber  \\
% & \ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = \true).& \nonumber
%\end{align}
%
%To compute the $\ct$-table conditional on the relationship $\R$ being unspecified, we use the equation
%\begin{align}
%&\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = *) =  &\label{eq:table-multiply}\\
%&\ct(\Nodes|\set{R} = \true, R =*) \times \ct(\X_{1}) \times \cdots \times \ct(\X_{l})& \nonumber
%\end{align}
%which holds because if the set $\Nodes$ does not contain a first-order variable of $\R$, then the counts of the associated $\eatts(\R)$ are independent of the counts for $\Nodes$. 
%If $l = 0$, there is no new first-order variable, and Equation~\eqref{eq:table-multiply} holds without  the cross-product term.
%%
%Together Equations~\eqref{eq:table-subtract} and~\eqref{eq:table-multiply} establish the proposition.
%\end{proof}

%\SetAlFnt{\small}
\begin{algorithm}[htbp]
\label{Pivot_CT_Computation}
%\linesnumbered
\SetKwData{KwCalls}{Calls}
\SetKwData{KwCondition}{Precondition:}
\KwIn{Two conditional contingency tables   $\ct_{\true} :=\ct(\Nodes,\it{\ratts}(R_{\it{pivot}})|R_{\it{pivot}}=\true$$,\set{R}=\true)$ and  $\ct_{*} :=\ct(\Nodes|R_{\it{pivot}} = *$$,\set{R}=\true)$ .}
\KwCondition  %$\Nodes:= \eatts(\R_{1},\ldots,\R_{\ell}) \cup \ratts(\R_{1},\ldots,\R_{\ell}) \cup (\R_1,\ldots,\R_{\ell}) - R_{\it{pivot}} - \ratts(R_{\it{pivot}}) $ \;
 The set $\Nodes$ does not contain the relationship variable $R_{\it{pivot}}$ nor any of its descriptive attributes $\ratts(R_{\it{pivot}}$).\;

% {The set $\Nodes$ contains $\eatts(\R_{1},\ldots,\R_{\ell}) \cup \ratts(\R_{1},\ldots,\R_{\ell}) \cup (\R_1,\ldots,\R_{\ell})$ but not the relationship variable $R_{\it{pivot}}$ nor any of its descriptive attributes $\ratts(R_{\it{pivot}}$) \;}

\KwOut{The conditional contingency table $ \ct(\Nodes,\it{\ratts}(R_{\it{pivot}}),R_{\it{pivot}}|$$\set{R}=\true)$ .}
\begin{algorithmic}[1]
%\STATE $CT_{\false} := \ct(\Nodes|R_{\it{pivot}} = *$$,\set{R}=\true) - \pi_{\Nodes} \ct(\Nodes,\it{\ratts}(R_{\it{pivot}})|R_{\it{pivot}}=\true$$,\set{R}=\true)$.
\STATE $\ct_{\false} := \ct_{*} - \pi_{\Nodes}\ct_{\true}$.

\COMMENT{Implements the algebra Equation~\ref{eq:update} in proposition~\ref{PivotCT}.}
\STATE $\ct_{\false}^{+}$ := extend  $\ct_{\false}$ with columns $R_{\it{pivot}}$ everywhere false and $\it{\ratts}(R_{\it{pivot}})$ everywhere $n/a$.
%\STATE $CT_{\true}^{+}$ := extend  $\ct(\Nodes,\it{\ratts}(R_{\it{pivot}})|R_{\it{pivot}}=\true$$,\set{R}=\true)$ with columns $R_{\it{pivot}}$ everywhere true.
\STATE $\ct_{\true}^{+}$ := extend  $\ct_{\true}$ with columns $R_{\it{pivot}}$ everywhere true.
\STATE \Return $\ct_{\false}^{+} \cup \ct_{\true}^{+}$
\end{algorithmic}
\label{alg:pivot}
\caption{The Pivot function returns a conditional contingency table for a set of attribute variables and all possible values of the relationship $R_{\it{pivot}}$, including $R_{\it{pivot}} = \false$. %It implements the algebra Equation~\eqref{eq:update}.
 The set of conditional relationships $\set{R} =(\R_{pivot},\ldots,\R_{\ell})$ %=\true$
 may be empty in  which case the Pivot computes an unconditional ct-table. }
\end{algorithm}

The construction of the $\ct_{\false}$ table in Figure~\ref{fig:flow} illustrates Equation~\eqref{eq:update}. Algorithm~\ref{Pivot_CT_Computation} provides pseudo-code for applying Equation~\eqref{eq:update} to compute a complete $\ct$-table given a partial table where a specified relationship variable $\Relation$  is true
and another partial table that does not contain the relationship variable. 
We refer to $\Relation$ as the \textbf{pivot} variable. 
For extra generality, we apply Equation~\eqref{eq:update} with a condition that lists a set of relationship variables fixed to be true.  
Algorithm~\ref{level-wise-subtract} shows how the Pivot operation can be applied repeatedly to find all contingency tables in the relationship lattice. Figure~\ref{fig:flow} illustrates the computation for the case of only one relationship. The computation proceeds as follows. 


\begin{algorithm*}[tb]
\label{level-wise-subtract}
\SetKwData{KwCalls}{Calls}
\SetKwData{Notation}{Notation}
\KwIn{A relational database $\D$; a set of  variables}
\KwOut{A contingency table that lists the count in the database $D$ for each possible assignment of values to each variable.}
\begin{algorithmic}[1]
\FORALL{first-order variables $\X$}
\STATE compute $\ct(\eatts(\X))$ using SQL queries.
\ENDFOR
\FORALL{relationship variable $\R$}
\STATE $\ct_{*} := \ct(\X) \times \ct(\Y)$ where $\X$,$\Y$ are the first-order variables in $\R$.
\STATE $\ct_{\true} := \ct(\eatts(\R)|\R = \true)$ using SQL joins.
\STATE Call  $\it{Pivot}(\ct_{\true},\ct_{*})$ to compute $\ct(\eatts(\R),\ratts(\R),\R)$.
\ENDFOR
\FOR{Rchain length $\ell=2$ to $m$}
\FORALL{Rchain $\R_{1},\ldots,\R_{\ell}$}
\STATE $Current\_\ct :=  \ct(\eatts(\R_{1},\ldots,\R_{\ell}),\ratts(\R_{1},\ldots,\R_{\ell})|\R_{1}=\true,\ldots,\R_{\ell}=\true)$ using SQL joins.
\FOR{$i=1$ to $\ell$} \label {reﬂine:innerloop}
\IF{ $i$ equals  1}
\STATE $\ct_{*} := \ct(\eatts(\R_{2},\ldots,\R_{\ell}),\ratts(\R_{2},\ldots,\R_{\ell})|
\R_{1}=*,\R_{2} = \true,\ldots,\R_{\ell}=\true) \times \ct(\X)$ where $\X$ is the first-order variable in $\R_{1}$, if any, that does not appear in $\R_{2},\ldots,\R_{\ell}$
\COMMENT{$\ct_{*}$ can be computed from a $\ct$-table for a Rchain of length $\ell-1$.}
\ELSE
\STATE $\eatts_{\bar{i}} := \eatts(\R_{1},\ldots,\R_{i-1},\R_{i+1},\ldots,\R_{\ell})$.
\STATE $\ratts_{\bar{i}} := \ratts(\R_{1},\ldots,\R_{i-1},\R_{i+1},\ldots,\R_{\ell})$.
\STATE $\ct_{*} := \ct(\eatts_{\bar{i}}, \ratts_{\bar{i}},\R_{1},\ldots,\R_{i-1})|
\R_{i}=*,\R_{i+1} = \true,\ldots,\R_{\ell}=\true) \times \ct(\Y)$ where $\Y$ is the first-order variable in $\R_{i}$, if any, that does not appear in $\set{\R}$. 
\ENDIF \\
\STATE $Current\_\ct :=  \it{Pivot}(Current\_\ct,\ct_{*})$.
\ENDFOR 
\COMMENT{Loop Invariant: After  iteration $i$, the table $Current\_\ct$ equals 
$\ct(\eatts(\R_{1},\ldots,\R_{\ell}), \ratts(\R_{1},\ldots,\R_{\ell}),\R_{1},\ldots,\R_{i}|\R_{i+1} = \true,\ldots,\R_{\ell}=\true)$}
\ENDFOR
\COMMENT{Loop Invariant: The $\ct$-tables for all Rchains of length $\ell$ have been computed.}
\ENDFOR 
\STATE \Return the $\ct$-table for the Rchain involves all the relationship variables.
\end{algorithmic}
\label{alg:fmt}
\caption{Virtual Join algorithm for Computing the Contingency Table for Input Database}%Dynamic Algorithm for Computing  {\em Sufficient Statistics} given one relational database }
\end{algorithm*}

{\em Initialization.} Compute $\ct$-tables for entity tables.
% (lines 1--3). 
Compute $\ct$-tables for each single relationship variable $\Relation$ , conditional on $\Relation = \true$. % (line 6).  
If $\Relation = \ast$, then no link is specified between the first-order variables involved in the relation $\Relation$. Therefore the individual counts for each first-order variable are independent of each other and the joint counts can be obtained by the cross product operation. % (line 5). 
Apply the Pivot function to construct the  complete $\ct$-table for relationship variable $\Relation$. % (line 7). 

{\em Lattice Computation.} The goal is to compute $\ct$-tables for all relationship chains of length $>1$. For each relationship chain, order the relationship variables in the chain arbitrarily. Make each relationship variable in order the Pivot variable $\Relation_{i}$. For the current Pivot variable $\Relation_{i}$, find the conditional $\ct$-table where $\Relation_{i}$ is unspecified, and the subsequent relations $\Relation_{j}$ with $j>i$ are true. This $\ct$-table can be computed from a $ct$-table for a shorter chain that has been constructed already. The conditional $ct$-table   has been constructed already, where $\Relation_{i}$ is true, and the subsequent relations are true (see loop invariant). Apply the Pivot function to construct the  complete $\ct$-table, for any Pivot variable $\Relation_{i}$,  conditional on the subsequent relations being true.

\subsection{Complexity Analysis} 
\label{sec:complexity} 

The key point about the Virtual Join algorithm %(VJA) %~\ref{alg:fmt} 
is that it avoids the materializing the cross product of entity tuples. {\em The algorithm accesses  only \textbf{existing} tuples, never constructs nonexisting tuples.} The number of $\ct$-table operation is therefore independent of the number of data records in the original database. We bound the total number of $\ct$-algebra operations performed by the Virtual Join algorithm in terms of the size of its output, the number of sufficient statistics that involve negative relationships. 
%
\begin{proposition}
The number of $\ct$-table operations performed by the Virtual Join algorithm is bounded as $$\it{\# \ct\_\it{ops}} = O(r \cdot \log_{2} r)$$ where $\row$ is the number of sufficient statistics that involve negative relationships.
\end{proposition}
%In the next section we discuss the cost of a single  $\ct$-algebra operation.
%
%We provide  upper bounds in terms of two parameters: the number of relationship variables  $m$, and  the number of rows $\row$  in the $\ct$-table that is the output of the algorithm. 
%For these parameters we establish that
%$$\it{\# \ct\_\it{ops}} = O(r \cdot \log_{2} r) = O(m \cdot 2^{m}) .$$
%This shows the efficiency of our algorithm for the following reasons.
%(i) (ii)  The second upper bound means that the number of $\ct$-algebra operations is fixed-parameter tractable with respect to $m$.\footnote{For arbitrary $m$, the problem of computing a $\ct$ table in a relational structure is \#P-complete \cite[Prop.12.4]{Domingos2007}.} In practice the number $m$ is on the order of the number of tables in the database, 
%which is very small compared to the number of tuples in the tables.

\emph{Derivation Outline.} Let $m$ be the  number of relationship variables. By counting $\ct$-table operations in the lattice, we can show that  
%
\begin{equation} \label{eq:rchain-bound}
\it{\# \ct\_\it{ops}} = O(m \cdot 2^{m-1}).
\end{equation}
%
Now let $\statistics$ be the number of sufficient statistics where all relationship variables are true. For instance, if there are $n$ binary attributes, then $\statistics = 2^{n}$. Then we have \begin{equation} \label{eq:row-stats} \row = \statistics \cdot (2^{m}-1) \end{equation} since there are $2^{m}-1$ combinations of relationship values with at least one negative relationship. Solving for $m$ and substituting the result into the expression~\eqref{eq:rchain-bound} leads to the desired bound.
%For the upper bound in terms of $\ct$-table rows $\row$, we note that the output $\ct$-table can be decomposed into $2^{m}$ subtables, one for each assignment of values to the $m$ relationship variables. 
%Each of these subtables contains the same number of rows $d$ , one for each possible assignment of values to the attribute variables. 
%Thus the total number of rows is given by $r = d \cdot 2^m.$ 
%Therefore we have 
%$m \cdot 2^{m} = \log_{2} (r/d) \cdot r/d \leq \log_{2}(r) \cdot r.$
%Thus the total number of $\ct$-algebra operations is $O(r \cdot \log_{2}(r))$.
%$\it{\# \ct\_\it{ops}} = O(r \cdot \log r) .$

%From this analysis we see that both upper bounds are overestimates. (1) Because relationship chains must be linked by foreign key constraints, the number of valid relationship chains of length $\ell$ is usually much smaller than the number of all possible subsets ${m\choose \ell}$. (2) The constant factor $d$ grows exponentially with the number of attribute variables, so $\log_{2}(r) \cdot r$ is a loose upper bound on $\log_{2} (r/d) \cdot r/d$. 

Since the time cost of any algorithm must be at least as great as the time for writing the output, which is as least as great as $\row$, 
the Virtual Join algorithm adds at most a logarithmic factor to this lower bound. 
This means that
%the number of table operations performed by Virtual Join algorithm is independent of the size of original data tables. It is almost linear in the number $\row$ of sufficient statistics that are to be computed. If 
if the number $\row$ of sufficient statistics is a reasonable bound on computational time and space, then computing the sufficient statistics is feasible. In our benchmark datasets, the number of sufficient statistics was feasible, as we report below. 
In Section~\ref{sec:conclusion} below we discuss options in case the number of sufficient statistics  grows too large.
% to be stored or computed.
%, one option is to use the Virtual Join algorithm with smaller s
%We conclude that the number of $\ct$-algebra operations is not the critical factor for scalability, but rather the cost of %processing tuples, 
%%or in other words, the cost of 
%carrying out a single $\ct$-algebra operation. 
%In Section~\ref{sec:cta} we discussed how to perform these operations in a scalable manner. 
%
%Here we give a quick proof of the identity ~\eqref{eq:upperbound} %or put it in the appendix
%\begin{proof}
%With the Binomial theorem, for any non-negative integers $l,m$, the binomial formula of two variables $a,b$ is 
%$$ f(a,b)=\sum_{\ell=0}^{m}\binom{m}{\ell} \cdot a^{m-\ell} \cdot b^\ell = (a+b)^m. $$
%Suppose we substitute $a$ with one, for any $b$, we have 
%$$
%f(1,b)= \sum_{\ell=0}^m\binom{m}{\ell} \cdot b^\ell=(1+b)^m.
%$$
%And then we could compute the partial derivative with respect to $b$
%$$
%f'_{b}(1,b)= \sum_{\ell=1}^m\binom{m}{\ell}\cdot \ell \cdot b^{\ell-1}=m \cdot (1+b)^{m-1}.$$
%By substituting $b$ with one, we have 
%$$
%f'_{b}(1,1)=  \sum_{\ell=1}^m\binom{m}{\ell}\cdot \ell \cdot 1^{\ell-1}= \sum_{\ell=1}^m\binom{m}{\ell}\cdot \ell = m\cdot(2)^{m-1}.$$
%
%That means the following identity
%$$\sum_{\ell=1}^{m} {m\choose \ell}\cdot \ell = m * 2^{m-1}  $$ holds.
%\end{proof}





%Thus the lattice structure defines a {\em multinet} rather than a single Bayes net. Multinets are a classic Bayes net formalism for modelling {\em context-sensitive} dependencies among variables. They have been applied  for modelling diverse domains, such as sleep disorders, eye diseases, and turbines that generate electricity. 
%Geiger and Heckerman contributed a standard reference article for the multinet formalism  \cite{Geiger1996}.

%\subsection{The Learn-and-Join Algorithm} \label{sec:laj}
%This Learn-and-Join Algorithm (LAJ)  takes as input a relational database and constructs a Bayes net for each relationship chain. The final Bayes net is the model associated with the largest chain. The LAJ algorithm was previously applied only conditional on all relationship variables being true. Because our virtual join algorithm provides sufficient statistics about negative relationships, we can extend the LAJ algorithm for learning a Bayes net model of dependencies among relationship variables. We provide a brief summary; for a detailed description please see \cite{Schulte2011}. The algorithm applies a standard single-table Bayes net learner, which can be chosen by the user, to the contingency table for each chain. 
%Computing the contingency tables is the conceptually and computationally most challenging part of our system. We describe a dynamic programming algorithm for precomputing the contingency tables in Section~\ref{sec:cta} below. In this section we describe the structure learning algorithm on the assumption that the contingency tables have been precomputed.

% The algorithm proceeds level-wise by considering relationship chains of length $\ell = 0,1, 2, \ldots$. After Bayes nets have been learned for length $\ell$, the learned edges are propagated to chains of length $\ell+1$. In the initial case of single relationship tables where $\ell=0$, the edges are propagated from Bayes nets learned for entity tables. %Propagated edge constraints are of two types: Required and forbidden edges. 
%In other words, the propagated edges in smaller relationship chains is inherited by larger relationship chains.
%
%Suppose that $\rchain$ at level $s$ is a parent of $\rchain'$ at level $s+1$. (A parent contains exactly one less relationship variable, see Figure~\ref{fig:big-lattice}.)  
%For required edges, if an edge $\variable \rightarrow \variable'$ is present in the Bayes net $\B_{\rchain}$ for the parent chain $\rchain$, the edge must also be contained in the Bayes net $\B_{\rchain'}$ for the child  chain $\rchain'$. 
%In other words, edges present in smaller relationship chains are inherited by larger relationship chains. 
%For forbidden edges, if an edge $\variable \rightarrow \variable'$ is absent from the Bayes nets of all parent chains of $\rchain'$, it must also be absent from the Bayes net of $\rchain'$. 
%In other words, the absence of edges in smaller relationship chains is inherited by larger relationship chains.
%
%{\em Example.} 
%If the $\student$ Bayes net contains an edge $$\it{intelligence}(\S) \rightarrow \it{ranking}(\S),$$ then the Bayes net associated with the relationship $\it{\ra(\S,\C)}$ must also contain this edge (see Figure~\ref{fig:university-tables}). 
%If the $\prof$ Bayes net does not contain an edge $$\it{popularity}(\P) \rightarrow \it{teachingability}(\P),$$ it must also be absent in the Bayes net associated with the relationship variable $\ra(\P,\S)$.
%%If the $\it{Course}$ Bayes net does not contain an edge $$\it{rating}(\C) \rightarrow \it{difficulty}(\C)$$, it must also be absent in the Bayes net associated with the relationship $\it{Registration(\S,\C)}$.

%
%\subsection{Implementation in SQL} 
%%\paragraph{The Model Manager: Implementing Hierarchical Model Search in SQL}
%%Figure~\ref{figure:flow} illustrates the system flow on part of the relationship lattice. The Bayes net learner receives a CT-table and edge constraints as input, and returns a Bayes net. The view mechanism propagates the results as constraints of learning to larger relationship chains, without the need for any further computation.
%
%The Bayes multinet can be stored in a master table called {\em $ChainBayesNet$}, as shown in Figure \ref{fig:general-flow-chart}. 
%An entry such as $$\langle [\reg(\S,\C),\ra(\S,\P)],\it{ranking}(\S),\it{intelligence}(\S)\rangle$$ means that the 
%Bayes net graph for the relationship chain $\reg(\S,\C),\ra(\S,\P)$ contains an edge $\it{ranking}(\S)\leftarrow \it{intelligence}(\S)$. 
%The Bayes net learner receives a $\ct$-table and edge constraints as input, and returns a Bayes net. The learned edges are exported to the database tables. 
%An SQL view defines tables that store the forbidden and required edges: each time that new edges are inserted in the $ChainBayesNet$ table, 
%the view mechanism propagates the results as constraints of learning to larger relationship chains.
%%, without the need for any further computation. 
%The view mechanism provides a compact and flexible update procedure (only 20 lines of SQL code).
%Figure~\ref{fig:general-flow-chart} shows the computation flow for our Bayes net learning system. 
%After structure learning is complete, the Bayes net parameters can be easily computed from the contingency tables using maximum likelihood estimation, and stored in conditional probability tables in the BN database together with the $ChainBayesNet$ table.
%
%\begin{figure}[tbp]
%\begin{center}
%\resizebox{0.5\textwidth}{!}{
%\includegraphics[width=0.8\textwidth]{figures/general-flow-chart.png}
%}
%
%\caption{Bayes net structure learning with SQL.
%%Here the $DB$ denotes the original database, $DB\_RV$ denotes the database storing all random variables, 
%%$DB\_CT$ denotes the database containing all the contingency tables  and $DB\_BN$ storing all the learned Bayes Nets.
%\label{fig:general-flow-chart}}
%\end{center}
%\end{figure}
%%The SQL View mechanism propagates edges compactly, as shown in Figure~\ref{fig:view-pro}. 
%%%The advantage of the view mechanism is that each time that the Bayes net learner completes learning for a lattice point, the view  update mechanism propagates the results as constraints on learning for the children of the lattice point. 
%%The {\em LatticeRel} table records which relationship chains are parents of which others. This auxilliary table makes it easy to write a query that finds, for a given child chain, all the parent chains and inserts their edges as entries for the child chain. This view implements the propagation of required edges. The view definition for forbidden edges is similar: the only difference is that we first need to find the set of complement edges that are {\em not} included in the Bayes net of a given relationship chain. The complement edges are also computed by a view (not shown). 


\section{Evaluation of Contingency Table Computation} 

We describe the system and the datasets we used.
Code was written in Java, JRE 1.7.0.  and executed with 8GB of RAM and a single Intel Core 2 QUAD Processor Q6700 with a clock speed of 2.66GHz (no hyper-threading). The operating system was Linux Centos 2.6.32. 
The MySQL Server version 5.5.34 was run with 8GB of RAM and a single core processor of 2.2GHz. 
All code and datasets are available on-line (pointer omitted for blind review). 
%~\cite{bib:jbnsite}.\textbf{check blind review}
%We made use of the following single-table Bayes Net search implementation:  GES search \cite{Chickering2003} with the BDeu score as implemented in version 4.3.9-0 of CMU's Tetrad package (structure prior uniform, ESS=10; \cite{2008a}).
%
\begin{table}[hbtp] \centering
%\scalebox{0.7in}{
\resizebox{0.5\textwidth}{!}{
\begin{tabular}[c]
{|l|c|c|r|c|}\hline
 \textbf{Dataset} & \textbf{\begin{tabular}[l] {ll} \#Relationship \\Tables/ Total \end {tabular}} & \textbf{\begin{tabular}[l] {ll} \#Self \\Relationships\end {tabular}}  & \textbf{\#Tuples} & \textbf{\#Attributes}  \\\hline
% \textbf{Dataset} & \textbf{Relationships} & \textbf{\begin{tabular}[l] {ll} Self \\Relationships \end {tabular}} &
% \textbf{\begin{tabular}[l] {ll} Same Type\\ Relationships \end {tabular}}& \textbf{\#Tuples} & \textbf{\begin{tabular}[l] {ll} \#Attribute  \\Columns \end{tabular}}  \\\hline
 %   University&2 & 0 & N & 171 & 12\\\hline
    Movielens &1 / 3 & 0  & 1,010,051 & 7\\\hline
%    Movielens(0.1M) &1 & N & N &  83,402 & 7\\\hline
    Mutagenesis & 2 / 4 & 0 & 14,540 & 11\\\hline
    Financial &3 / 7 & 0  &  225,932& 15\\\hline
   Hepatitis &3 / 7 & 0 &12,927  & 19\\\hline
   IMDB &3 / 7 & 0 &1,354,134  & 17\\\hline
    Mondial &2 / 4 & \textbf{1} &  870& 18\\\hline
    UW-CSE &2 / 4 & \textbf{2}  & 712 & 14\\\hline   
\end{tabular}
}
 % end scalebox
\caption{Datasets characteristics. \#Tuples = total number of tuples over all tables. 
  \label{table:datasetsize}}
\end{table}

\subsection{Datasets}
%\emph{Datasets.}
We used seven benchmark real-world databases,  described by Schulte and Khosravi~\cite{Schulte2012}. See that article for more details and the sources of the databases. Table~\ref{table:datasetsize} summarizes basic information about the benchmark datasets.  A  self-relationship %\cite{Heckerman+al:SRL07} 
relates two entities of the same type (e.g. $\it{Borders}$ relates two countries in Mondial). Random variables for each database were defined as described in Section~\ref{sec:variables} (see also \cite{Schulte2012}). IMDB is the largest dataset in terms of number of total tuples (more than 1.3M tuples)
and schema complexity. %attributes.
Our dataset combines the MovieLens database with data from the Internet Movie Database (IMDB)\footnote{www.imdb.com, July 2013} following \cite{Peralta2007}.

%The databases are fairly complex, so the experiments are computationally demanding, especially the Alchemy inference component, which needs to be applied to all groundings of all descriptive attributes to compute average predictive performance. The databases and their main characteristics are as follows. 
% and on-line sources such as \cite{bib:jbnsite}.
%In this paper we report the average result over all subdatabases in this paper and leave the evaluation of how models should evolve based on the size of data to an extension of the work in a journal paper. 
%
%
%\noindent\textbf{University Database.} We manually created a small dataset, based on the schema given in Figure~\ref{fig:university-schema}.% omitting the $\it{Teaches}$ relationship for simplicity. 
%The dataset is small and is used as a testbed for the correctness of our algorithms.
%
%\noindent\textbf{MovieLens.} 
%A dataset from the GroupLens\footnote{www.grouplens.org}. 
%%The data are organized in 3 tables (2 entity tables, 1 relationship table). %, and 7 descriptive attributes). 
%The Rated table contains Rating as descriptive attribute.
%%; more than 1 million ratings are recorded.  
%%We performed a preliminary data analysis following \cite{Schulte2012}. 
%%
%%This is a standard dataset from the UC Irvine machine learning repository.  \textbf{really? I can NOT find it online.}
%%%user:6039; Movie: 3883; rating: 1000129
%%%age, 7 bins based on the original MovieLens design
%% \cite{Schulte2012}.
%%
%%It contains two tables representing entity sets: User with 6,039 tuples and Item (Movies) with 3,883 tuples.
%%The User table has 2 descriptive attributes, $\age$ and $\it{gender}$. We discretized the attribute $\age$ into three equal-frequency bins. 
%%The table Item represents information about the movies. 
%%
%%There is one relationship table Rated corresponding to a Boolean predicate. 
%
%\noindent\textbf{Mutagenesis.}  This dataset is widely used in Inductive Logic Programming research.% \cite{Srinivasan1996}. %It contains 4 tables total to 15218 tuples. 
%It contains 2 entity tables and 2 relationships.
%%We used a previous discretization \cite{Schulte2012}.
%%Mutagenesis has two entity tables, Atom with 3 descriptive attributes, and Mole (decribing molecules), with 5 descriptive attributes. 
%%%including two attributes that are discretized into ten values each (logp and lumo).
%%There are two relationship tables, MoleAtom, indicating which atoms are parts of which molecules, and Bond, which relates two atoms and has 1 descriptive attribute. 
%
%\noindent\textbf{Financial.} 
%%This dataset stores the loan information about the clients at the associated banks.
%This dataset is a modified version of the financial dataset from the discovery challenge at PKDD'99 %We adapted the database design to fit the ER model 
%by following the modification from CrossMine~\cite{Yin2004}.% and Graph-NB~\cite{han2009}. 
%The data are organized in 7 tables (4 entity tables, 3 relationship tables).% with 15 descriptive attributes in total).
%
%\noindent\textbf{Hepatitis.} This dataset is a modified version of the PKDD02 Discovery Challenge database.% \cite{Frank2007}. %, which includes removing tests with null values. 
%%The database contains information on laboratory examinations of 771 hepatitis B- and C-infected patients, taken between 1982 and 2001.
%The data are organized in 7 tables (4 entity tables,  3 relationship tables) .%with 16 descriptive attributes. 
%%They contain basic information about the patients, results of biopsy, information on interferon therapy, results of out-hospital examinations, and results of in-hospital examinations. 
%
%\noindent\textbf{IMDB.} %, combination?.}
%This is the largest dataset in terms of number of total tuples (more than 1.3M tuples)
%and schema complexity. %attributes.
%The dataset combines the MovieLens database with data from the Internet Movie Database (IMDB)\footnote{www.imdb.com, July 2013} \cite{Peralta2007}.
%%We performed a very  similar preliminary data processing with MovieLens dataset \cite{Peralta2007}.
%%Two more entity tables representing actors and directors were introduced, and we added more related information as descriptive attributes about the movies from IMDB. 
%%In spite of the rating table, we added another two relationship tables. One is to store who directed the movie with genre information, and the other to store which actors participated in which movies.
%The schema contains 4 entity tables and 3 relationship tables.
%%And also includes 3 relationship tables to represent the 5 star rating score given per movie, which actors are involved in given movie and who directed that movie. 
%%4 entity tables : actors (gender 2, quality 6: 98690 ),directors(quality 6,avg\_revenue 5:2201),movies(year 4,country 4,runningtime 4:3832),users(age 7,gender 2,occuption 5 :6039)
%%and 3 relation tables: u2base(rating 5: 996159), movies2actors(cast\_num 4:138349), movies2directos(genre 9:4141)
%
%
%
%\noindent\textbf{Mondial.} %A geography database, featuring one self-relationship, $\it{Borders}$, that indicates which countries border each other. 
%This dataset is a geography database, and contains data from multiple geographical web data sources. 
%%We follow the modification of She~\cite{wangMondial}, and use a subset of the tables and discretized features. %: 2 entity tables, $\it{Country},\it{Economy}$. 
%%The descriptive attributes of Country are continent, government, percentage, majority religion, population size. The descriptive attributes of Economy are inflation, gdp, service, agriculture, industry. 
%%Another relationship table is  Economy\_Country specifying which country has what type of economy. %A self-relationship table Borders relates two countries.
%  %$\it{Country},\it{Continent},\it{Economy},\it{Government}$, where the latter three are related to Country by many-one relationships, and one relationship table $\it{Borders}$ that relates two countries. 
%It includes a self-relationship table Borders that relates two countries.
%
%
%\noindent\textbf{UW-CSE.}
%This dataset lists facts about the Department of Computer Science and Engineering at the University of Washington.
%%~\cite{Domingos2007}. 
%There are 2 entity tables % (i.e., $Person$, $Course$) 
%and 2 relationship tables; %($AdvisedBy$, $TaughtBy$)
%both are person-person self-relationships. 
%%The total number of ground atoms is 4,106,841. The database contained a total of 3380 ground atoms. 



%\begin{table}[btp] \centering
%%\scalebox{0.7in}{
%\resizebox{0.5\textwidth}{!}{
%\begin{tabular}[c]
%{|l|c|c|c|r|r|}\hline
% \textbf{Dataset} & \textbf{\#Relations} & \textbf{Self} &
% \textbf{SameType}& \textbf{\#Tuples} & \textbf{\#Attribute}  \\\hline
%% \textbf{Dataset} & \textbf{Relationships} & \textbf{\begin{tabular}[l] {ll} Self \\Relationships \end {tabular}} &
%% \textbf{\begin{tabular}[l] {ll} Same Type\\ Relationships \end {tabular}}& \textbf{\#Tuples} & \textbf{\begin{tabular}[l] {ll} \#Attribute  \\Columns \end{tabular}}  \\\hline
% %   University&2 & 0 & N & 171 & 12\\\hline
%    Movielens &1 & 0 & N & 1,010,051 & 7\\\hline
%%    Movielens(0.1M) &1 & N & N &  83,402 & 7\\\hline
%    Mutagenesis &2 & 0 & \textbf{Y} & 14,540 & 11\\\hline
%    Financial &3 & 0 & N &  225,932& 15\\\hline
%   Hepatitis &3 & 0 & N &12,927  & 19\\\hline
%   IMDB &3 & 0 &N &1,354,134  & 17\\\hline
%    Mondial &2 & \textbf{1} & N &  870& 18\\\hline
%    UW-CSE &2 & \textbf{1} & N & 712 & 14\\\hline
%   
%\end{tabular}
%}
% % end scalebox
%\caption{Real datasets characteristics, including size of datasets in total number of table tuples. 
% \# Attribute should not count the $Rvariables$ ? Here Self means self relationship, and type means same type relationship or not.
% \label{table:datasetsize}}
%\end{table}


%database contains a self-relationship when there is a 
%Basically, there is a random variable for each attribute and relationship. A complication arises when the database contains a self-relationship %\cite{Heckerman+al:SRL07} 
%that relates two entities of the same type. If the schema contains a self-relationship, we add one first-order variable for each role in the relationship, and one set of $\eatts$ for each first-order variable. 
%%%, when we need to introduce more than one relationship variable for the relationship functor. 
%For example, the Mondial database contains a self-relationship $\it{Borders}$ that relates two countries. 
%In the ER diagram, there are two lines from the $\it{Countries}$ entity set to the $\it{Borders}$ relationship set 
%The ring structure may be represented with two different first-order variables that each refer to the $\it{Countries}$ entity set. 
%The corresponding relationship variable is $\it{Borders}(\C_{1},\C_{2})$. The different positions of the first-order variables can represent different roles in the self-relationship.
%There are two $\eatts$, namely $\it{continent}(\C_{1})$ and $\it{continent}(\C_{2})$. For more details please see \cite{Schulte2012}.




\subsection{Contingency Tables With Negative Relationships}


\begin{table} \centering
%\scalebox{0.7in}{
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|l|r|r|r|r|r|}\hline 
 \textbf{Dataset} & \textbf{VJ-time}(s) & $\textbf{SQL-time}(s)$&\textbf{Cross Product}  & \textbf{\#Statistics} & \textbf{\begin{tabular}{l}Compress \\Ratio\end{tabular}} \\\hline
Movielens &2.70&703.99 &23M &252 &93,053.32\\\hline
Mutagenesis &1.67&1096.00 & 1M &1,631 &555.00  \\\hline
Financial &  1421.87&N.T. &149,046,585M &3,013,011 &49,467,653.90   \\\hline
Hepatitis &3536.76&N.T. &17,846M& 12,374,892 &1,442.19 \\\hline
IMDB &7467.85&N.T. &5,030,412,758M&15,538,430 & 323,740,092.05 \\\hline
Mondial &1112.84&132.13&5M&1,746,870&2.67  \\\hline
UW-CSE &3.84&350.30& 10M&2,828 & 3,607.32\\\hline

\end{tabular}
}
 % end scalebox
\caption{Constructing the contingency table for each dataset. 
%Computation times are given in seconds. 
M = million. N.T. = non-termination.
  \label{table:cttimes}}
\end{table}

Table~\ref{table:cttimes} reports measurements about our Virtual Join (VJ) algorithm for constructing the joint contingency tables for all variables together, for each database. 
We compare the VJ algorithm with using an SQL query to construct the contingency table with negative relationships. Cross-checking the VJ contingency tables with the SQL contingency tables confirmed the correctness of our implementation. The SQL query materializes the cross product of the entity tables for each first-order variable (primary keys).  
%The size of this cross product is identical to the sum of counts in the $\ct$-table, reported in Table~\ref{table:cttimes}. 
The ratio of the cross product size to the number of statistics in the $\ct$-table measures how much compression the $\ct$-table provides compared to enumerating the cross product. The $\ct$-table provides a substantial compression of the statistical information in the database, by a factor of over 4,500 for the largest database IMDB. 

{\em Computation Time.} The numbers shown are the complete computation time for all statistics. For faster processing, both methods used an extra B+tree index built on each column in the original dataset. The VJ method also utilized B+ indexes on the $\ct$-tables; we include the cost of building these indexes in the reported time. 
%
% (we did not include the time for building the indexes). 
%
The Virtual Join algorithm returned a contingency table with negative relationships in feasible time. On the biggest dataset IMDB with 1.3 million tuples, it took just over 2 hours. 
%Figure~\ref{fig:runtime-vj} shows the near linear time consumption of the VJ algorithm, as a function of the number of negative relationships statistics that it computes. 

The SQL query did not always terminate, crashing after around 4, 5, and 10 hours on Financial, IMDB and Hepatitis respectively. When the SQL query did terminate, it took orders of magnitude longer than the VJ method except for the Mondial dataset. Generally the higher the compression ratio, the higher the time savings. On Mondial the compression ratio is especially low, so materializing the cross-product was faster. We also tried a more complex SQL query that combines the cross-product (in the FROM clause) with the Count aggregate function and Group By on the variables. This ran even more slowly because it requires counting in addition to processing the cross-product. 
\begin{table}[htbp]
  \centering
%    \begin{tabular}{|l|R{2.5cm}|R{2.5cm}|}    \hline
%    Dataset & Link Analysis On & Link Analysis Off\\    \hline
%    MovieLens & 252   & 210 \\    \hline
%    Mutagenesis & 1,631 & 565 \\    \hline
%    Financial & 3,013,011 & 8,733\\    \hline
%    Hepatitis & 12,374,892 & 2,487\\    \hline
%    IMDB  & 15,538,430 & 1,098,132 \\    \hline
%    Mondial & 1,746,870 & 0 \\    \hline
%    UW-CSE & 2,828 & 2 \\    \hline
%\end{tabular}%
% Table generated by Excel2LaTeX from sheet 'cikm'
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|l|r|r|r|r|}\hline
Dataset & \multicolumn{1}{r|}{Link On} & \multicolumn{1}{c|}{Link Off} & \multicolumn{1}{c|}{\#extra  statistics} & \multicolumn{1}{c|}{extra time (s)}    \\ \hline
MovieLens & 252   & 210   & 42    & 0.27    \\ \hline
Mutagenesis & 1,631 & 565   & 1,066 & 0.99    \\ \hline
Financial & 3,013,011 & 8,733 & 3,004,278 & 1416.21    \\ \hline
Hepatitis & 12,374,892 & 2,487 & 12,372,405 & 3535.51    \\ \hline
IMDB  & 15,538,430 & 1,098,132 & 14,440,298 & 4538.62    \\ \hline
Mondial & 1,746,870 & 0     & 1,746,870 & 1112.31    \\ \hline
UW-CSE & 2,828 & 2     & 2,826 & 3.41    \\ \hline
\end{tabular}%

}
  \caption{Number of Sufficient Statistics for Link Analysis On and Off.}
  \label{table:link-onoff}%
\end{table}%

\begin{figure}[htbp]
\begin{center}
\resizebox{0.4\textwidth}{!}{
\includegraphics[width=0.6\textwidth]{figures/extra_time.png}
}

\caption{The VJ Algorithm Running Time (s)
\label{fig:runtime-vj}}
\end{center}
\end{figure}



%
%\begin{table} \centering
%%\scalebox{0.7in}{
%\resizebox{0.5\textwidth}{!}{
%\begin{tabular}
%{|l|r|r|r|r|r|}\hline 
% \textbf{Dataset} & \textbf{VJ} & $\textbf{SQL}$&\textbf{Sum(counts)}  & \textbf{\#Tuples} & \textbf{Compress Ratio} \\\hline
%University&1.68&0.05 & 2,280 & 351 &6.50 \\\hline
%Movielens &2.70&703.99 &23,449,437 &252 &93,053.32\\\hline
%%Movielens(0.1M) &0.62 & 1,582,762 &239 &6,622.44 \\\hline
%Mutagenesis &1.67&1096.00 & 905,205 &1,631 &555.00  \\\hline
%Financial &  1421.87&%12592.41
%N.T. &149,046,585,349,303 &3,013,011 &49,467,653.90   \\\hline
%Hepatitis &3536.76&%141944.16(
%N.T. &17,846,976,000 & 12,374,892 &1,442.19 \\\hline
%IMDB &7467.85&%17526.86(
%N.T. &5,030,412,758,502,710&15,538,430 & 323,740,092.05 \\\hline
%Mondial &1112.84&132.13&4,655,957&1,746,870&2.67  \\\hline
%UW-CSE &3.84&350.30& 10,201,488&2,828 & 3,607.32\\\hline
%
%\end{tabular}
%}
% % end scalebox
%\caption{Constructing the full contingency table for a database.  
%  \label{table:cttimes}}
%\end{table}
%


%The SQL query enumerates tuples of entities, whose total number is the sum of counts in the contingency table. The ratio of this sum to the number of tuples in the $\ct$-table measures how much compression the $\ct$-table provides compared to enumerating instantiations (cases).


\subsection{Contingency Tables with Negative Relationships vs. Positive Relationships Only} 

Here and below we use the following terminology. \textbf{Link Analysis On} refers to using a contingency table with sufficient statistics for both positive and negative relationships. 
An example is table $\ct$ in Figure~\ref{fig:flow}. 
\textbf{Link Analysis Off} refers to using a contingency table with sufficient statistics for positive relationships only. An example is table $\ct_{\true}^{+}$ %$\ct_{T^{+}}$
 in Figure~\ref{fig:flow}. Table~\ref{table:link-onoff} shows the  number of sufficient statistics required for link analysis on vs. off. The difference between the link analysis on statistics  and the link analysis statistics is the number of Extra Statistics.
The Extra Time column shows how much time the VJ algorithm requires to compute the Extra Statistics {\em after} the contingency tables for positive relationships are constructed using SQL joins. As Figure~\ref{fig:runtime-vj} illustrates, the Extra Time stands in a nearly linear relationship to the number of Extra Statistics, which confirms the analysis of Section~\ref{sec:complexity}. Figure~\ref{fig:breakdown-vj} shows that most of the VJ run time is spent on the Pivot component (Algorithm~\ref{alg:pivot}) rather than the main loop (Algorithm~\ref{alg:fmt}). In terms of $\ct$-table operations, most time is spent on subtraction/union rather than cross product.


\begin{figure}[htbp]
\begin{center}
\resizebox{0.4\textwidth}{!}{
\includegraphics[width=0.6\textwidth]{figures/sep_time.png}
}

\caption{Breakdown of VJ Running Time
\label{fig:breakdown-vj}}
\end{center}
\end{figure}
% Table generated by Excel2LaTeX from sheet 'cikm-new-tables'
%\begin{table}[htbp]
%  \centering
%  \caption{Add caption}
%    \begin{tabular}{|l|r|r|r|}
%    \hline
%    Dataset & \multicolumn{1}{c|}{Pivot} & \multicolumn{1}{c|}{Cross Product} & \multicolumn{1}{c|}{Total}\\
%    \hline
%    MovieLens & 87.05\% & 12.95\% & 0.14 \\
%    \hline
%    Mutagenesis & 85.85\% & 14.15\% & 1.05 \\
%    \hline
%    Financial & 93.77\% & 6.23\% & 1234.81 \\
%    \hline
%    Hepatitis & 91.87\% & 8.13\% & 3344.56 \\
%    \hline
%    IMDB  & 92.89\% & 7.11\% & 4472.75 \\
%    \hline
%    Mondial & 88.19\% & 11.81\% & 1301.50 \\
%    \hline
%    UW-CSE & 82.55\% & 17.45\% & 3.32 \\
%    \hline
%    \end{tabular}%
%  \label{tab:addlabel}%
%\end{table}%



{\em If} the link analysis off contingency table contains no zero count statistics, the number of Extra Statistics is at most $2^{m}-1$ times the number required for link analysis off, where $m$ is the number of relationships (cf. Equation~\eqref{eq:row-stats}). For example, the IMDB dataset contains three relationship variables (cf. Table~\ref{table:datasetsize}), so the link analysis on statistics increase by at most $2^{3} = 8$. However, the actual number is 15 times bigger. This shows that when all relationships are positive, many possible attribute combinations do not occur in IMDB. The reason for this is that not all entities participate in every relationship. Attribute information for these entities is lost conditional on the relationship being true. 
%In the university example of Figure~\ref{fig:big-lattice}, only a fraction of students may be an RA for a professor. For query counts where $\ra(\P,\S) = \true$, information from such students is lost. 
An extreme example is the Mondial dataset where all relationship variables are never simultaneously true.
%For more discussion of how joining existing relationship tables can lose information, please see \cite{Raedt1998}. 
In such cases there is a trade-off between turning the link analysis on and off: On the one hand, the number of extra sufficient statistics and hence the space storage requirements is bigger than one would expect from the number of relationships alone. On the other hand, turning link analysis off loses information not only about the distribution of relationships, but also about the distribution of attributes in the database. We next examine how this loss of information affects statistical learning and mining tasks.

\section{Statistical Applications} We evaluate using link analysis on three different types of cross-table statistical analysis: feature selection, association rule  mining, and learning a Bayesian network.

\subsection{Feature Selection} For each database, we selected a target for classification, then used Weka's CFS feature subset selection method (Version 3.6.7) to select features for classification \cite{Hall2009}. The idea is that if the existence of relationships is relevant to classification, then there should be a difference between the set selected with link analysis on and that selected with link analysis off. 
%Weka accepts instance weights as inputs, so its functions can be applied directly with contingency tables. 
We measure how different two feature sets are by 1-Jaccard's coefficient:
$$\it{Distinctness}(A,B) = 1- \frac{A \cap B}{A \cup B}.$$


\begin{table}[htbp] \centering
%%\scalebox{0.7in}{
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|l|R{2.2cm}|R{1.8cm}|R{2cm}|r|} \hline
{\multirow{2}[4]{*}{Dataset}} &{\multirow{2}[4]{*}{Target variable}} & \multicolumn{2}{c|}{\# Selected Attributes} & {\multirow{2}[4]{*}{Distinctness}}\\ \cline{3-4} 
 & & Link Analysis Off & Link Analysis On / Rvars & \\\hline
MovieLens & Horror(M) & 2 & 2 / 0 &  0.0 \\\hline
Mutagenesis & inda(M) & 3 & 3 / 0 & 0.0 \\\hline
Financial & balance(T) & 3 & 2 / 1 & 1.0 \\\hline
Hepatitis & sex(D) & 1 & 2 / 1 & 0.5 \\\hline
IMDB & avg\_revenue(D) & 5 & 2 / 1 & 1.0 \\\hline
Mondial & percentage(C) & Empty CT  & 4 / 0 & 1.0 \\\hline
UW-CSE & courseLevel(C) & 1 & 4 / 2 & 1.0 \\\hline
\end{tabular}
}
\caption{Selected Features for Target variables for  Link Analysis Off vs. Link Analysis On. Rvars denotes the number of relationship features selected. \label{table:feature-select}}
\end{table}

%MovieLens & Rated(User,Movie0) & n/a & ??? & n/a \\\hline
%Mutagenesis & inda(Molecule0) & 3 & 3/0 & 0.00 \\\hline
%Financial & balance(Transaction0) & 3 & 2/1 & 1.00 \\\hline
%Hepatitis & sex(Dispatch0) & 1 & 2/1 & 0.50 \\\hline
%IMDB & avg\_revenue(Director0) & 5 & 2/1 & 1.00 \\\hline
%Mondial & percentage(Country0) & empty CT table & 4/0 & 1.00 \\\hline
%UW-CSE & courseLevel(Course0) & 1 & 4/2 & 1.00 \\\hline

Distinctness measures how different the selected feature subset is with link analysis on and off, on a scale from 0 to 1. Here 1 = maximum dissimilarity.
%
Table~\ref{table:feature-select} compares the feature sets selected. 
In 4/7 datasets, the feature sets are disjoint (coefficient = 0). For the Mutagenesis and MovieLens data sets, no new features are selected. 
%On Mondial, there are no instances where all relationships are true. 
%On MovieLens, we selected a relationship indicator variable as the target for prediction, which is constantly true with link analysis off.\textbf{maybe this doesn't make sense for MovieLens?}
%
% \textbf{explain why for Mondial?} 
%{\em zqian: border(c0,c1), ecoR(c0,e0), ecoR(c1,e1); country0 and coutrny1 are bordered and has the same economy type??}
%
In almost all datasets, sufficient statistics about negative relationships generate new relevant features for classification.

%\textbf{OS: explain why we don't use classification?}


\subsection{Association Rules} A widely studied task is finding interesting association rules in a database. We considered association rules of the form $\it{body} \rightarrow \it{head}$, where $\it{body}$ and $\it{head}$ are conjunctive queries. 
%
%The multi-relational contingency tables allow an association rule miner to find rules that combine condition from different database tables. 
We searched for interesting rules using both the link analysis off and the link analysis on contingency tables for each database. The idea is that if a relationship indicator is relevant for other features, it should appear in an association rule. With link analysis off, all relationship indicators always have the value $\true$, so they do not appear in any association rule. We used Weka's Apriori implementation to search for association rules in both modes. The interestingness metric was Lift. Parameters were set to their default values. Table~\ref{table:association} shows the number of rules that utilize relationship indicators with link analysis on, out of the top 20 rules. In all cases, a majority of rules utilize relationship indicators, all of them in Mutagenesis and IMDB. An example of an association rule for Financial is 
%
$$\it{statement\_freq.(acc)} = \it{monthly} \rightarrow \it{HasLoan}(\it{acc},\it{Loan}) = \true.$$
%\textbf{false relationships? usually yes} 
%
\begin{table}[htbp] \centering
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Dataset} & MovieLens & Mutagenesis & Financial & Hepatitis & IMDB  & Mondial & UW-CSE \\
\hline
\# rules  & 14/20  & 20/20 & 12/20 & 15/20 & 20/20 & 16/20 & 12/20 \\ %invovle rvariable
\hline
\end{tabular}%
}
\caption{Number of top 20 Association Rules that use relationship indicator features.}
  \label{table:association}%
\end{table}%


%\textbf{How did we get these rules? Use population variable notation.}
%
%\begin{table}[htbp] \centering
%  \resizebox{0.5 \textwidth}{!} {
%    \begin{tabular}{|l|l|R{3.5cm}|r|r|}    \hline
%   {Dataset} &\multicolumn{1}{c|}{Rule Head} & \multicolumn{1}{c|}{$\leftarrow$ Rule Body} & {Confidence} & {Prior} \\    \hline
%    MovieLens & Rated(U,M)=T & action(M)=T, gender(U)=M & 0.07  & 0.03 \\    \hline
%    Mutagenesis & inda(M)=1 & logp(M)= 6 , Bond(M,A)=F, MoleAtom(M,A)=T    &    0.64   &0.03  \\    \hline
%    Financial & balance(T) = 0 &   operation(T) = remit, Loan\_Order(T,L)=T, Loan\_Trans(O,L)=T  & 0.76  & 0.66 \\    \hline
%   % Hepatitis & sex(D) &       &             &  \\    \hline
%    IMDB  & avg\_revenue(D)=4 &    quality(D)=5, Directed(M,D)=T, Rated(U,M)=T   &0.87       &0.18  \\    \hline
%    Mondial & percentage(C0)=3 &       percentage(C0) = 3, Border(C0,C1)=T, EcoR(C0,E0)= T   &0.35   &0.31  \\    \hline
%    UW-CSE & Student(P) = F &   InPhase(P)=F, TaughtBy(C,P)=T         &  1.0     &0.22  \\    \hline % deterministic rule
%    \end{tabular}%
%}
%
%
%\caption{Association Rules: Hepatitis, no special rules}
%  \label{tab:addlabel}%
%\end{table}%



%\begin{align}
% \it{action(Movie)} = \true, \it{gender(User)} = \it{Man} \rightarrow \it{Rated}(\it{User},\it{Movie}) = \true \\
% \it{confidence} = 0.07, \it{prior} = 0.03.
%\end{align}
%
%\begin{align}
%\it{a} = \false, \it{b} = \false, \it{logp(Mole)} = 5  \rightarrow \it{inda}(\it{Mole}) = 0.\\
%\it{confidence} = 1.0, \it{prior} = 0.03. [check this] %zqian:typo
%\end{align}

%\begin{align}
%\it{a} = \false, \it{b} = \true, \it{logp(Mole)} = 6  \rightarrow \it{inda}(\it{Mole}) = 1.\\
%\it{confidence} = 0.64, \it{prior} = 0.03. 
%\end{align}


%\begin{align}
%\it{b} = \true, \it{c} = \true, \it{operation}(\it{Transaction}) = \it{remit}  \rightarrow \it{balance}(\it{Transaction}) = 0.\\
%\it{confidence} = 0.76, \it{prior} = 0.66.
%\end{align}
%
%\begin{align}
%\it{b} = \true, \it{c} = \true, \it{quality}(\it{Director}) = 5  \rightarrow \it{avg\_revenue}(\it{Director}) = 4.\\
%\it{confidence} = 0.87, \it{prior} = 0.18.
%\end{align}

%\begin{align}
%\it{a} = \true, \it{b} = \true  \rightarrow \it{percentage}(\it{Country}) = 3.\\
%\it{confidence} = 0.35, \it{prior} = 0.31.
%\end{align}

%\begin{align}
%\it{b} = \true, \it{InPhase(Person)} = \false \rightarrow \it{Student}(\it{Person}) = \false.\\
%\it{confidence} = 1.0, \it{prior} = 0.22.
%\end{align}

%
%\begin{figure*}[htbp] %  figure placement: here, top, bottom, or page
%   \centering
%
%\includegraphics[width=7in,height=0.3\textheight]{unielwin.png}
%\includegraphics[width=7in,height=0.3\textheight]{movielens.png}
%\includegraphics[width=7in,height=0.3\textheight]{muta.png}
%
%  \caption{Real Bayes Net Examples:.}
%   \label{fig:BN_Examplse}
%\end{figure*}

\subsection{Learning Bayesian Networks}

Our most challenging application is constructing a Bayesian network for a relational database. For single-table data, Bayesian network learning has been considered as a benchmark application for precomputing sufficient statistics \cite{Moore1998,lv2012}. A Bayesian network structure is a directly acyclic graph whose nodes are random variables. Given an assignment of values to its parameters, a Bayesian network represents a joint distribution over both attributes and relationships in a relational database. Several researchers have noted the usefulness of constructing a graphical statistical model for a relational database ~\cite{Graepel_CIKM13,Wang2008}.
%, for instance for exploratory data analysis and dealing with uncertainty.
% because computing sufficient statistics is the key factor in scaling Bayes net learning to large datasets. 
For data exploration, a Bayes net  model provides a succinct graphical representation of complex statistical-relational correlations. The model also supports probabilistic reasoning for answering ``what-if'' queries about the probabilities of uncertain outcomes conditional on observed events. 

We used the previously existing learn-and-join method (LAJ), which is the state of the art for Bayes net learning in relational databases \cite{Schulte2012}. The LAJ method takes as input a contingency table for the entire database, so we can apply it with both link analysis on and link analysis off to obtain two different Bayes net structures for each database. Our experiment is the first evaluation of the LAJ method with link analysis on. We used the LAJ implementation provided by its creators.
%
%\begin{description}
%\item[Link Analysis On ] The learn-and-join algorithm applied to a contingency table with both positive and negative relationships.
%\item[Link Analysis Off ] The learn-and-join algorithm applied to a contingency table with positive and relationships only.
%\item[Complete Graph] Fully connected DAG. The complete graph has the maximum number of parameters.
%\item[Disconnected Graph] All nodes are disconnected. 
%\end{description}
%
We score all learned graph structures using the same full contingency table with link analysis on, so that the scores are comparable. The idea is that turning link analysis on should lead to a different structure that represents correlations, involving relationship variables, that exist in the data.


\subsubsection{Structure Learning Times} 
Table~\ref{table:runtimes} provides the model search time for structure learning with link analysis on and off. Both learning methods are fast, even for the largest contingency table  IMDB (less than 10 minutes run-time). With link analysis on, structure learning takes more time as it processes more information. 
%This does not include the time for computing table joins since this is essentially the same for all methods (the cost of the full join table). 
%On the smaller and simpler datasets, all search strategies are fast, but on the medium-size and more complex datasets (Hepatitis, MovieLens),%hierarchical search 
%%LAJ is much faster due to its use of constraints. 
In both modes, the run-time for building the contingency tables (Table~\ref{table:cttimes}) dominates the structure learning cost.
%\begin{table} \centering
%%\scalebox{0.7in}{
%\resizebox{2.5in}{!}{
%\begin{tabular}[c]
%{|l|R{2cm}|R{2cm}|R{2cm}|}\hline
% \textbf{Dataset}  & \textbf{LAJ+} & \textbf{LAJ}  &\textbf{Complete} \\\hline
%Movielens & 1.53& ? &  0.10 \\\hline
%Mutagenesis & 1.78& & 0.11\\\hline
%Financial  &96.31&  & 0.20 \\\hline
%Hepatitis   & 416.70& & 0.27 \\\hline
%IMDB   & 551.64 & & 0.28 \\\hline
%Mondial & 190.16& &0.28 \\\hline
%UW-CSE & 2.89& &0.18\\\hline
%\end{tabular}
%} % end scalebox
%\caption{Model Structure Learning Time  in seconds.  N.T. = nontermination.
%%[2 decimals only]
%% \textbf{Zhensong: may show some realy SQL queries?}
% \label{table:runtimes}}
%\end{table}

\begin{table} \centering
%\scalebox{0.7in}{
\resizebox{2.5in}{!}{
\begin{tabular}[c]
{|l|R{3cm}|R{3cm}|}\hline
 \textbf{Dataset}  & \textbf{Link Analysis On } & \textbf{Link Analysis Off } \\\hline
Movielens & 1.53&1.44 \\\hline
Mutagenesis & 1.78&1.96 \\\hline
Financial  &96.31& 3.19 \\\hline
Hepatitis   & 416.70& 3.49\\\hline
IMDB   & 551.64 & 26.16 \\\hline
Mondial & 190.16&N/A\\\hline
UW-CSE & 2.89&2.47 \\\hline
\end{tabular}
} % end scalebox
\caption{Model Structure Learning Time  in seconds.  %N.T. = nontermination.
%[2 decimals only]
 \label{table:runtimes}}
\end{table}
%Adding prior knowledge as constraints could speed the structure learning substantially.
%The reason for w LAJ+ starts with the previous LAJ method as the first phase. The edges among attributes that are discovered in the first phase are treated as fixed background knowledge in the second phase. 


%\begin{table} \centering
%%\scalebox{0.7in}{
%\resizebox{0.5\textwidth}{!}{
%\begin{tabular}[c]
%{|l|r|r|r|r|r|}\hline
% \textbf{Dataset}  & \textbf{BBH} & \textbf{Flat} & \textbf{Compl.} & \textbf{Discon.}\\\hline
%%University&1.523&1.486& 0.186 &0.135 \\\hline
%Movielens & 1.53&0.96& 0.10&0.06 \\\hline
%%Movielens(0.1M) &1.178& 0.986& 0.083&0.065 \\\hline
%Mutagenesis & 1.78&1.86& 0.11&0.09 \\\hline
%Financial  &96.31& 1,241.07& 0.20&0.08 \\\hline
%Hepatitis   & 416.70& N.T.& 0.27&0.13 \\\hline
%IMDB   & 551.64 & N.T.&0.28 &0.22 \\\hline
%Mondial & 190.16&1,289.53&0.28 &0.09 \\\hline
%UW-CSE & 2.89&2.36&0.18 &0.10 \\\hline
%\end{tabular}
%}
% % end scalebox
%\caption{Model Structure Learning Time  in seconds. 
%%[2 decimals only]
%% \textbf{Zhensong: may show some realy SQL queries?}
% \label{table:runtimes}}
%\end{table}

%\paragraph{Performance Metrics }
%\paragraph{Statistical Scores}
\subsubsection{Statistical Scores.}
%\textbf{Pseudo Relation Model Selection Scores  \cite{Schulte2012} ?}, advantages \cite{Schulte2013} ?
%We report learning time, log-likelihood, Bayes Information Criterion (BIC), and the Akaike Information Criterion (AIC).
% BIC and AIC are standard scores for Bayes nets \cite{Chickering2003}, defined as follows.
We report two model metrics, the log-likelihood score, and the model complexity as measured by the number of parameters. The \textbf{log-likelihood} is denoted as $L(\hat{G},\d)$, where $\hat{G}$ is the BN $\G$ with its parameters instantiated to be the maximum likelihood estimates given the dataset $\d$, and the quantity $L(\hat{G},\d)$ is the log-likelihood of $\hat{G}$ on $\d$. 
We use the relational log-likelihood score defined in \cite{Schulte2011}, 
which differs from the standard single-table Bayes net  likelihood %\cite{Chickering2003} 
only by replacing counts by frequencies  so that scores are comparable across different nodes and databases. 
%The complete graph has the maximum number of parameters and the  highest likelihood score. The disconnected graph has the minimum number of parameters and the lowest likelihood score. 
%So these extreme graphs provide a benchmark for both model metrics.  
%
To provide information about the qualitative graph structure learned, we report edges learned that point to a relationship variable as a child. Such edges can be learned only with link analysis on. We distinguish edges that relationship variables---R2R---and that link attribute variables to relationships---A2R.
%These include direct dependencies between relationship indicator variables on three datasets, and correlations between attributes and relationships where parent variable attributes predict child variable relationships. 
%An edge with a relationship variable as child will not be selected by a statistical method if the relationship variable has the constant value ``true'', as is the case when the sufficient statistics are for positive relationships only. 
%\section{Empirical Evaluation: Statistical Scores}

% Table generated by Excel2LaTeX from sheet 'cikm'
% Table generated by Excel2LaTeX from sheet 'cikm'

\begin{table}[htb] 
%\begin{center}
%\resizebox{0.5 \textwidth}{!}{
%\begin{tabular}{|l|r|r|r|r| }
%		\hline \textbf{University} &{Pseudo\_BIC}& {Pseudo\_AIC} &{Norm\_log-likelihood} &{\# Para.}\\
%			\hline BBH &-554.01 & -99.50 & -5.50& 94\\
%			\hline Flat  & -3731.88 & -668.20 & -5.20& 663\\
%			 \hline Complete &-386566.28  & -50000.10& -5.10& 49995 \\
%		        \hline Disconnected &-121.44 &-31.89 &-8.89 &23 \\
%			\hline
%		\end{tabular}
%}
%\end{center}
% Table generated by Excel2LaTeX from sheet 'cikm'

\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|m{2.5cm}|R{2cm}|R{1.8cm}|R{1cm}|R{1cm}| }
\hline \textbf{Movielens } &{log-likelihood} &{\#Parameter}& {R2R}&{A2R}\\
\hline
Link Analysis Off    & -4.68 & \textbf{164} & 0     & 0 \\
\hline
Link Analysis On  & \textbf{-3.44} & 292   & 0     & 3 \\
\hline
%Complete & -3.44 & 675   & 0     & 0 \\
%\hline
%Disconnected & -4.31 & 15    & 0     & 0 \\
%\hline
		\end{tabular}
}
\end{center}

%
%\begin{center}
%\resizebox{0.5 \textwidth}{!}{
%\begin{tabular}{|l|r|r|r|r| }
%		\hline \textbf{Movielens(0.1M) } &{Pseudo\_BIC}& {Pseudo\_AIC} &{Norm\_log-likelihood} &{\# Para.}\\
%			\hline BBH &-1198.42&-87.10& -3.10&84\\
%			\hline Flat &-1691.20&-123.10 &-3.10& 120\\
%			 \hline Complete &-4160.11  &-294.09 &-3.09 &291 \\
%		        \hline Disconnected &-120.88 &-14.34 & -3.34&11 \\
%			
%			\hline
%		\end{tabular}
%}Mutagenesis
%\end{center}
\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|m{2.5cm}|R{2cm}|R{1.8cm}|R{1cm}|R{1cm}|  }\hline 
\textbf{Mutagenesis } &{log-likelihood} &{\#Parameter}& {R2R}&{A2R}\\	 \hline 
Link Analysis  Off   & -6.18 & \textbf{499} & 0     & 0 \\ \hline
Link Analysis  On  & \textbf{-5.96} & 721   & 1     & 5  \\ \hline
%Complete & -5.59 & 423,369 & 1     & 0 \\ \hline
%Disconnected & -7.91 & 36    & 0     & 0 \\ \hline
		\end{tabular}
}
\end{center}
%Financial
\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|m{2.5cm}|R{2cm}|R{1.8cm}|R{1cm}|R{1cm}|  }
\hline \textbf{Financial } &{log-likelihood} &{\#Parameter}& {R2R}&{A2R}\\	  \hline
Link Analysis Off   & -10.96 & 11,572 & 0     & 0 \\ \hline
Link Analysis On  & \textbf{-10.74} & \textbf{2433} & 2     & 9 \\ \hline
%Complete & -10.67 & 374,399,999 & 3     & 0 \\ \hline
%Disconnected & -12.79 & 49    & 0     & 0 \\ \hline
		\end{tabular}
}
\end{center}	

\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|m{2.5cm}|R{2cm}|R{1.8cm}|R{1cm}|R{1cm}|  }
\hline \textbf{Hepatitis  } &{log-likelihood} &{\#Parameter}& {R2R}&{A2R}\\	  \hline
Link Analysis Off   & \textbf{-15.61} & 962   & 0     & 0 \\ 			\hline
Link Analysis On  & -16.58 & \textbf{569} & 3     & 6 \\ 			\hline
%Complete & NT    & NT    & 3     & 0 \\ 			\hline
%Disconnected & -18.30 & 58    & 0     & 0 \\			\hline
		\end{tabular}
}
\end{center}

\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|m{2.5cm}|R{2cm}|R{1.8cm}|R{1cm}|R{1cm}|  }
\hline \textbf{IMDB  } &{log-likelihood} &{\#Parameter}& {R2R}&{A2R}\\	  \hline
Link Analysis Off    & -13.63 & 181,896 & 0     & 0 \\ 		\hline
Link Analysis On  & \textbf{-11.39} & \textbf{60,059} & 0     & 11 \\ 		\hline
%Complete & N.T.   & N.T.    & 3     & 0 \\ 		\hline
%Disconnected & -12.01 & 54    & 0     & 0 \\		\hline
		\end{tabular}
}
\end{center}

\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|m{2.5cm}|R{2cm}|R{1.8cm}|R{1cm}|R{1cm}|  }
\hline \textbf{Mondial  } &{log-likelihood} &{\#Parameter}& {R2R}&{A2R}\\	  \hline
Link Analysis Off   &   N/A    &  N/A     & N/A     & N/A \\ 		\hline
Link Analysis On& -18.2 & 339   & 0     & 4 \\ 		\hline
%Complete & N.T.    & N.T.    &     3  & 0 \\ 		\hline
%Disconnected & -19.98 & 55    & 0     & 0 \\		\hline
		\end{tabular}
}
\end{center}

\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|m{2.5cm}|R{2cm}|R{1.8cm}|R{1cm}|R{1cm}|  }
\hline \textbf{UW-CSE   } &{log-likelihood} &{\#Parameter}& {R2R}&{A2R}\\	  \hline
Link Analysis Off   & \textbf{-6.68} & 305   & 0     & 0 \\  \hline
Link Analysis On & -8.13 & \textbf{241} & 0     & 2 \\  \hline
%Complete & -6.02 & 22,118,399 & 3     & 0 \\  \hline
%Disconnected & -10.24 & 45    & 0     & 0 \\ \hline
		\end{tabular}
}
\end{center}
\caption{Comparison of Statistical Performance of Bayesian Network Learning.
}
\label{table:result_scores}
\end{table}

%\paragraph{Discussion} 



Structure learning can use the new type of dependencies to find a better, or at least different, trade-off between model complexity and model fit.
% than the baselines, whether link analysis is used or not. 
%A comparison on Mondial is not possible because there are no instances where all relationships are true. 
On two datasets (IMDB and Financial), link analysis leads to a superior model that achieves better data fit with fewer parameters. These are also the datasets with the most complex relational schemas (see Table~\ref{table:datasetsize}). On IMDB in particular, considering only positive links leads to a very poor structure with a huge number of parameters.
%, yet worse data fit than the disconnected graph. 
On four datasets, extra sufficient statistics lead to different trade-offs: On MovieLens and Mutagenesis, link analysis leads to better data fit but higher model complexity, and the reverse for Hepatitis and UW-CSE. 


%In conclusion, link analysis allows Bayesian network learning to discover qualitatively new assocations involving the distribution of relationships. In some cases, the new associations lead to a structure with superior statistical metrics in both data fit and complexity. In other case, the link analysis structure offers an alternative trade-off. 
%In the next subsection we provide examples of the new statistical patterns that can be discovered by link analysis. 

\section{Related Work} 

{\em Sufficient Statistics for Single Data Tables.} Several data structures have been proposed for storing sufficient statistics defined on a {\em single} data table. 
One of the most well-known are ADtrees \cite{Moore1998}. 
%The branches in an ADtree are labelled with variable values, so a path defines a conjunctive query. 
%A node stores the count of the query that corresponds to the path from the root to the node. 
The ADtree provides a memory-efficient data structure for {\em storing} and retrieving sufficient statistics once they have been computed. 
In this paper, we focus on the problem of {\em computing} the sufficient statistics, especially for the case where the relevant rows have not been materialized. 
%We store the sufficient statistics in contingency tables represented by relational database tables. 
%For computing the sufficient statistics, storing them in contingency tables  has the advantage that blocks of sufficient statistics 
%for related sets of random variables 
%can be processed all at once as a table algebra operation. 
Thus ADtrees and contingency tables are complementary representations for different purposes: contingency tables support a computationally efficient block access to sufficient statistics, whereas ADtrees provide a memory efficient compression of the sufficient statistics. 
%once they have been gathered from the data. 
An interesting direction for future work is to build an ADtree for the contingency table once it has been computed. 
%Similar points apply to other data structures for storing sufficient statistics with minimal memory, such as KDtrees, data cubes and frequent item sets; for discussion,  see \cite{Moore1998}. 
%
%There are many proposals for utilizing relational database management systems to speed up traditional single-table machine learning (e.g., with User-Defined Functions~\cite{Ordonez2010}). 
%The Unpivot operator aims for efficiently computing sufficient statistics from data using a relational database \cite{unpivot_kdd98}. 
%In contrast to our work, Unpivot gathers sufficient statistics only from a single table, and only for two random variables at a time (an attribute and the class label). 
%Our work aims to support cross-table machine learning for the entire relational database.

%{\em Virtual Joins for Multiple Data Tables.} 
%Tuple ID propagation provides a Virtual Join method for computing query counts based on joins of existing database tables \cite{Yin2004}. 
%It expands the original data records so that query counts can be computed quickly. 
%Within our dynamic program, Tuple ID propagation can be used to compute counts for sufficient statistics that involve positive relationships only. 
%Since methods such as Tuple ID propagation address the case of positive relationships only, this paper focuses on computing query counts with a combination of positive and \textbf{negative} relationships. 
%
%Getoor {\em et al.} provided a subtraction method for the special case of estimating counts with only a single negative relationship \cite[Sec.5.8.4.2]{Getoor2007c}. 
{\em Relational Sufficient Statistics.} Getoor et al. 
provided a subtraction method for the special case of estimating counts with only a single negative relationship \cite[Sec.5.8.4.2]{Getoor2007c}. 
They did not treat contingency tables with multiple negative relationships.
%
Schulte {\em et al.} show that the fast M\"obius transform can be used to extend the subtraction method to the case of multiple negative relationships \cite{Schulte2014}. 
They considered only Bayes net parameter learning, not structure learning. 
Parameter learning requires sufficient statistics only for a child node and its parents (at most 6 nodes in their experiments). 
%In their experiments the child + parent families were fairly small (at most 6 nodes). 
Their evaluation involved statistics for only one relationship. 
In contrast, our method can be used to support Bayes net structure learning, which requires joint sufficient statistics over the entire database. 
Other novel aspects are the $\ct$-table operations and using the relationship chain lattice to facilitate dynamic programming. 

%\emph{Learning Graphical Models for Relational Databases.} 
%Several researchers have noted the usefulness of constructing a graphical statistical model for a relational database ~\cite{Deshpande2007,Graepel_CIKM13,Wang2008}, for instance for exploratory data analysis and dealing with uncertainty.
%Computing sufficient statistics is a fundamental requirement for learning graphical models. 
%Singh and Graepel propose an algorithm for compiling a database schema 
%%into a set of random variables 
%into a Bayes net model based on latent variables \cite{Graepel_CIKM13}. 
%This is like our approach to generating a default set of random variables from the database schema. The main difference is that in our application we learn the Bayes net structure from the data rather than generating a fixed structure as in \cite{Graepel_CIKM13}.{\em (zqian: more freedom, less hyper parameters tuning?)}
%The fixed Bayes net structure is based on latent variables, whereas in this paper we do not address learning with latent variables.
%Latent variable methods such as matrix and tensor factorization  \cite{Papalexakis2013} have been successfully applied to learning graphical models for relational data. 
%An interesting direction for future work is to extend our Virtual Join algorithm to support latent variable learning. 

%Inductive Logic Programming (ILP) is an approach to discriminative learning for relational data based on logical clauses ~\cite{Lavravc1994}. 
%For a comparison of ILP with graphical model learning please see~\cite{Domingos2007,Khosravi2012a}. 
%Most ILP systems {\em do not utilize} a relational database management system (RDBMS). 
%The user provides metainformation about the data and possible patterns through mode declarations, which requires significant expertise~\cite{Walker2010}. 
%Our approach of storing metainformation in database tables is like the BayesStore system \cite{Wang2008}, where all components of a statistical model are treated as first-class citizens in the RDBMS. 
%That is, structured random variables, structured models, model parameter values and sufficient statistics are all stored in database tables. 
%The BayesStore system focuses on relational probabilistic {\em inference} with a Bayes net model. 
%Our application is complementary in that it focuses on {\em learning} the Bayes net model; BayesStore can be  used to perform inference on the model once it is learned. The Tuffy system also performs statistical-relational inference utilizing an RDBMS \cite{DBLP:journals/pvldb/NiuRDS11}. 
%



\section{Conclusion} \label{sec:conclusion} Databases contain information about which relationships do and do not hold among entities. To make this information accessible for statistical analysis requires computing sufficient statistics  that combine information from different database tables, and may involve any number of {\em positive and negative} relationships. With a naive enumeration approach, computing sufficient statistics for negative relationships is feasible only for small databases. We solve this problem with a new dynamic programming algorithm that performs a virtual join, where the requisite counts are computed without materializing any join tables. 
%
%The required sufficient statistics are stored in a contingency table in the database. 
%Our dynamic algorithm computes the joint contingency table for an entire relational database. 
%The contingency table includes %We present a dynamic program that extends a cache of sufficient statistics for positive relationships to a cache for negative relationships. 
%The basis of the program is a subtraction method for obtaining query counts with $k+1$ negative relationships, from counts with only $k$ negative relationships. In this way, query counts with negative relationships can be reduced to query counts for positive relationships only. Such counts can be efficiently obtained with standard SQL queries.
%To derive the subtraction method, we introduced $\ct$-table algebra, an extension of relational algebra designed for computation with contingency tables. 
A new extension of relational algebra, called $\ct$-table algebra, facilitates the efficient implementation of this virtual join operation. 
%
%The joint contingency table can be applied to learning a Bayes net structure that represents probabilistic dependencies across the entire database. 
The efficient computation of sufficient statistics allows the system to scale to large datasets (over 1M tuples) with complex schemas. Empirical evaluation with seven benchmark datasets showed that information about the presence and absence of links can be exploited in feature selection, association rule mining, and Bayesian network learning. 
%\textbf{link on v.s. off}
%We found that a hierarchical lattice search strategy \cite{Schulte2012} performs better than simply using the joint contingency table, in terms of both learning time and model selection scores. 

%In addition to computational efficiency, an attractive property of our program is that it applies to SQL-based databases in general, and requires almost no configuration input from the  user to prepare a target database for analysis.
%We achieve this generality by extracting from the system metadata information about the random variables that are the target of statististical analysis.  


\emph{Limitations and Future Work.} 
Our dynamic program scales well with the number of rows, but not with the number of columns and relationships in the database. 
This limitation stems from the fact that the contingency table size grows exponentially with the number of random variables in the table. In this paper, we applied the algorithm to construct a large table for {\em all} variables in the database. We emphasize that this is only one way to apply the algorithm. The Virtual Join algorithm efficiently finds cross-table statistics for any set of variables, not only for the complete set of all variables in the database. An alternative is to apply the virtual join only up to a pre-specified relatively small relationship chain length.
%A sufficient chain length could be determined by a learning algorithm or specified by the user. 
Another possibility is to use postcounting \cite{lv2012}: Rather than precompute a large contingency table prior to learning, compute many small contingency tables for  small subsets of variables on demand during learning. 
%
%In a postcounting approach, generating a contingency table for a target set of variables is a service that can be called dynamically during the executation of a learning program. 
%{\em zqian: add other applications?}
%A cache of sufficient statistics can be applied to many learning problems, in addition to learning Bayes net structure. Examples include relational classification and link prediction methods. 
%

%While we have focused on statistical analysis, another potential application is in probabilistic first-order inference \cite{Poole2003}. Such inferences often require sufficient statistics (parfactors) defined with respect to one or more specified individuals (e.g., the number of user $\it{jack's}$ male friends). It is easy to extend our Virtual Join algorithm to compute sufficient statistics for specified individuals. 

In sum, our Virtual Join algorithm efficiently computes query counts which may involve any number of {\em positive and negative }relationships. %for negative relationships. 
These sufficient statistics support a scalable statistical analysis of  associations among both relationships and attributes in a relational database.


%ACKNOWLEDGMENTS are optional
%\section{Acknowledgments}
%
%This research was supported by a Discovery grant to Oliver Schulte by the Natural Sciences and Engineering Research Council of Canada. 
%Zhensong Qian was supported by a grant from the China Scholarship Council.
%[preliminary results about We thank the organizers for providing a discussion venue, and the anonymous reviewers for constructive criticism.


% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{master} 

\balance
 % vldb_sample.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references



%APPENDIX is optional.
% ****************** APPENDIX **************************************
% Example of an appendix; typically would start on a new page
%pagebreak

%\begin{appendix}
%You can use an appendix for optional proofs or details of your evaluation which are not absolutely necessary to the core understanding of your paper. 
%
%\section{Final Thoughts on Good Layout}
%Please use readable font sizes in the figures and graphs. Avoid tempering with the correct border values, and the spacing (and format) of both text and captions of the PVLDB format (e.g. captions are bold).
%
%At the end, please check for an overall pleasant layout, e.g. by ensuring a readable and logical positioning of any floating figures and tables. Please also check for any line overflows, which are only allowed in extraordinary circumstances (such as wide formulas or URLs where a line wrap would be counterintuitive).
%
%Use the \texttt{balance} package together with a \texttt{\char'134 balance} command at the end of your document to ensure that the last page has balanced (i.e. same length) columns.
%
%\end{appendix}



\end{document}

We evaluate the Virtual Join algorithm on seven benchmark datasets. According to the theoretical analysis of Section~\ref{sec:complexity}, the algorithm's run time should be nearly linear in the number of sufficient statistics that involve a false relationship. Our experiments provide empirical confirmation for this result. The Virtual Join algorithm scales to realistic datasets, one with over one million data records, and another with over one million new sufficient statistics to be computed. 

Given that computing sufficient statistics for negative relationships is {\em feasible}, the remainder of our experiments evaluates the {\em usefulness} of these sufficient statistics. For the purposes of statistical analysis, the import of these sufficient statistics is that they allow the analysis to utilize the absence or presence of a relationship as a feature. In a contingency table where the relationship indicators take on the value $\true$ in all rows, a relationship indicator feature does not carry any information about any of the other features (see Table $\ct_{T^{+}}$ in Figure~\ref{fig:flow}). It may as well be omitted from the contingency table (as in Table $\ct_{T}$ in Figure~\ref{fig:flow}). After carrying out our Virtual Join algorithm, the relationship features in the resulting contingency table are $\true$ in some rows and $\false$ in others, depending on how they correlate with other features (see Table $\ct$ in Figure~\ref{fig:flow}). Our benchmark datasets provide evidence that the relationship indicator features can be useful for different type of statistical analysis, as follows. (1) Feature selection: We learn two different feature sets for each of our databases and a given target class label. A standard feature selection method, implemented in the Weka system, selects different features for classification when provided with statistics for negative and positive relationships, from those that it selects when given positive relationship statistics only. (2) Association Rule Mining: A standard association rule learning method, included in the Weka system, includes many association rules with relationship conditions in its top 20 list. On all data sets, relationship conditions are included in the majority of rules. 
%\textbf{including relationship variable = false?}. 
(3) Bayesian network learning. A Bayesian network provides a graphical summary of the probabilistic dependencies among relationships and attributes in a database. We learn two Bayesian network structures on each of our database, one whose input is a contingency table with only positive relationships, one whose input is a contingency table with both positive and negative relationships. The Bayes net structures learned are different for all but one database, and make different trade-offs between model likelihood and model complexity. 

\emph{Derivation Outline.} We start with an upper bound in terms of the number of relationship variables  $m$, then transform this into a bound in terms of the number of sufficient statistics $\row$. For the number of relationship variables $m$, we have
%
\begin{equation} \label{eq:rchain-bound}
\it{\# \ct\_\it{ops}} = O(m \cdot 2^{m-1})
\end{equation}
%
as follows. For a given relationship chain of length $\ell$, the Virtual Join algorithm goes through the chain linearly (Algorithm~\ref{alg:fmt} inner for loop line ~\ref {reﬂine:innerloop}%12
). 
% check line number
At each iteration, it computes a $\ct_{*}$ table with a single cross product, then performs a single Pivot operation.
 Each Pivot operation requires three  $\ct$-algebra operations. 
Thus overall, the number of  $\ct$-algebra operations for a relationship chain of length $\ell$ is $6 \cdot \ell = O(\ell)$. For a fixed length $\ell$, there are at most $\binom{m}{\ell}$ relationship chains. Using the known identity\footnote{math.wikia.com/wiki/Binomial\_coefficient, Equation 6a}
\begin{align} 
\sum_{\ell=1}^{m} {m\choose \ell} \cdot \ell = m \cdot  2^{m-1} \label{eq:upperbound}
\end{align}
we obtain Equation~\ref{eq:rchain-bound}. Equation~\ref{eq:rchain-bound} entails an upper bound in terms of $\row$ as follows. %{http://goo.gl/x65yl3}. 
%\textbf{URL}
%With the Binomial theorem the identity ~\eqref{eq:upperbound} is easy to prove, we omit further details due to space constraints.
Let $\statistics$ be the number of sufficient statistics where all relationship variables are true. For instance, if there are $n$ binary attributes, then $\statistics = 2^{n}$. Then we have \begin{equation} \label{eq:row-stats} \row = \statistics (2^{m}-1) \end{equation} since there are $2^{m}-1$ combinations of relationship values with at least one negative relationship. Solving for $m$ and substituting the result into the expression $m \cdot 2^{m-1}$, we obtain 
$$\it{\# \ct\_\it{ops}} = O(\frac{1}{2} \log_{2} (\frac{\row}{\statistics} + 1) \cdot (\frac{\row}{\statistics}+1)). $$ Since $\row/\statistics +1 \leq 2 \row$ for $r,s \geq 1$, it follows that $\it{\# \ct\_\it{ops}} = O(r \cdot \log_{2} r)$, which was to be shown. 
%For the upper bound in terms of $\ct$-table rows $\row$, we note that the output $\ct$-table can be decomposed into $2^{m}$ subtables, one for each assignment of values to the $m$ relationship variables. 
%Each of these subtables contains the same number of rows $d$ , one for each possible assignment of values to the attribute variables. 
%Thus the total number of rows is given by $r = d \cdot 2^m.$ 
%Therefore we have 
%$m \cdot 2^{m} = \log_{2} (r/d) \cdot r/d \leq \log_{2}(r) \cdot r.$
%Thus the total number of $\ct$-algebra operations is $O(r \cdot \log_{2}(r))$.
%$\it{\# \ct\_\it{ops}} = O(r \cdot \log r) .$

%From this analysis we see that both upper bounds are overestimates. (1) Because relationship chains must be linked by foreign key constraints, the number of valid relationship chains of length $\ell$ is usually much smaller than the number of all possible subsets ${m\choose \ell}$. (2) The constant factor $d$ grows exponentially with the number of attribute variables, so $\log_{2}(r) \cdot r$ is a loose upper bound on $\log_{2} (r/d) \cdot r/d$. 

