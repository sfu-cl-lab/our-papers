From: "Guest Editors of SI: Inductive Logic Programming" <fabrizio.riguzzi@unife.it>
Subject: Decision on your Manuscript #MACH1518
Date: 22 February, 2013 9:05:12 PST
To: "Oliver Schulte" <oschulte@cs.sfu.ca>

CC: fabrizio.riguzzi@unife.it, zelezny@fel.cvut.cz

Dear Oliver Schulte:

The reports from the reviewers of your manuscript, "Modelling Relational Statistics With Bayes Nets", which you submitted to Machine Learning, have now been received.

Based on the reviewers' reports your manuscript could be accepted for publication should you be prepared to incorporate minor revisions as detailed below. You will however note that some of the requested revisions are on the "major side of minor" so this manuscript will require further substantial attention from you.

When preparing your revised manuscript, you should carefully consider the reviewers' comments, and submit a pointwise list of responses to the comments.  Your list of responses should be uploaded as a separate file in addition to your revised manuscript.  We will examine the manuscript and the responses, and determine whether the revised manuscript can be accepted. We may send the manuscript back to the reviewers for comment, depending on their reviews and the revisions.

Please submit your revision by March 22, i.e. in a month from now.

In order to submit your revised manuscript electronically, please access the following web site:

     http://mach.edmgr.com/

Your login is:  OSchulte
To access your password, please complete the following steps:

  1.	Open the URL:  http://mach.edmgr.com/
  2.	Click the LOGIN button on the banner.
  3.	Click "Send Username/Password".  
  4.	Complete the required information (First Name, Last Name, Email Address). 
  5.	Click "Send Username and Password".  

Your Password will be sent to you by email.

Click "Author Login" to submit your revision.

Thank you.


Best regards,

    Fabrizio Riguzzi, Filip Zelezny
    Guest Editors of Special Issue on Inductive Logic Programming (ILP-2012)
    Machine Learning


COMMENTS FOR THE AUTHOR:




Reviewer #1: Recommendation:

I believe the paper can be accepted for MLj with minor revisions (some clarifications / minor corrections are needed, see below).


Summary:

The authors consider the problem of learning class-level statistics from data. As a representation, Parameterized Bayes Nets are used, with an adapted semantics (class-level rather than the usual instance-level). For inference, the authors propose to use the Inverse Mobius Transform to efficiently compute probabilities (in the presence of link uncertainty). For learning, the authors use the MPLE approach introduced in previous work.


Comments:

The extended version of the paper looks nice. There is an extended section describing the background and the semantics of the considered models, which makes the paper much more self contained. There is also an extended experimental evaluation, now also comparing the proposed method to MLNs.

I have only some minor questions/comments left:

* Table 1 is incorrect in several ways, making the example rather confusing.
(a)  In the second row, the last two probabilities in the product are 0.3 and 0.6, but it seems they should be 0.7 and 0.4. E.g. for the latter one: this is the probability of X NOT being a coffee drinker given that X is a man. According to the CPT in Fig.1, this should be 1-0.6=0.4. Please check also the other entries and rows.
(b)  The table seems to assume that Anna is a coffee drinker. However, according to the relational structure in Fig.1, there are no coffee drinkers (because there is no coffee drinker table).
(c)  You use {F,M} as the range for gender, while before it was {W,M}.

* Difference with SRMs, p.17+18: I don't understand your point "(2)". Is this really a restriction of the SRM semantics, or of the inference method in SRMs? Is the point you raise here in some may related to the restriction of inference in SRMs mentioned on p.11, namely the restriction that the 1-minus trick can handle at most one negative relation? Or are these things unrelated? 

* p.9: You talk about "complete database" and "complete structure", where the former seems to be a subset of the latter. I'm not following here. What exactly is the "complete structure" ?

* Fig.4, caption of left part: You mention the "number of queries", e.g. 506 for Mondial, etc. What are these numbers? According to the text (p.14, line3) you use 100 test queries for all datasets?

* Fig.4, right part: Why are there no boxes/whiskers for the 70% and 90% on Financial? Also, what do the whiskers represent? As far as I know, it is common that the boxes in a box plot show 25% and 75% percentile, but I don't know of a common interpretation for the whiskers, so please specify.

Other comments (typos, details):
* p.2 line1: Patterns -> patterns
* p.4 omega_f is defined as P(tau_1), ..., P(tau_a) but the comma here does not really have a meaning. It seems that what you really mean is that it is the Cartesian product (or "product set"), so use the $\times$ in LaTeX.
* p.8 (and rest of the paper) You say you refer to a PBN as simply a "Bayes net". I'm not a big fan of this terminology: a PBN and a regular BN are really different things, with e.g. a different semantics. I would prefer the acronym PBN instead of 'Bayes net'.
* p.11 "P(R_1 = true, ...)" -> true should be 'T'
* Fig.4 and Table 3: I believe MLj requires fig captions below the figure and table captions above the figure. But please check.



Reviewer #2: SUMMARY: BRIEFLY STATE WHAT THE PAPER IS REALLY ABOUT AND BRIEFLY SUMMARIZE REVIEW.

This paper presents a new method to compute joint event probabilities in  Bayes Nets, in  presence of non-grounded information.  As rightly claimed in the paper, similar "ground-formula" methods quickly become computationally intractable in presence of big domains for variable instantiation. Moreover, grounding sometimes obscures the relational nature of some domains. However, this motivation is not exactly new, cf. e.g. [3,4,7]. 

This paper presents a combination of several already known statistical and logical methods of handling first-order probabilistic reasoning:
parametrised  Bayes Nets [7,8], first-order random selection method [3-4], safe domain relational calculus [13], and the use of inverse Mobius transform for the purpose of working with negated atoms [9,20]. 

CONTRIBUTIONS: WHAT IS (ARE) THE CLAIMED CONTRIBUTION(S) OF THE PAPER? HOW SIGNIFICANT/IMPORTANT IS (ARE) THE CLAIMED CONTRIBUTION(S)?

The paper rightly claims originality in combining the above several methods together. This being a combination of a few known methods, the originality is limited.
It is not entirely correct to call this proposed method a non-ground semantics, it is really still a grounding semantics (hence you can reuse several "old" methods), but it is random grounding semantics. So, you still have to ground formulas to proceed with the method, but you ground a few, rather than all.



SUPPORT: HOW ARE THESE CONTRIBUTIONS SUPPORTED?  IS THE SUPPORT
ADEQUATE?  CORRECT?  IS THE EVALUATION ADEQUATE?  WELL-DESIGNED? COMMENT ON THE SUPPORT FOR EACH CLAIMED CONTRIBUTION AND, IF APPROPRIATE, ON HOW THE SUPPORT COULD BE IMPROVED.

The soundness of this paper partially relies on the soundness of all the preceeding results and publications: this concerns in particular complexity analysis, results on inverse Mobius transform, and query probability estimation.

There are no theorems in this paper.
The main evaluation under-taken is running several datasets [from 22], and comparing the results with several related methods. 
The authors claim that they achieve an enormous speed-up. I have no reasons to doubt their experimental results. 

TECHNICAL QUALITY: IS THE PAPER TECHNICALLY SOUND?  DOES IT ADEQUATELY ADDRESS THE LIMITATIONS OF THE RESULTS?  COMMENT ON HOW TECHNICAL QUALITY COULD BE IMPROVED.  ARE THE  STRENGTHS/WEAKNESSES AND REASONS FOR OBSERVED RESULTS EXPLORED SYSTEMATICALLY?

The results are sound, as far as I can see. 
I particularly enjoyed reading the analysis of  various interpretations to the approach to "atoms as random variables".
Using the data-base methods for random grounding is a very interesting idea.

ORIGINALITY: ARE THE CLAIMED CONTRIBUTIONS NOVEL?  DOES THE PAPER PLACE THE WORK APPROPRIATELY IN THE CONTEXT OF RELATED RESEARCH?

It is not a major break-through in the area, and its originality is limited, due to its being combination of several existing methods.  But the overall application design brings some novelty to the field.  The paper does a very good job in relating this work to the existing research, and explaining all the nuances.


CLARITY: COMMENT ON THE WRITING, ORGANIZATION & EXPOSITION.  IS
TECHNICAL MATERIAL (ALGORITHMS, EVALUATION METHODOLOGY, ETC.) DESCRIBED IN SUFFICIENT DETAIL?  COULD A READER REPLICATE THE WORK? ARE THE TITLE AND ABSTRACT APPROPRIATE?  ARE THE FIGURES/TABLES SUFFICIENT?

The paper seems technically sound and the new method is successfully tested on benchmark real-world datasets. There are many examples throughout, and they are nicely chosen. The paper is very well written;  it explains nicely and clearly several existing approaches, thereby putting them into one common context. The way in which these several known methods are combined is simple, elegant and efficient.

DETAILED COMMENTS: 

Being a second revision after ILP'12 submission, the paper seems well-polished.

Conclusions, line 11: Mobious -> Mobius.

REQUIRED CHANGES: DISTINGUISH BETWEEN MINOR AND MAJOR CHANGES THAT NEED TO BE MADE FOR THE PAPER TO BE ACCEPTABLE.

I see no major issues, apart from more general originality concerns.





Reviewer #3: The authors address learning a Parameterized Bayes net (PBN) in the first-order semantics defined by Bacchus, where probabilities are put on the domain and which allows to evaluate statements as for instance "the probability that a bird flies". This differs from classical works in SRL, which rather consider semantics that defines the probability of a world.

To compute the parameters of the PBN, they consider the classical observed database frequency that counts the number of objects that satisfy a statement on the number of possible objects. They address the problem of computing the parameters, i.e. the probability of conjunction of statements. The expensive point is computing a statement involving the absence of a link (for instance the probability that X and Y satisfy properties and are not friend). They propose to use the fast MÃ¶bius transformation, which relies on the fact that P(A=true, B=false) = P(A=true) - P(A=true, B=true).

The paper is quite clear and well written. The questions I had in the first version for ILP have been answered but new questions have been raised . 


1- The parameterized Bayes network is a DAG, but when grounded, it may no longer be a DAG (for instance when X=Y, there is a cycle on gender(X)). How is this taken into account?

The paper is illustrated by a quite simple example: gender(X), gender(Y), friend(X,Y). What happens  with more complicated dependencies involving several relations? 

2- The second one concerns Mobius transformation: it is used only for boolean literals. Attribute literals are an extension of boolean literals and it could be used to compute the probability for the last value of the literal, given the (n-1) other ones. Does it mean that for the attribute literals the information must be complete? 

3- The structure of the Bayes network is learned by the learn-and-join algorithm. In fact, learning the structure of the model needs an evaluation of the model under construction. What is the underlying semantics used in the learn-and-join algorithm? Is it the same semantics as used in previous works or has it been adapted to this class-level semantics?

4- In the experiments, the structure of the database should be given.

5- In the experiments what are the lengthes of the query? They seem quite short. How many relations are involved?

I do think that an important parameter, which is not taken into account is the number of relations  in the databases and in the queries. 

The experiment section must be developed for publication in Machine Learning
