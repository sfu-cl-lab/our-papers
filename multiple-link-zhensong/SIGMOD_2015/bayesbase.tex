%deadline Nov 6, PST, midnight. What is the feedback model? page limit is 12 pages, with 4 more for 
% Server Space execution
\documentclass{acm_proc_article-sp}

\usepackage{graphicx}
\usepackage{alltt}

\input{preamble-stuff}

\newcommand{\ct}{\mathit{ct}}


%\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)
\graphicspath{{../../}{figures/}}

\begin{document}

% ****************** TITLE ****************************************
%BayesBase: Managing Relational Database Schemata for Learning Graphical Models
%\title{BayesBase: Managing Graphical Models with-in RDBMS}
\title{MRLBase: Multi-Relational Learning with Multi-Relational Databases}
%\author{
%Zhensong Qian, Oliver Schulte\\
% Simon Fraser University, Canada\\
%\{zqian, oschulte\}@sfu.ca
%}

\maketitle  
%\textbf{\today}
\begin{abstract} 
The statistical analysis of structured data requires building structured machine learning models. We describe MRLBase, a new SQL-based framework that leverages the capabilities of an RDBMS to support multi-relational learning applications. Most previous machine learning applications assume a flat data representation with a single data table or matrix. However, many real-word datasets have a more complex structure, and are often maintained in a relational database.
Building machine learning applications for multi-relational data requires new system capabilities. These capabilities include both representation and computation, such as: 1) A description language for specifying metainformation about structured random variables. 2) Efficient mechanisms for constructing, storing, and transforming complex statistical objects, such as cross-table sufficient statistics, parameter estimates, and model selection scores. 3) Computing model predictive scores for structured test instances. Our system design represents statistical objects as  relational tables, on a par with the original data tables, so that SQL can be used  to manage them. A case study on six benchmark databases shows how our system supports a challenging and important machine learning application, namely learning a Bayesian network model for an entire database. Our implementation shows how our SQL constructs in MRLBase facilitate fast, modular, and reliable program development. Empirical evidence indicates that leveraging the RDBMS capabilities  achieves scalable learning and fast model testing.
\end{abstract}
%

\section{Introduction} Machine learning for large datasets is a growing application area at the intersection of machine learning and systems research. Several well-developed software packages implement standard machine learning algorithms (e.g., R, Weka). More recent developments support learning with large datasets. 
%(e.g., Spark, Madlib). 
These packages assume that data is represented in a single table or data matrix, where each row represents a data point or feature vector. The single-table representation is appropriate when the data points represent a homogeneous class of entities with similar attributes, where the attributes of one entity are independent of those of others \cite{Bishop2006}. However, many real-word enterprise datasets have a more complex structure, with different classes of entities (customers, products, factories etc.), that have different attributes, and may be interrelated in multiple ways. Such heterogeneous data are often represented using a relational database management system (RDBMS). The field of {\em multi-relational learning} aims to extend machine learning to multi-relational data \cite{SRL2007,Domingos2009,Dzeroski2001c}. Database researchers have noted the usefulness of multi-relational statistical models for knowledge discovery representing uncertainty in databases \cite{Deshpande2007,Graepel_CIKM13,Wang2008}. Multi-relational learning is the process of building multi-relational statistical models.

In this paper we present MRLBase for ``Multi-relational Learning Base'', a framework for building the system capabilities required for multi-relational learning that go beyond what is required for single-table learning. Statistical system tasks include accessing data accesses, constructing, storing, querying, and transforming parameter estimates and model structures. MRLBase follows a client-server paradigm, where the client is a machine learning application for multi-relational data, and the server is an RDBMS that supports a machine learning application. The RDBMS is used not only to store data, but also to store structured objects for statistical analysis as first-class citizens in the database. The basic principle of MRLBase is to build the required system capabilities by leveraging RDBMS capabilities via SQL scripts, tables, and views.  SQL provides high-level constructs for multi-relational machine learning, which minimize the programming overhead for handling system tasks. By separating system tasks from statistical issues, MRLBase facilitates extending single-table applications to multi-relational data.
Our argument is that relational algebra can play the same role for multi-relational machine learning that linear algebra does for single-table machine learning:  a unified language for both representing and computing with objects that support statistical analysis. 

%MRLBase thus allows application developers to maximize their focus on statistical and learning issues. 


We provide an empirical evaluation of MRLBase on six benchmark databases, two of which contain over 1M records. 
%For the largest database, our system computes over xM sufficient statistics in less than y minutes. 
MRLBase supports scalable multi-relational model learning, taking minutes on medium-size databases, and less than two hours on the largest database. Previously existing multi-relational learning methods do not scale to the largest database sizes. 
For the task of scoring model predictions on a set of test instances, the RDBMS capabilities easily implement block access to test instances, which leads to a 1,000 to 10,000-fold speed up compared to a simple loop.

{\em Paper Organization.} We begin with an overview of MRLBase. Based on this overview, we discuss the relationship to related works. The bulk of the paper discusses the details of implementing the system based on SQL: We begin with representing metainformation about relational random variables. Then we describe gathering multi-relational sufficient statistics via metaqueries. The sufficient statistics support the computation of model selection scores, and of model structure learning. These counting methods can be adapted for scoring models against   structured test instances. 
%To illustrate the broader applicability of MRLBase, we discuss applying it to learn Markov networks, and to feature generation for multi-relational classification. 

{\em Contributions.} The main contributions of this paper may be summarized as follows.

\begin{enumerate}
\item Identifying new system requirements for multi-relational machine learning that go beyond traditional single-table machine learning.
\item An integrated set of SQL-based solutions for providing these system capabilities, including 
\begin{enumerate}
\item Defining a default set of relational random variables, and extracting metainformation about them from the RDBMS system catalog.
\item Computing contingency tables that store multi-relational sufficient statistics as database tables.
\item Storing and scoring probabilistic models.
\end{enumerate}

\end{enumerate}



\section{System Overview} 



Figure~\ref{fig:architecture} represents key system components. The starting point is a multi-relational database containing original data. 
We outline the main principles behind our design, then discuss how the key system components implement these principles.

\subsection{Design Principles} \label{sec:design} The main design principles of MRLBase are the following. 

(1) {\em Tabular Representation.} Structured objects for statistical analysis are stored in the relational database. 
%This means that they are stored in relational tables with an appropriate schema. 

(2) {\em Computation by SQL.} We use SQL queries to construct, query, and transform statistical objects. 

(3) {\em Update by Views.} We create tables that represent statistical objects using the relational view mechanism. 

(4) {\em Modularity and Independence.} We organize statistical objects in layers to minimize dependencies among them. 

%For example, parameter estimates depend directly on sufficient statistics, but not directly on the data from which sufficient statistics are computed. 


The motivation for our design includes the following advantages. 

%\begin{enumerate}
%\item 
(1) The tabular representation makes it possible to use SQL high-level programming language for constructing and querying statistical objects. SQL is portable across different database systems and machine learning applications.  SQL program execution is reliable even for complex manipulations.

%\item 
(2) Large objects that do not fit in main memory can be stored on disk, managed by the RDBMS. Scalability is further supported by leveraging RDBMS capabilities such as query optimization and indexing.

(3) 
Distributing the information about statistical objects across different database tables provides a more compact and more intuitive representation that combining them into a single vector or matrix. This is similar to the difference between a normalized and unnormalized data representation.

%\item 
(4) Server-side management of statistical objects reduces the computational resources required by the machine learning application.

%\item 
(5) The relational view mechanism ensures that statistical objects are updated automatically when the results of learning and/or the original data change. This minimizes the extent to which the machine learning application has to manage such updates.

%\item 
(6) Modularity and independence facilitate porting single-table learning applications to multi-relational applications. For instance, many model selection methods invoke a subroutine to compute a model selection score for a given dataset. MRLBase implements this subroutine for multi-relational data as a service to a machine learning client. Other parts of the model selection algorithm, such as model search heuristics, do not need need further adaptation. 
%
%\end{enumerate}



\begin{figure}[htbp]
\begin{center}
\resizebox{0.5\textwidth}{!}{
\includegraphics
%[width=0.8\textwidth]
{architecture}
}
\caption{MRLBase Key Components. Arrows show dependencies.
%Here the $DB$ denotes the original database, $DB\_RV$ denotes the database storing all random variables, 
%$DB\_CT$ denotes the database containing all the contingency tables  and $DB\_BN$ storing all the learned Bayes Nets.
\label{fig:architecture}}
\end{center}
\end{figure}

\subsection{System Components}  

\subsubsection{The Schema Analyzer}
Statistical analysis is based on a set of random variables. Single-table data is basically self-describing with respect to random variables: each column header other than row id fields represents a random variable. In contrast, a set of data tables that represents heterogeneous multi-relational data needs to be augmented with metadata about which columns represent which entity class. Such metadata requires a {\em data description language} \cite{Ullman1982}; in SQL the relevant key concepts are primary and foreign keys. 
%Previous work on multi-relational learning typically assumed that information about random variables for heterogeneous is specified by the user using a logical language. 
A novel aspect of MRLBase is analyzing the RDBMS system catalog to translate metadata about primary and foreign keys into a specification of relational random variables. 

The Schema Analyzer examines the information in the DB system catalog to define a default set of random variables for statistical analysis.  Since the system catalog is itself stored in tables, the default set of random variables can be constructed using SQL queries. Metainformation about the random variables is stored in the \textbf{random variable database} $\RVD$. This database may be edited by the user to add further random variables of interest. Another possibility is for a machine learning application to create the $\RVD$ database, for example based on metainformation specified in another format.

\subsubsection{The Count Manager}
Access to sufficient statistics or event counts are a basic requirement for most machine learning application. The speed of this access is often the main factor for determining scalability~\cite{Russell2010,Moore1998}. This is even more the case for multi-relational machine learning because sufficient statistics often combine information from different tables, which involves count operations over table joins. The Count Manager uses the metainformation in the $\RVD$ database to compute multi-relational sufficient statistics for a set of random variables.  The sufficient statistics are stored as {\em contingency tables} in the \textbf{Count Database} CDB as database tables. MRLBase constructs contingency tables as views defined by SQL metaqueries, which are queries that build queries.
%The contingency table SQL definitions are themselves stored in a database table. The query tables are constructed by SQL metaqueries, that use the meta information in the $\RVD$ database. %Figure~\ref{fig:metaquery-concept} illustrates this design. 
%
%
%A novel aspect of MRLBase is storing sufficient statistics in contingency tables defined by database views. These views are defined by SQL queries, which are constructed using the novel concept of an SQL metaquery. An SQL metaquery takes as input the metainformation about random variables, and builds a set of tables that represent a view definition. 


\subsubsection{The Model Manager} 

The Model Manager supports the construction and querying of large structured statistical models. These models are also represented in relational database tables in the \textbf{Model Database} MDB. Services provided by the Model Manager include the following. (1) Compute parameter estimates for the model using the sufficient statistics in the Count Database.  (2) Computing model characteristics such as the number of parameters or degrees of freedom in a model. (3) Computing a model selection score that quantifies how well the model fits the multi-relational data. 
%Such model scores are usually functions of the number of parameters and the parameter estimates.  By employing the view mechanism, parameter estimates and model selection scores can be updated automatically when the original data change.

While MRLBase provides good solutions for each of these system capabilities in isolation, the ease with which the system components can be integrated is a key feature. Because information about random variables, sufficient statistics, and models 
is all represented in relational database tables,
a machine learning application can access and combine the information in a uniform way via SQL queries.




\section{Related Work} \label{sec:related}
We review the work around the topics of machine learning and data management most relevant to our research.
%Given the density of the research landscape around the topics of machine learning and data management, Figure~\ref{fig:novelty} provides a graphical illustration of where MRLBase is located with respect to other work.

\paragraph{Single-Table Machine Learning and Data Management Systems}
We briefly review several systems that leverage the advantages of advanced data management for machine learning applications. 
%These systems utilize support for stroring and computing with large data structures that do not fit in main memory. 
They still 
assume a single-table data representation of homogeneous data points.  Most of this work complements ours, in that it focuses on different tasks such as inference or distributed processing. As a general comment, 
the MRLBase framework faciliates porting single-table methods to multi-relational data. Especially for single-table systems that leverage an RDBMS this is a natural extension that increases their usefulness even further.


%Deshpande and Madden  present the 
The MauveDB system \cite{Deshpande2006a} emphasizes the importance of several features for combining statistical analysis with databases: A statistical layer should provide data independence by abstracting from the data that generated the model, and models should be updated automatically as data change. MRLBase shares these properties.
%by storing models and associated parameters as objects in their own right independent from data, and by using the view mechanism. 
A difference is that MauveDB presents model-based views of the {\em data} to the user, whereas MRLBase presents views of the models themselves to machine learning applications. 

Several papers have shown that an RDBMS provides strong support for inference given a model. 
In machine learning theory and implementations, different objects are often represented as matrices, and combined using matrix multiplication. The same results can be obtained in a relational format using natural joins with summations \cite{Wong1995}. The Bismarck system leverages RDBMS capabilities to find fast solutions to convex programming problems in data analysis \cite{Feng_SIGMOD_2012}. Bismarck could be extended to implement more sophisticated parameter estimation methods in MRLBase than the maximum likelihood method we describe in this paper (e.g., regularization). 
%The MCDB system \cite{MCDB_SIGMOD_2008} uses 
Monte Carlo methods have also been explored to perform inference with uncertain data, based on user-defined variable generating functions \cite{MCDB_SIGMOD_2008} or a probabilistic model \cite{Wick_VLDB_2010}.  
%MCDB stores parameters in database tables like MRLBase. The Monte Carlo methods are based on user-defined variable generating functions rather than a probabilistic model. Wick {\em et al.} develop Monte Carlo methods for probabilistic database based on a probabilistic model \cite{Wick_VLDB_2010}. 

There are several software collections that aim to provide users with high-level constructs for specifying statistical models and learning algorithms. These include the classic WinBUGS \cite{Lunn2000}, as well as the more recent MADLib \cite{MADlib_VLDB_2012} and MLBase systems \cite{MLbase_ICDR_2013}. The MADLib vision is based on leveraging RDBMS capabilities through SQL programmin; MRLBase is a good fit for learning with a multi-relational component data source in the MADLib framework. 
%A key difference between MLBase and MRLBase is the ``R'': Our system supports the analysis of relational data. 
The MLBase system emphasizes distributed processing and automatic refinement of machine learning algorithms and models. For systems that aim to produce an easy machine learning interface for an end user, such as WinBUGS and probabilistic programming and markup languages \cite{Guazzelli2009,Milch2005}, MRLBase can serve as a backend that implements the learning algorithms for multi-relational data. For instance, WINBUGS is an object-oriented system with a class ``random variables''. MRLBase would support the creation of a subclass ``relational random variables'', as well as the implementation of methods for the subclass (e.g., ``build Bayesian network'').

\paragraph{Multi-Relational Learning} 
Multi-Relational learning has been investigated by many researchers; for book-length overviews, please see \cite{SRL2007,Domingos2009,Dzeroski2001c,sun2012mining}.
Our case study using Bayesian network learning belongs to a subfield called statistical-relational learning that is largely focused on learning graphical models for multi-relational data. Most implemented systems use a logic-based representation of data derived from Prolog facts, that originated in the Inductive Logic Programming community; representative systems include Alchemy \cite{Kok2009a} and Aleph.
%\footnote{\url{http://www.cs.ox.ac.uk/activities/machlearn/Aleph/}}.
 System issues for learning applications with a logic-based data representation have been handled separately for each learning approach using file systems and in-memory data structures. The logic-based approaches do not make use of SQL/RDBMS. The ClowdFlows system \cite{Lavravc2014} allows a user to specify a MySQL database as a data source, then converts the MySQL data to a logic-based representation. 

Singh and Graepel \cite{Graepel_CIKM13} present an algorithm that translates key constraints from a relational database system catalog into a set of relational random variables and a Bayesian network structure. This approach utilizes SQL constructs as a data description language in a way that is similar to our Schema Analyzer. Differences include the following. (1) The Bayesian network structure is fixed and based on latent variables, rather than learned for observable variables only as in our case study. (2) The RDBMS is not used to support the learning after random variables have been extracted from the schema. 

Using database systems to compute sufficient statistics has been well explored for single-table data \cite{Moore1998,Graefe1998}, but much less for multi-relational statistics that combine information from different tables.
Yin {\em et al.} \cite{Yin2004} present a Virtual Join algorithm for computing sufficient multi-relational statistics. They do not use contingency database tables to store the sufficient statistics. In terms of our system, the Virtual Join algorithm is an alternative to metaqueries. Qian {\em et al.} \cite{Qian2014} independently propose the use of contingency database tables. Their paper focuses on a Virtual Join algorithm for computing sufficient statistics that involve negated relationships. They do not discuss integrating contingency tables with other structured objects for multi-relational learning. 


\paragraph{Multi-Relational Inference} Several researchers have noted the usefulness of constructing a graphical statistical model for a relational database~\cite{Deshpande2007,Graepel_CIKM13,Wang2008}, for instance for exploratory data analysis and dealing with uncertainty. Database researchers have developed powerful probabilistic inference  algorithms for multi-relational models. These models leverage RDBMS capabilities for inference much as MRLBase does for learning. The BayesStore system \cite{Wang2008} introduced the principle of treating all statistical objects as first-class citizens in a relational database as MRLBase does. The Tuffy system \cite{DBLP:journals/pvldb/NiuRDS11} achieves highly reliable and scalable inference for Markov Logic Networks (MLNs) with an RDBMS. The MRLBase can be used to learn an MLN. A very useful future project would be to combine MLN learning by MRLBase with inference by the Tuffy system to produce a single integrated RDBMS package for both learning and inference. 



\section{The Random Variable Database} 


Statistical analysis begins with a set of random variables. Formally, a \textbf{random variable} $\node$ is defined by a domain of possible values and a probability distribution over that domain. In this section we discuss what types of random variables are suitable for analyzing relational databases; we refer to these as {\em relational random variables.} The more complex structure of multi-relational data leads to more complex structure for relational random variables, compared to random variables for single-table data.  
%A major difference between single-table data and multi-relational data is that the latter has a more complex structure that represents heterogeneous classes of entities and multiple types of relationships. 
Multi-relational learning requires making this structure explicit in machine-readable {\em metainformation}. We discuss how to find and store relevant metainformation about relational random variables. The metainformation for a random variable must include the following at a minimum.

(1) The domain of the random variable. For discrete random variables, this is a finite set of possible values.

(2) Pointers to the table and/or column in the original database that contains the data relevant to the random variables. 

%\begin{enumerate}
%\item The domain of the random variable. For instance, a range of continuous values, or a finite set of discrete values.
%\item Pointers to the table and/or column in the original database that contains the data relevant to the random variables. 
%\end{enumerate}

There are various different notational systems for defining random variables in relational structures, of equivalent expressive power. We adopt function-based notation from logic  \cite{Russell2010}. The expressive power of this formalism is equivalent to well-known logical query languages such as the domain relational calculus \cite{Ullman1982}. MRLBase can be adapted for other notational systems.

\subsection{Relational Random Variables}
A domain or \textbf{population} is a set of individuals.
Individuals are denoted by lower case expressions (e.g., $\it{bob}$). 
%A \textbf{first-order variable} is capitalized. 
A \textbf{functor} represents a mapping
$\functor: \population_{1},\ldots,\population_{a} \rightarrow \outdomain_{\functor}$
where $\functor$ is the name of the functor, each $\population_{i}$ is a population, and $\outdomain_{\functor}$ is the output type or \textbf{range} of the functor. 
In this paper we consider only functors with a finite range, disjoint from all populations.  If $\outdomain_{\functor} = \{\true,\false\}$, the functor $\functor$ is a (Boolean) \textbf{predicate}. A predicate with more than one argument is called a \textbf{relationship}; other functors are called \textbf{attributes}. We use uppercase for predicates and lowercase for other functors. Throughout this paper we assume that all relationships are binary, though this is not essential for our algorithm.
%the populations associated with 
 %the variables are of the appropriate type for the functor.
%
A \textbf{Relational random variable} (RRV) is of the form $\functor(\X_{1},\ldots,\X_{a})$, where each $\X_{i}$ is a first-order variable \cite{Poole2003,Russell2010}. 
Each first-order variable is associated with a population/type. In the context of RRVs, we therefore refer to first-order variables also as \textbf{population variables}. An RRV has two components: A functor and a list of population variables. We discuss first how the Schema Analzyer translates a relational database schema into a set of functors. Second, we describe a default method for combining population variables with functors. 

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
 \centering
\resizebox{0.45\textwidth}{!}{
 \includegraphics{university-schema.pdf} 
 } 
\caption{A relational ER Design for a university domain.}
 \label{fig:university-schema}
\end{figure}


\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
 \centering
\resizebox{0.4\textwidth}{!}{
 \includegraphics[width=0.5\textwidth]{university-tables.pdf} 
} 
\caption{Database Table Instances: (a) $\student$, (b) $\course$, (c) $\prof$, (d) $\ra$, (e) $\reg$.  
}
 \label{fig:instance}
\end{figure}


\subsection{Translating Entity-Relationship Models Into Functors}
 We assume a standard \textbf{relational schema} containing a set of tables, each with key fields, %typically
descriptive attributes, and possibly foreign key pointers. 
A \textbf{database instance} specifies the tuples contained in the tables of a given database schema. 
We assume that tables in the relational schema can be divided into {\em entity tables} and {\em relationship tables} (ER model) \cite[Ch.2.2]{Ullman1982}.
%This is the case whenever a relational schema is derived from an entity-relationship model  
 Figure~\ref{fig:university-schema} shows an ER diagram for a toy university domain. An ER diagram shows entity sets as boxes, relationships between entities as diamonds, and attributes as ovals. Figure~\ref{fig:instance} shows a database instance for this ER diagram. An ER diagram for single-table data would contain just one entity set box with attributes, and no relationships. Our approach is to translate the components of the ER diagram into random variables for statistical analysis \cite{Heckerman+al:SRL07}. The translation of an ER diagram into a set of functors converts each element of the diagram into a functor, except for entity sets and key fields. Table~\ref{table:translation} illustrates this translation.






%The functor formalism is rich enough to represent the constraints of an entity-relationship schema via the following translation: Entity sets correspond to populations, descriptive attributes to attribute functors, relationship tables to relationship functors, and foreign key constraints to type constraints on the arguments of relationship predicates. %, distinguishing attributes of entities ($\eatts$) and attributes of relationships ($\ratts$). 


%\begin{table}[btp] \centering
%\resizebox{0.4\textwidth}{!}{
%\begin{tabular}[c]
%{|l|c|l|l|}\hline
% ER Design &Type & Functor &\RRV \\\hline
%    \begin{tabular}{l}Entity \\Tables\end{tabular}&\begin{tabular}{l}Population \\Variables \end{tabular} & Student, Course & S, C \\\hline
%    \begin{tabular}{l}Relation \\Tables \end{tabular}&RNodes &RA & RA(P,S) \\\hline
%   \begin{tabular}{l}Entity \\Attributes \end{tabular}&1Nodes & intelligence, ranking &\begin{tabular}{l} \{intelligence(S), ranking(S)\}  \\=1Nodes(S) \end{tabular} \\\hline
%  \begin{tabular}{l} Relationship \\Attributes \end{tabular}&2Nodes & teachingability, salary &\begin{tabular}{l}   \{teachingability(P,S), salary(P,S)\}  \\= 2Nodes(RA(P,S))\end{tabular}\\\hline
%   
%\end{tabular}
%}
% % end scalebox
%\caption{Translation from ER Diagram to Relational Random Variable. \textbf{OS fix table}
% \label{table:translation}}
%\end{table}
\begin{table}[btp] \centering
\resizebox{0.4\textwidth}{!}{
\begin{tabular}[c]
{|l|c|l|l|}\hline
 ER Design &Type & Functor &\RRV \\\hline
    \begin{tabular}{l}Entity \\Tables\end{tabular}&\begin{tabular}{l}Population \\Variables \end{tabular} & Student, Course & $\S, \C$ \\\hline
    \begin{tabular}{l}Relation \\Tables \end{tabular}&Relationship &RA & RA($\P$,$\S$) \\\hline
   \begin{tabular}{l}Entity \\Attributes \end{tabular}&1Attributes & intelligence, ranking &\begin{tabular}{l} \{intelligence($\S$), ranking($\S$)\}  \\=1Attributes($\S$) \end{tabular} \\\hline
  \begin{tabular}{l} Relationship \\Attributes \end{tabular}&2Attributes & capability, salary &\begin{tabular}{l}   \{capability($\P, \S$), salary($\P, \S$)\}  \\= 2Attributes(RA($\P, \S$))\end{tabular}\\\hline
   
\end{tabular}
}
 % end scalebox
\caption{Translation from ER Diagram to Relational Random Variable. %\textbf{OS fix table}
 \label{table:translation}}
\end{table}



There are different types of functors corresponding to the different types of ER diagram components. The simplest type represents an attribute of an entity set. We refer to such functors as \textbf{1Attributes}. In Figure~\ref{fig:university-schema}, there are six 1Attributes corresponding to attributes of Professors (2), Students (2), and Courses (2). 
%The primary keys (p\_id, s\_id, c\_id) are identifiers that are not treated as random variables. 
The metainformation about 1Attributes can be stored in a database table as shown in Figure~\ref{fig:attributes}. 

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
 \centering
\resizebox{0.4\textwidth}{!}{
 \includegraphics[width=0.5\textwidth]{attribute-domain.pdf} 
} 
\caption{The metainformation about attributes represented in database tables.  Left: The table $\it{AttributeColumns}$ specifies which tables and columns that contain the functor values observed in the data. The column name is also the functor ID. Right: The table $\it{Domain}$ lists the domain for each functor.
}
 \label{fig:attributes}
\end{figure}

1Attributes correspond to {\em columns}. These are the only functors that appear in single-table data. 
%, just as random variables in single-table data do. 
Relationship functors have the Boolean domain $\{\true,\false\}$. Relationship functors correspond to {\em tables}, not columns. There are two relationship variables in the diagram~\ref{fig:university-schema} corresponding to the $\it{Registered}$ and $\it{RA}$ relationships. A relationship table stores the information about which entities are related to each other in a certain way. For example, the database instance of Figure~\ref{fig:instance} represents that student Jack took course 101, and that student Kim did not take course 101. Including a relationship random variable in a statistical model allows the model to represent uncertainty about whether or not a relationship exists \cite{linkGetoor2003}. This supports applications like link prediction, (e.g., predicting whether two users are friends, \cite{Liben-Nowell2007}), and multiple link analysis for finding correlations among relationships (e.g., if user $u$ performs a web search for item $i$, is it likely that $u$ watches a video about $i$ ?). 
%
%Managing relationships raises several new issues compared to managing single-table column-based variables. 
To relate a relationship variable to the original relationship data, we need to store pointers to the related entity sets as metainformation. 

We refer to attributes of relationships as \textbf{2Attributes}. In Figure~\ref{fig:university-schema}, there are four 2Attributes corresponding to attributes of Registered (2) and RA (2). An important issue for relational data is that the values of descriptive attributes of relationships are undefined for entities that are not related. Following \cite{Russell2010}, we represent this by introducing a new constant $\it{n/a}$ in the domain of a 2Attribute; see Figure~\ref{fig:attributes} (right). RRV's are called \textbf{1Variables} if their functor is a 1Attribute, \textbf{2Variables} if it is a 2Attribute, and \textbf{RVariables} if it is a relationship.
%\cite{BLOG}, 
%$\it{capability(\P,\S)} = n/a $
%we indicate this by writing 
%%$\it{grade}(\s,\c) = n/a$ 
%$\it{capability(\P,\S)} = n/a $ for a reserved constant $\it{n/a}$. 
%The assertion $\it{capability(\P,\S)}$ = n/a is therefore equivalent to the assertion that $\ra(\P,\S) = \false$.

%\subsection{Generating Default Relational Random Variables}

%We use upper case for Boolean-valued variables with domain $\{\true,\false\}$, such as RVariables. We use
%lower case for other random variables. 

\subsection{Population Variables and Self-Relationships} \label{sec:pvariables}

The Schema Analyzer combines population variables with functors to obtain full RRVs. The random variable database specifies a set of population variables. A population variable refers to an entity set. With each population variable, we store a pointer to the original entity set as metainformation. For example, the $\it{S}$ population variable is associated with the the $\it{Student}$ table in the original database. 
%In the example of Figure~\ref{fig:university-schema}, the type/foreign key constraints on the functors determine which population variables to add, as shown in Table~\ref{table:translation}. In this case adding population variables to functors enhances readability but is redundant given the type information in the schema.

Some relational schemas require more than one population variable for the same entity set. 
This additional expressive power becomes important in the presence of {\em self-relationships} \cite{Heckerman+al:SRL07}. 
A self-relationship relates two entities of the same type.
%, when we need to introduce more than one relationship node for the relationship functor. 
For example, the Mondial database contains a self-relationship $\it{Borders}$ that relates two countries, as shown in the ER diagram Figure~\ref{fig:mondial-er}. As they lead to major conceptual differences between relational and single-table data, we discuss self-relationships in some detail. Neville and Jensen provide evidence that self-relationships are very common, so it is important to accommodate them in a statistical model \cite{Neville2007}. Self-relationships give rise to {\em relational autocorrelations,} where the attribute value of an entity depends probabilistically on the attribute value of related entities \cite{Neville2007}. 
Relational autocorrelations are analogous to temporal autocorrelations in time series analysis, where there is a correlation between the value of a quantity at one time and the value of the same quantity at another. An autocorrelation cannot be represented in a model that contains only one random variable referring to the quantity in question. For time series, a common solution is to introduce {\em two} distinct random variables  that refer to the same quantity, but at different times. For instance, we may have two random variables $\it{temperature}_{t}$ and $\it{temperature}_{t+1}$. Similarly, to represent a relational autocorrelation between the continents of two different countries, we may introduce two random variables $\it{continent}_{\C_{1}}$ and $\it{continent}_{\C_{2}}$. The index notation can equivalently be replaced by function notation, writing $\it{continent}(\C_{1})$ and $\it{continent}(\C_{2})$ \cite{Milch2005}. 
%This leads to the concept of a relational random variable as previously defined. 
%
%In the ER diagram, there are two lines from the $\it{Countries}$ entity set to the $\it{Borders}$ relationship set 
%The ring structure may be represented with two different population variables that each refer to the $\it{Countries}$ entity set. 
The corresponding relationship random variable is $\it{Borders}(\C_{1},\C_{2})$. The different positions of the 1st-order variables can represent different roles in the relationship. A random variable for a descriptive 2Attribute can also be represented using population variables, for instance $\it{Border\_Length}(\C_{1},\C_{2})$. %\textbf{OS: add ER diagram for this simple Mondial example}. 
This notation is expressive enough to represent autocorrelations. 
For instance, the association rule

\begin{small}
$\it{continent}(\C_{1}) = \it{Europe}, \it{Borders}(\C_{1}, \C_{2}) = \true \rightarrow \\\it{continent}(\C_{2}) = \it{Europe}$
\end{small}

expresses that if two countries are neighbors and one is in Europe, the other country is likely to be in Europe. 

Some complex patterns require more than one relationship variable for the same relationship functor. For instance, representing transitivity requires three relationship variables.
%as in 
%$$\it{Borders}(\C_{1}, \C_{2}) = \true,  \it{Borders}(\C_{1}, \C_{3}) = \true \rightarrow \it{Borders}(\C_{1}, \C_{3}) = \true.$$
%Since we introduce two population variables referring to the same entity set, we need to duplicate the 1Nodes for that entity set. 
%This allows the logical language to represent that attributes of neighboring countries influence each other, 
%%Using a second relationship node with the $\it{Borders}$ functor allows us to represent such influences, 
%for instance in the following rule
%$$\it{continent}(\C_{1}) = \it{Europe}, \it{Borders}(\C_{1}, \C_{2}) = \true \rightarrow \it{continent}(\C_{2}).$$
If the database schema contains a self-relationship, 
by default, the Schema Analyzer creates three RVariables and three population variables for the self-relationship.
%, together with the applicable 1Variables.
% (e.g., $\it{continent}(\C_{3}$).
It adds one population variable for each role in the relationship. For each population variable there is a separate set of corresponding 1Variables. In the Mondial example, there are two 1Variables $\it{continent}(\C_{1})$ and $\it{continent}(\C_{2})$.
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
 \centering
\resizebox{0.4\textwidth}{!}{
 \includegraphics[width=0.4\textwidth]{mondial-er.pdf} 
} 
\caption{The ER diagram for Mondial database. $\it{Borders}$ is a self-relationship. 
}
 \label{fig:mondial-er}
\end{figure}

\subsection{SQL Implementation}
The metainformation about the random variables is stored in the \textbf{random variable database} ($\RVD$). The relational schema of the $\RVD$ is shown in Figure~\ref{fig:rv_db}. %\textbf{OS: make 6 relations}. 
The relational schema of the original database is represented in tables in the database system catalog. Given that both the system catalog and the $\RVD$ are databases, we can use SQL to transfer information from one to the other. %\textbf{OS make SQL query} 
We show some of the SQL queries in the Schema Analzyer that the metainformation about 1Attributes from the system catalog in our  university example. The full SQL script, included in the appendix, also constructs tables for 2Attributes and relationship variables, and recognizes self-relationships by using foreign key pointer information. Figure~\ref{fig:rv_db} shows tables that are constructed by the SQL queries in the Schema Analyzer.
%The full script is included in the supplementary material. 


The first SQL query shows the construction of population variables in our university example. It assume the existence of a KeyColumns table that contains triples (TableName, ColumnName, ConstraintName) to record which column in which table is associated with a key constraint. The query uses this
information to identify tables with just one primary key column, which represent entity sets. The name of such tables is concatenated with an index to produce indexed population variables as discussed in Section~\ref{sec:pvariables}. The second query constructs a table listing 1Variables, which are relational random variables representing 1Attributes, identified by a 1Attribute name and a population variable. The use of the natural join operation to combine functors with population variables illustrates how relational algebra elegantly supports defining new statistical objects from existing statistical objects.

%After the random variable database has been created from the system catalog, the user  can add or substract relational random variables depending on the statistical patterns of interest.  such as WinBugs or mode declarations in Aleph.
As we discuss next, the remaining components of MRLBase depend only on the metainformation in the $\RVD$ database, not on the system catalog.
\begin{alltt}
CREATE TABLE PVariables AS  SELECT 
CONCAT(EntityTables.TABLE_NAME, `0') AS pvid,
EntityTables.TABLE_NAME FROM
(SELECT distinct TABLE_NAME, COLUMN_NAME 
FROM   KeyColumns T WHERE
1 = (SELECT  COUNT(COLUMN_NAME)   
FROM   KeyColumns T2  WHERE     
T.TABLE_NAME = T2.TABLE_NAME
AND CONSTRAINT_NAME = 'PRIMARY')) 
as EntityTables 

CREATE TABLE 1Variables AS
SELECT CONCAT(`\`', COLUMN_NAME,
 `(', pvid, `)', `\`') AS 1VarID,
COLUMN_NAME,   pvid  FROM    
PVariables   NATURAL JOIN  AttributeColumns;
\end{alltt}

\begin{figure}[htbp]
\begin{center}
\resizebox{0.5\textwidth}{!}{
%\includegraphics[width=0.5\textwidth]{rv_db.pdf} %move to the appendix
\includegraphics[width=0.5\textwidth]{rv_db_tables.pdf}
}
\caption{Metainformation: Main Tables in the Random Variable Database $\RVD$. 2Variables are relational random variables that represent attributes of binary relationships.
\label{fig:rv_db}}
\end{center}
\end{figure}

\section{The Count Manager} 
A key service for machine learning that a data management system needs to provide is counting how many times a given pattern is instantiated in the data. Such counts are known as {\em sufficient statistics} \cite{Graefe1998}. 
%Many statistical machine learning algorithms do not require full access to the original data, but only to sufficient statistics (hence the name ``sufficient''). So if a database management system is extended to provide sufficient statistics on demand, a learning algorithm for single-table data can be applied to relational database with few modifications. 
%
A novel aspect of MRLBase is managing sufficient statistics in the RDBMS, which has several important advantages. 
%There are several reasons for employing an RDBMS for gathering sufficient statistics. 
%(1). The machine learning application saves expensive data transfer by executing count operations in the database server space rather than local main memory. (2) SQL optimizations for aggregate functions such as SUM and COUNT can be leveraged. (3) Sufficient statistics can be computed once and stored/cached for future reuse in the RDBMS. For many datasets, the number of sufficient statistics runs in the millions and is too big for main memory \cite{Moore1998}.  
%
An RDBMS provides disk storage and fast access for large numbers of sufficient statistics. Previous work has exploited these advantages for single-table data \cite{Ordonez2010}. We discuss how to compute sufficient statistics for a more general situation where sufficient statistics combine information {\em across} different tables in the relational database. In SQL terms, this requires combining aggregate functions with table joins. 


\subsection{Relational Contingency Tables}
%Our starting point is the observation that a statistical learning algorithm like a Bayes net learner does not require an enumeration of individuals tuples, but only {\em sufficient statistics} \cite{Heckerman1995,Schulte2011}. 
%It is well-known in machine learning that a statistical learning algorithm like a Bayes net learner does not require an enumeration of individuals tuples, but only {\em sufficient statistics} \cite{Heckerman1995,Schulte2011}. 

 %A contingency table is defined as follows.
%The Bayes net is learned from the contingency table. 

%The count column in the $\ct$-table represents the number of instantiations for a given tuples of values in a row in the input database. 
Sufficient statistics can be represented in {\em contingency tables} as follows \cite{Moore1998}. 
Consider a fixed list of relational variables.
%$\R_{1}, R_{2},\ldots,R_{m}$, and attribute nodes $\functor_{1},\ldots,\functor_{j}$. 
A \textbf{query} is a set of $(variable = value)$ pairs where each value is of a valid type for the variable. 
The \textbf{result set} of a query in a database $\D$ is the set of instantiations of the population variables such that the query evaluates as true in $\D$.
For example, in the database of Figure~\ref{fig:instance} the result set for the query 
%$(\it{intelligence}(\S) = 2$, $\it{rank}(\S) = 1$, $\it{rating}(\C) = 3$, $\it{diff}(\C) = 1$, $\reg(\S,\C) = F)$
$(\it{intelligence}(\S) = 2$, $\it{rank}(\S) = 1$, $\it{popularity}(\P) = 3$, $\it{teachingability}(\P) = 1$, $\ra(\P,\S) = T)$ 
is the singleton $\{\langle \it{kim}, \it{Oliver}\rangle\}$. 
% $\{\langle \it{kim}, \it{101}\rangle\}$
The \textbf{count} of a query is the cardinality of its result set. 

Every set of variables $\set{V} \equiv \{\V_{1},\ldots,\V_{n} \}$ has an associated \textbf{contingency table} ($\cttable$) denotes by $\cttable(\set{V})$. %$CT(\set{V})$. 
This is a table with a row for each of the possible assignments of values to the variables in $\set{V}$, and a special integer column called $\qcount$. 
The value of the $\qcount$ column in a row 
corresponding to $V_{1} = v_{1},\ldots,V_{n} = v_{n}$ records the count of the 
corresponding query. 
Figure~\ref{fig:ct} shows the contingency table for the university database. The \textbf{contingency table problem} is to compute a contingency table for a target set of variables $\set{V} $ and a given database $\D$. 
%Machine learning algorithms can leverage this capability in several different ways. A pre-counting approach computes a joint contingency table for all relational random variables at once \cite{Moore1998,Qian2014a}. Another possibility is to use postcounting \cite{lv2012}: Rather than precompute a large contingency table prior to learning, compute many small contingency tables for  small subsets of variables on demand during learning. I
%n this paper, we describe an approach to compute contingency tables that leverages SQL capabilities. First, 
MRLBase stores contingency tables as first-class database tables.
 %as in \cite{Qian2014a}. 
 The column headers of a contingency table are IDs for the relational random variables, plus an integer-valued count column. 

%The value of a relationship attribute is undefined for entities that are not related.
%Following \cite{Milch2005}, %\cite{BLOG}, 
%%$\it{capability(\P,\S)} = n/a $
%we indicate this by writing 
%%$\it{grade}(\s,\c) = n/a$ 
%$\it{capability(\P,\S)} = n/a $ for a reserved constant $\it{n/a}$. 
%The assertion $\it{capability(\P,\S)}$ = n/a is therefore equivalent to the assertion that $\ra(\P,\S) = \false$.
%For example, if student $\s$ is not registered in course $\c$, the value of $\it{grade}(\s,\c)$ is undefined. 


\begin{figure}[htbp]
\begin{center}
\resizebox{0.5\textwidth}{!}{
%%!%\includegraphics[width=1\textwidth]{pbn}
%\includegraphics{figures/ct-table.pdf}
\includegraphics[width=0.5\textwidth]{uni-ct-table.pdf}
}
\caption{The contingency table for the university database. % for the attribute-relation table of Figure~\ref{fig:university-tables}
Each row defines a query and its instantiation count in the database. For legibility, we show only functors, not the population variables. %one possible assignment of such variables with its count. 
% where for illustration we have added counts for another student like Jack and another course like 103.
%use partial real table
\label{fig:ct}}
\end{center}
\end{figure}

%A \textbf{conditional contingency table}, written 
%$$\ct(V_{1},\ldots,V_{k}|V_{k+1} = v_{k+1},\ldots, V_{k+m} = v_{k+m})$$
%is the contingency table whose column headers are $V_{1},\ldots,V_{k}$ and whose counts are  defined by subset of instantiations that match the conditions to the right of the $\vert$ symbol.  %  \textbar 



%To illustrate the SQL-based approach, we examine the following problem: precomputing a contingency table for all relational random variables that are associated with a given chain of relationships. A relationship chain is a list $$[\Relation_{1}(\argterms_{1}),\ldots,\Relation_{k}(\argterms_{k})]$$
% of relationship variables such that each relationship variable shares at least one first-order variable with the preceding terms. In SQL terms, a relationship is formed by following foreign key pointers from one relationship table to the next. Sufficient statistics for relationship chains are important for discovering correlations between the attributes of entities that are related by paths of a certain type \cite{Getoor2007c} (called ``metapaths'' in \cite{Sun2012}). 
 
\subsection{Contingency Tables in SQL} The \textbf{Count Database} (CDB) stores a set of contingency tables each defined as a view. We describe how to create these views using SQL.  
Each row in a $\cttable$ represents the count for a conjunctive query in a logical calculus; we refer to these as count-conjunction queries.
It is well-known that conjunctive queries in a logical calculus can be algorithmically translated into relational algebra queries and hence into an SQL query \cite{Ullman1982}. Our approach uses SQL itself to construct the count-conjunction query. We refer to this construction as an SQL \textbf{metaquery}. Metaqueries share the advantages of SQL over using a general programming language discussed in Section~\ref{sec:design}: They are compact, portable, and support automatic updates through the view mechanism.
% (1) Succinctness and portability: Given the metainformation in the random variable database, the meta query is short, mainly requires a few natural joins. The meta query requires only standard SQL, whereas a translation in an application language (e.g., Java) would have to be ported for each different application language. (2) Automatic Update: We can store the result of the meta query as a database view. 
% This means that changes in the original database schema are propagated to contingency tables. For instance, if a column $\it{gpa}$ is added to the $\it{Student}$ table in the original database schema, then the view definition adds this column automatically to contingency tables derived from the $\it{Student}$ table.

A contingency table for a set of random variables can be computed by an SQL count-conjunction query of the form 
\begin{alltt}
CREATE CT-table(<VARIABLE-LIST>) AS
SELECT COUNT(*) AS count, <VARIABLE-LIST>
FROM TABLE-LIST
GROUP BY VARIABLE-LIST
WHERE <Join-Conditions>
\end{alltt}
%As the top level of  Figure~\ref{fig:flow} illustrates, 
%The required counts involving only true relationships can be computed using the standard SQL constructs COUNT(*) and GROUP BY. 
%The general form of these operations is the same for every input database. 
%The varying part is the list of columns to be included, which depends on the input database. 
%For a fixed database, the column lists can be hard-coded. 
%To achieve a general solution when the column list is not known in advance, we introduce a new approach that we refer to as an SQL \textbf{meta query}. 

%Thus an SQL meta query maps schema information to the components of another SQL query. 
 Given a list of $\RRV$'s as input, the meta query is constructed as follows from the metainformation in the random variable DB.  
\begin{description}
\item[FROM LIST] Find the tables referenced by the $\RRV$'s. An $\RRV$ references the entity tables associated with its population variables (see $\RVD.Pvariables$). Relational $\RRV$'s also reference the associated relationship table (see $\RVD.Relationship$). 
\item[WHERE LIST] Add join conditions on the matching primary keys of the referenced tables in the WHERE clause. The primary key columns are recorded in table $\RVD.KeyColumns$. 
\item[SELECT LIST] For each attribute $\RRV$, find the corresponding column name in the original database (see $\RVD.AttributeColumns$). Rename the column with the ID of the $\RRV$.
\end{description}

We represent a count-conjunction query of this form in 
four kinds of tables: the Select, From, Where and Group By tables. The Select table lists the entries in the Select clause of the target query, the From table lists the entries in the From clause, and similar for Where and GROUP BY tables. The entries of the Group By table are the same as in the Select table without the $\qcount$ column.
Given the four query tables, the corresponding SQL count query can be easily executed in an application or stored procedure to construct the contingency table.
% The metaquery accesses tables in the Relational Random Variable Database. %We omit further details due to space constraints.

\begin{figure}[htb]
\begin{center}
\resizebox{0.5\textwidth}{!}{
\includegraphics[width=0.5\textwidth]{meta-query.pdf}
}
\caption{Example of metaqueries and metaquery results based on university database. The parameter $\it{@database@}$ refers to the name of the input database. 
~\label{fig:meta-query} }
\end{center}
\end{figure}

Figure~\ref{fig:meta-query} shows an example of metaqueries for the university database. This metaquery defines a view that in turn defines a contingency table for the random variable list associated with the relationship table $\ra$. This list includes the 1Attributes of professors and of students, as well as the 2Attributes of the $\ra$ relationship. The resulting $\cttable$ is like that of Figure~\ref{fig:ct}, but without the $\it{Reg.}$ column and only with rows where the value of $\ra$ is true. This $\cttable$ can extended to include counts for when $\ra$ is false using the M\"obius Virtual Join~\cite{Schulte2014}.


\section{The Model Manager}

There is a large space of machine learning models, which require support for a diverse set of capabilities. We focus on three services that are required in almost all model selection tasks: 1) Estimating and storing parameter values. 2) Computing one or more model selection scores. 3) Inferring a model prediction for a set of test instances and scoring the model against the true values. 
%We also discuss model structure learning. 


Our case study describes how MRLBase can be used to implement a challenging machine learning application: Constructing a Bayesian network model for a relational database. Managing Bayesian networks are a good illustration of typical challenges and how RDBMS capabilities can address them because: (1) Bayesian networks are a structured graphical model. (2) BN parameters are localized and not simply a flat vector. (3) Bayesian networks are widely regarded as a very useful model class in machine learning and AI, that supports decision making and reasoning under uncertainty. At the same time, they are considered challenging to learn from data. (4) Database researchers have proposed Bayesian networks for combining databases with uncertainty \cite{Wang2008,Deshpande2007}. 
%
%After we have illustrated the our SQL-based approach for BN learning, we discuss how to apply it with other machine learning models.

\subsection{Bayesian Networks for Relational Data}
A {\bf Bayesian Network (BN)} is a directed acyclic graph (DAG) whose nodes comprise a set of random variables and conditional probability parameters.
The parameters of the BN  
are the conditional probabilities of the form, $P(\it{child}|\it{parent\_values}$), that specify the probability of a child node value given an assignment of values to its parents. 
In this paper we consider only Bayesian networks whose nodes are relational random variables (called ``Parametrized Bayesian Networks'' in \cite{Poole2003}). When discussing a BN, we interchangeable refer to its nodes or to its random variables.
%If a Parametrized random variable appears in a Bayes net, we often refer to it simply as a node. 
Figure \ref{fig:pbn} shows a Bayesian network for the University domain (only considering the $\ra$ relationship for simplicity.) 
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
 \centering
\resizebox{0.4\textwidth}{!}{
 \includegraphics[width=0.5\textwidth]{pbn.pdf} 
} 
\caption{(a) Bayesian network for the University domain. (b) Conditional Probability table $Capability(\P,\S)\_\cptable$, for the node $Capability(\P,\S)$. Only value combinations that occur in the data are shown. (c) Contingency Table $Capability(\P,\S)\_\cttable$ for the node $Capability(\P,\S)$ and its parents. Both CP and CT tables are stored as database tables.
}
 \label{fig:pbn}
\label{fig:ct-cp-table}
\end{figure}


%\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
% \centering
%\resizebox{0.4\textwidth}{!}{
% \includegraphics[width=0.5\textwidth]{ct-cp-table.pdf} 
%} 
%\caption{ct-cp-table}
% \label{fig:ct-cp-table}
%\end{figure}

%\subsection{Model Representation} 
\paragraph{SQL Representation} A Bayesian network structure is a directed graph that can be stored straightforwardly in a database table $\it{BayesNet}$ whose columns are $\it{child}$ and $\it{parent}$. The table entries are the IDs of relational random variables defined in the Relational Random Variable database. An entry such as $(\it{Capability}(\P,\S),\it{Salary}(\P,\S))$ means that $\it{Capability}(\P,\S)$ is a child of $\it{Salary}(\P,\S)$. This table is stored in the Models Database $\MDB$. The relational schema for the Models Database is shown in Table~\ref{table:mdb-schema}.

\begin{table}[tbp] \centering
 \begin{tabular}
[c]{|l|}\hline
$\it{BayesNet}$(\underline{$child$,$parent$})\\
$\it{@nodeID@\_\cptable}$(\underline{$@nodeID@$,$parent_{1},\ldots,parent_{k}$},\cpcol)\\ 
$\it{Scores}$(\underline{$nodeID$},$\loglikelihood$,$\parameters$,$\aic$)\\
\hline
\end{tabular}
\caption{The main tables in the Models Database $\MDB$. For a Bayesian network, the Models Database stores its structure, parameter estimates, and model selection scores. The $\it{@nodeID@}$ parameter refers to the ID  of a Bayesian network nodes, for instance $\it{Capability}(\P,\S)$.
\label{table:mdb-schema}} 
\end{table}



\subsection{Parameter Manager}

To make predictions with a model, we need to estimate values for its parameters. MRLBase stores the parameters for a Bayesian network as database tables, called \textbf{conditional probability tables.} Conditional probability tables have the same structure as contingency tables, but with a special column $\cp$ instead of $\it{count}$. Maximizing the data likelihood is the basic parameter estimation method for Bayesian networks. It can be shown that the maximum likelihood estimates use the observed frequency of a child value given its parent values \cite{Neapolitan2004,Schulte2011}. 

\paragraph{SQL Construction of Conditional Probability Tables} Given the sufficient statistics in a contingency table, a conditional probability table containing the maximum likelihood estimates can be computed using SQL as follows. More complex smoothing methods such as the Laplace correction can be easily computed from the maximum likelihood estimates.

\begin{enumerate}
\item For each node, construct a local contingency table whose variable set comprises the node and its parents. This table can be computed from scratch using the count manager, or may already be available as part of model structure learning (see below). 
\item Construct a  parent contingency table whose variable set comprises the parents only:  On the local contingency table, apply a query with SUM(count) AS parent\_count in the Select clause and <parent-list> in the GROUP BY clause. 
\item Carry out a natural join of the local contingency table with the parent contingency table, dividing each local contingency count by the parent\_count. The natural join matches the values of parent variables. 
\end{enumerate}

All of these tables can be stored as views. 
%This means that parameter values are updated automatically when contingency tables are updated, that is, when new data are obtained. 
Figure~\ref{fig:ct-cp-table} shows a local contingency table and a conditional probability table for the node Capability(P, S). 



\subsection{Model Score Computation} \label{sec:model-score}

Model structure learning uses a model selection score to find an optimum model for a given database. Model selection scores can be computed and stored in an RDBMS as well. Caching model selection scores is important for scalable structure learning \cite{Russell2010}. A typical model selection approach is to maximize the likelihood of the data, balanced by a penalty term. For instance, the Akaike Information Criterion (AIC) is defined as follows \cite{Witten2005}.
\[
\mathit{AIC}(\G,\D) \equiv ln(P_{\widehat{G}}(\D)) - \parameters(\G) \]
where $\widehat{G}$ is the BN $\G$ with its parameters instantiated to be the maximum likelihood estimates given the database $\D$, and $\parameters(\G)$ is the number of free parameters in the structure $\G$. Computing this expression requires two terms, the likelihood and the number of parameters. The number of parameters for a node is the product the possible values for the parent nodes, multiplied by (the number of the possible values for the child node -1). 
%One complication is that if a relationship node has the value $\false$, then all associated 2Variables (relationship attributes) must have the value n/a. The computation of the number of parameters must be adjusted to account for this deterministic relationship; for the details see the script in the supplementary material. 
%We discuss how to use SQL for computing each.

Assume that the maximum likelihood estimates are represented in a conditional probability table as discussed above. For a BN $\G$, the log-likelihood function $ln(P_{\widehat{G}}(\D))$ can be computed node by node, as follows \cite{Schulte2011}. For each child node value, and for each combination of parent values: (1) find the instantiation count in the data for the conjunction of child node value and parent node values. (2) Find the conditional probability of the child node value given the parent node values. (3) Multiply the instantiation count by the logarithm of the conditional probability. Finally, sum these products to obtain a total likelihood score for the child node.



\paragraph{SQL Computation of Model Scores} We assume that for each node with ID $@nodeID@$, a conditional probability database table $@nodeID@\_\cptable$ has been built in the Models database $\MDB$. Similarly, we assume that
a local contingency database table $\CDB.@nodeID@\_\cttable$ has been built in the Count Database $\CDB$ (see Figure~\ref{fig:ct-cp-table}). The model likelihood for node $@nodeID@$ can be computed in SQL simply using the natural join of the two tables summing over a row-wise product, as follows. The $\loglikelihood$ value for $@nodeID@$ is inserted into the $\it{Scores}$ table. 
\begin{alltt}
SELECT @nodeID@,  SUM
(MDB.@nodeID@\_CPT.cp * CDB@nodeID@\_CT.count) 
AS loglikelihood
FROM MDB.@nodeID@\_CPT NATURAL JOIN CDB.@nodeID@\_CT
\end{alltt}
%our system creates a view $@nodeID@\_Likelihood$. This view has the same schema as the CP database table $@nodeID@\_\cptable$, except that instead of a conditional probability for a given child-parent configuration, we store a likelihood score. 

%We assume that a local contingency database table $\ct$ and a conditional probability database table $\cptable$ have been built for the child node as describe above (see Figure~\ref{fig:ct-cp-table}). The model likelihood can be computed in SQL simply as the natural join of the two tables, with a new column $\it{likelihood}$ defined by $$\it{likelihood} = \it{ct.count} * log(\cptable.\cp).$$ 

The complex aggregate computation in this short query illustrates how well the high-level SQL constructs support computation with structured objects. Computing the multi-relational likelihood in a general programming language (e.g., Java) would require significant development effort and result in a solution that is less concise, portable, and reliable.

To determine the number of parameters, the number of possible variable values can be found in the $\it{Domain}$ table of the Random Variable Database $\RVD$.  The $\parameters$ number for $@nodeID@$ is also inserted into the $\it{Scores}$ table. The AIC column is then defined as $\aic = \loglikelihood - \parameters$. These values can be inserted directly or the AIC column can be implemented as a derived column. %Given the table $\it{BayesNet}$ that specifies the Bayes net graph, we create a view $\it{NumParameters}$ whose columns are $\it{Node}$ and $\it{Parameters}$. An entry such as $(\it{Capability}(\P,\S),16)$ means that the number of parameters associated with the capability node in the BN model is 16. 
%
Other model selection scores such as BIC and BDeu can be computed in a similar way given the model likelihood and number of parameters.


\section{Test Set Predictions}

A basic procedure for evaluating the accuracy of a machine learning algorithm is the train-and-test paradigm, where the system is provided a training set for learning and then we test its predictions on unseen test cases. %
%
We first discuss how to compute a prediction for a single test case, then how to compute an overall prediction score for a set of test cases. For single-table data, BN prediction is straightforward. A test case is represented by a single predictive feature vector $\set{x}$ and a class label $\y$, and the BN product formula defines a joint probability for $P_{\G}(\set{x},\y)$ and hence a conditional probability $P(\y|\set{x})$. Thus for example, if we want to predict the intelligence of student Jack given that his ranking is 1, a single-table BN would define a conditional probability $P(\it{intelligence}(jack) = \y|\it{rank}(jack)=1)$ where $\y$ is a possible value for intelligence.
For multi-relational data, a prediction model is much more difficult to specify, and 
a number of prediction models have been explored \cite{Dzeroski2001c}. The difficulty is that a test case corresponds to a subdatabase or substructure, not just a single flat feature vector. For example, if we want to predict the intelligence of student Jack given information about his courses and his grades, we have to somehow aggregate the course information. 

\textbf{Log-linear models} are a prominent prediction model class that has performed well with graphical models \cite{Taskar2002,Domingos2009,Sutton2007}, including Bayesian networks \cite{Schulte2012}. The log-linear BN model can briefly be explained in terms of the model likelihood function $P_{G}(\D)$ discussed in Section~\ref{sec:model-score}. Let $\Y$ denote a ground target node to be classified. The term ``ground'' refers to replacing population variables by target instances. For example, a ground target node may be $\it{intelligence}(jack)$. In this example, we refer to Jack as the \textbf{target entity}. Write $\set{X}_{-\Y}$ for a database instance that specifies the values of all ground nodes, except for the target node, which are used to predict the target node. Let $[\set{X}_{-\Y},\y]$ denote the completed database instance where the target node is assigned value $\y$. The log-linear model uses the likelihood function as the joint probability of the label and the predictive features. In symbols, the log-linear model defines
\begin{equation} \label{eq:classify}
P(\y|\set{X_{-\Y}}) \propto P_{G}([\set{X}_{-\Y},\y])
\end{equation}
where the model likelihoods of the possible class labels need to be normalized to define a conditional probabilities. 
%Recall from Section~\ref{sec:model-score} that the log-likelihood is a linear sum over the nodes in the BN, and the parent-child value assignments, of the log-conditional probability of child value given parent values times instance count for the parent-child values. If we view the log-conditional probabilities of child value given parent values as weight parameters, the log-likelihood is a weighted sum of feature counts, which is a type of log-linear equation.
%
%
%$$P(\Y = \y|\set{\X} = \x) \propto \exp(\sum_{i} w_{i} \cdot f_{i}(\y,\x))  $$
%
%where $\Y$ denotes a ground target node to be classified, $\y$ a class label, $\X = \x$ is an assignment of values $\x$ to predictors or input variables $\y$, a feature function $f_{i}$ maps the class label and input variable values to a real number, and $w_{i}$ is a weight parameter for the feature function $f_{i}$. The term ``ground'' refers to replacing population variables by target instances. For example, a ground target node may be $\it{intelligence}(jack)$, meaning that the goal is to predict the intelligence of student Jack. In this example, we refer to Jack as the \textbf{target entity}. A log-linear classification model is associated with the prominent Markov Logic Network model class (Tuffy, Domingos). A log-linear classification model for Bayesian networks can be defined as follows [cite]. For the class variable and for each child of the class variable, there is a feature for each joint assignment of values to child and parents. The associated feature function $f_{i}(\it{child\_value},\it{parent\_values})$ is the instantiation count for the conjunction of child and parent values, {\em given the target instance}. The weight parameter $w_{i} \equiv ln(P((\it{child\_value}|\it{parent\_values}))$ is the log-conditional probability stored in the BN parameter table. For instance, suppose that we want to use the BN of Figure~\ref{fig:ct-cp-table} to predict $\it{intelligence}(jack)$. One feature function in the log-linear equation would correspond to the conjunctive feature $$\it{intelligence}(jack) = hi,\it{capability}(\P,jack) = 1, \it{RA}(\P,Jack) = \true.$$ The feature function is the number of true instances of this query in the data, i.e. it is the number of capability scores 1 that Jack has earned for each professor that he has worked with (i.e., $\it{RA}(\P,Jack) = \true$). This number is multiplied by $\ln(P(\it{intelligence}(jack) = hi|\it{capability}(\P,jack) = 1, \it{RA}(\P,Jack) = \true)$. For further details please see ...

\paragraph{SQL Computation of the Log-Linear Classification Score}

The obvious approach to computing the log-linear score would be to use the computation of Section~\ref{sec:model-score}.
%to find the score $P_{G}(\set{X}_{-\Y,\y})$, then normalize. 
This is inefficient because instance counts that do not involve the target entity do not change the classification probability. For example, if Jack is the target entity, then the grades of Jill do not matter. This means that we need only consider query instantiations that match the appropriate population variable with the target entity (e.g., $\S = Jack$). For a given set of random variables, such query instantiation counts can be represented in a contingency table that we call the \textbf{target contingency table}. Figure~\ref{fig:targetc} shows the format of a contingency table for target entities Jack and Jill.

\begin{table*}[t]
\caption{SQL queries for computing target contingency tables supporting test set prediction. <Attribute-List> and <Key-Equality-List> are as in Figure~\ref{fig:meta-query}.}
\begin{center}
\begin{tabular}{|c||p{6.5cm}|p{4cm}|p{5cm}}
Access &SELECT&WHERE&GROUP BY\\\hline
Single &COUNT(*) AS count, <Attribute-List>, S.sid&<Key-Equality-List> AND S.s\_id = jack& <Attribute-List>\\
\hline
Block & COUNT(*) AS count, <Attribute-List>, S.sid&<Key-Equality-List> & <Attribute-List>, S.sid\\
\end{tabular}
\end{center}
\label{table:target-query}
\end{table*}%

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
 \centering
\resizebox{0.4\textwidth}{!}{
 \includegraphics{targetct} 
} 
\caption{Target contingency tables for target = Jack and for target = Jill}
 \label{fig:targetc}
\end{figure}

{\em Assuming} that for each node with ID $@nodeID@$, a target contingency table $\CDB.@nodeID,target@\_\cttable$ has been built in the Count Database $\CDB$, the log-likelihood SQL is as in Section~\ref{sec:model-score}:
\begin{alltt}
SELECT @nodeID@,  SUM
(MDB.@nodeID@\_CPT.CP * CDB.@nodeID,target@\_CT.count) 
AS loglikelihood
FROM MDB.@nodeID@\_CPT NAT. JOIN CDB.@nodeID,target@\_CT
\end{alltt}
%The conditional probability database table $@nodeID,target@\_\cptable$ can be found in the Models database $\MDB$. (Notice that the conditional probabilities do not depend on the target entity, only on the model.)  
This query computes the classification score for a BN node, the total score is the sum over BN nodes. As is well-known in Bayes net theory, we need only sum scores for the target node and its children \cite{Neapolitan2004}. 
%
The new problem is finding the target contingency table. SQL allows us to solve this very easily by restricting counts to target entity in the WHERE clause. To illustrate, suppose we want to modify the contingency table query of Figure~\ref{fig:meta-query} to compute the contingency table for $\S = Jack$. We add the student id to the SELECT clause, and the join condition $S.s\_id = jack$ to the WHERE clause; see Table~\ref{table:target-query}. (Omit apostrophes for readability.) The FROM clause is the same as in Figure~\ref{fig:meta-query}. The metaquery of Figure~\ref{fig:meta-query} is easily changed to produce these SELECT and WHERE CLAUSES. 


%First we add the student id to the SELECT clause, so the new SELECT clause becomes
%
%\begin{alltt}
%COUNT(*) AS count, popularity(P), teachingability(P),
%intelligence(S), ranking(S), \em{S.sid}
%\end{alltt}
%
%and the new WHERE list becomes
%
%\begin{alltt}
%RA.p\_id = P.p\_id AND RA.s\_id = S.s\_id AND \em{S.s\_id = jack}.
%\end{alltt}

%We emphasize the new parts of the queries and omit apostrophes for readability. 
%
Next consider a setting where a model is to be scored against an entire test set. 
%This occurs for instance in the standard cross-validation computation for model scoring. 
For concreteness, suppose the problem is to predict the intelligence of a set of students
 $\it{intelligence}(jack)$, $\it{intelligence}(jill)$,
 $\it{intelligence}(student_{3}),\ldots, \it{intelligence}(student_{m})$.
 An obvious approach is to loop through the set of test instances, repeating the likelihood query above for each single instance. Instead, SQL supports {\em block access} where we process the test instances as a block. Intuitively, instead of building a contingency table for each test instance, we build a single contingency table that stacks together the individual contingency tables (Figure~\ref{fig:targetc}). Blocked access can be implemented in a beautifully simple manner in SQL: we simply add the primary key id field for the target entity to the GROUP BY list; see Table~\ref{table:target-query}. 
%To illustrate,  suppose for simplicity that the $\it{Student}$ table contains all test target entities. The query for the stacked contingency table is constructed as in Figure~\ref{fig:meta-query}, with the change that: We add the $\it{S.sid}$ field to the SELECT and to the GROUP BY lists. The FROM and WHERE clauses are as in Figure~\ref{fig:meta-query}. 
 
In contrast, programming blocked access in a general purpose programming language would require significant development effort and probably new file or data structures. If the set of test instances is large, a stacked contingency table is also unlikely to fit into main memory.
  %
To sum up, the log-likelihood computation of Section~\ref{sec:model-score} can be easily adapted to compute a log-linear classification score by adjusting the contingency table counts to the target entity. Only one or two small modifications to the log-likelihood SQL queries are required. This illustrates the modularity of MRLBase and the reusability of its components for different machine learning tasks. 


\section{Evaluation} 
We describe the system and the datasets we used.
Code was written in MySQL Script and Java, JRE 1.7.0.  and executed with 8GB of RAM and a single Intel Core 2 QUAD Processor Q6700 with a clock speed of 2.66GHz (no hyper-threading). The operating system was Linux Centos 2.6.32. 
The MySQL Server version 5.5.34 was run with 8GB of RAM and a single core processor of 2.2GHz. 
All code and datasets are available on-line (pointer omitted for blind review). 


\subsection{Datasets}
We used six benchmark real-world databases. For detailed descriptions and  the sources of the databases, please see reference~\cite{Schulte2012}. Table~\ref{table:datasetsize} summarizes basic information about the benchmark datasets.  
IMDB is the largest dataset in terms of number of total tuples (more than 1.3M tuples) and schema complexity. %attributes.
It combines the MovieLens database\footnote{www.grouplens.org, 1M version} with data from the Internet Movie Database (IMDB)\footnote{www.imdb.com, July 2013} following \cite{Peralta2007}. 

For Bayesian network structure learning, we used MRLBase to implement the previously existing learn-and-join algorithm (LAJ). The LAJ method takes as input a joint contingency table for all relational random variables in the  database, which we computed using the Count Manager. The model search strategy of the LAJ algorithm is an iterative deepening search for correlations among attributes along longer and longer chains of relationships. For more details please see \cite{Schulte2012}. 


\begin{table}[hbtp] \centering
%\scalebox{0.7in}{
\resizebox{0.5\textwidth}{!}{
\begin{tabular}[c]
{|l|c|c|r|c|}\hline
 \textbf{Dataset} & \textbf{\begin{tabular}[l] {ll} \#Relationship \\Tables/ Total \end {tabular}} & \textbf{\begin{tabular}[l] {ll} \#Self \\Relationships\end {tabular}}  & \textbf{\#Tuples} 
%& \textbf{\#Attributes}  
\\\hline
% \textbf{Dataset} & \textbf{Relationships} & \textbf{\begin{tabular}[l] {ll} Self \\Relationships \end {tabular}} &
% \textbf{\begin{tabular}[l] {ll} Same Type\\ Relationships \end {tabular}}& \textbf{\#Tuples} & \textbf{\begin{tabular}[l] {ll} \#Attribute  \\Columns \end{tabular}}  \\\hline
 %   University&2 & 0 & N & 171 & 12\\\hline
    Movielens &1 / 3 & 0  & 1,010,051 
%& 7
\\\hline
%    Movielens(0.1M) &1 & N & N &  83,402 & 7\\\hline
    Mutagenesis & 2 / 4 & 0 & 14,540 
%& 11
\\\hline
 UW-CSE &2 / 4 & \textbf{2}  & 712 %& 14
\\\hline   
  Mondial &2 / 4 & \textbf{1} &  870
%& 18
\\\hline
    %Financial &3 / 7 & 0  &  225,932& 15\\\hline
   Hepatitis &3 / 7 & 0 &12,927  
%& 19
\\\hline
   IMDB &3 / 7 & 0 &1,354,134  %& 17
\\\hline   
\end{tabular}
}
 % end scalebox
\caption{Datasets characteristics. \#Tuples = total number of tuples over all tables in the dataset. 
  \label{table:datasetsize}}
\end{table}



Table~\ref{tab:rvd} provides information about the number of relational random variables generated for each database, and the number of tuples required to store metainformation about them. More complex schemas and self-relationships lead to more random variables. 
%
%The metainformation requires little storage. The reason for using an RDBMS to store it is not because it requires large memory, but because relational algebra is an excellent language for manipulating structured objects.

% Table generated by Excel2LaTeX from sheet 'model-manager'
\begin{table}[htbp]
  \centering
    \begin{tabular}{|l|c|c|}
    \hline
   \textbf{Dataset} &\textbf{\# RRV}&  \textbf{\# Tuples in \RVD}  \\
    \hline
    Movielens & 7     & 72   \\
    \hline
    Mutagenesis & 11    & 124   \\
    \hline
    UW-CSE & 14    & 112   \\
    \hline
    Mondial & 18    & 141   \\
    \hline
    Hepatitis & 19    & 207   \\
    \hline
    IMDB  & 17    & 195   \\
    \hline
    \end{tabular}%
  %
  \caption{Random variable database statistics \label{tab:rvd}}
\end{table}%




%% Table generated by Excel2LaTeX from sheet 'Sheet1'
%\begin{table}[htbp]
%  \centering
%   \resizebox{0.5\textwidth}{!}{ \begin{tabular}{|l|r|r|r|r|r|}
%    \hline
% \textbf{  Dataset  }&  \textbf{ \begin{tabular}[l] {ll} \# Database\\Tuples  \end{tabular} }&\textbf{ \begin{tabular}[l] {ll}\# Sufficient \\ Statistics \end{tabular}}& \textbf{\begin{tabular}[l] {ll} SS \\Computing\\ Time (s) \end{tabular}} &\textbf{\begin{tabular}[l] {ll}  \#BN \\ Parameters  \end{tabular}} & \textbf{\begin{tabular}[l] {ll}Para. \\Learning\\ Time (s) \end{tabular} }\\
%    \hline
%    Movielens & 1,010,051 & 252   & 2.7   & 292   & 0.57  \\
%    \hline
%    Mutagenesis & 14,540 & 1,631 & 1.67  & 721   & 0.98  \\
%    \hline
%    UW-CSE & 712   & 2,828 & 3.84  & 241   & 1.14  \\
%    \hline
%    Mondial & 870   & 1,746,870 & 1,112.84 & 339   & 60.55  \\
%    \hline
%%    Financial & 225,932 & 3,013,011 & 1,421.87 & 2,433  & 88.974  \\
%%    \hline
%    Hepatitis & 12,927 & 12,374,892 & 3,536.76 & 569   & 429.15  \\
%    \hline
%    IMDB  & 1,354,134 & 15,538,430 & 7,467.85 & 60,059 & 505.61  \\
%    \hline
%    \end{tabular}%
%}
%  \label{tab:addlabel}%
%  \caption{Count Manager}
%\end{table}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'

The number of sufficient statistics reported in Table~\ref{tab:counts}  is that for constructing the joint contingency table required for the Learn-and-Join algorithm. This number depends mainly on the number of random variables. The number of sufficient statistics can be quite large, over 15M for the largest dataset IMDB. RDBMS support is key for managing counts in such cases. Even with such large numbers, constructing contingency tables using the SQL metaqueries is feasible, taking just over 2 hours for the very large IMDB set. The number of Bayesian network parameters is much smaller than the number of sufficient statistics because it depends mainly on the indegree of the nodes. The Bayesian network can be seen as a compact representation of the statistical information in the joint contingency table \cite{Schulte2014}. So the difference between the number of parameters and the number of sufficient statistics measures how compactly the BN summarizes the statistical information in the data.  Table~\ref{tab:counts} shows that Bayesian networks provide very compact summaries of the data statistics. For instance for the Hepatitis dataset, the ratio is  12,374,892/569 > 20,000. The IMDB database is an outlier, showing a complex correlation pattern that leads to a dense Bayesian network structure.

\begin{table}[htbp]
  \centering
   \resizebox{0.5\textwidth}{!}{ \begin{tabular}{|l|r|r|r|r|r|}
    \hline
 \textbf{  Dataset  }&  \textbf{ \begin{tabular}[l] {ll} \# Database\\Tuples  \end{tabular} }&\textbf{ \begin{tabular}[l] {ll}\# Sufficient \\ Statistics (SS) \end{tabular}}& \textbf{\begin{tabular}[l] {ll} SS \\Computing\\ Time (s) \end{tabular}} &\textbf{\begin{tabular}[l] {ll}  \#BN \\ Parameters  \end{tabular}}%& \textbf{\begin{tabular}[l] {ll}Para. \\Learning\\ Time (s) \end{tabular} }
\\
    \hline
    Movielens & 1,010,051 & 252   & 2.7   & 292   \\%& 0.57  \\
    \hline
    Mutagenesis & 14,540 & 1,631 & 1.67  & 721   \\%& 0.98  \\
    \hline
    UW-CSE & 712   & 2,828 & 3.84  & 241   \\%& 1.14  \\
    \hline
    Mondial & 870   & 1,746,870 & 1,112.84 & 339  \\% & 60.55  \\
    \hline
%    Financial & 225,932 & 3,013,011 & 1,421.87 & 2,433  & 88.974  \\
%    \hline
    Hepatitis & 12,927 & 12,374,892 & 3,536.76 & 569   \\%& 429.15  \\
    \hline
    IMDB  & 1,354,134 & 15,538,430 & 7,467.85 & 60,059 \\%& 505.61  \\
    \hline
    \end{tabular}%
}
  \caption{Count Manager: Sufficient Statistics and Parameters \label{tab:counts}}
\end{table}%

Table~\ref{tab:model} shows that the graph structure of a Bayesian network contains a small number of edges relative to the number of parameters. 
%The reason for using an RDBMS to store the graph is again because relational algebra allows us to easily combine it with other statistical objects. 
The parameter manager provides fast maximum likelihood estimates for a given structure. This is because the variable set for a local contingency tables for BN parameter estimation comprises only a child node and its parents, so it is much smaller than the variable set in the joint contingency table.


% Table generated by Excel2LaTeX from sheet 'model-manager'
\begin{table}[htbp]
  \centering
      \resizebox{0.5\textwidth}{!}{ \begin{tabular}{|l|r|r|r|r|}
    \hline
     \textbf{ Dataset} %&  \textbf{ \begin{tabular}[l] {ll}\# Tuples In \\  RRVD \end{tabular} } 
&  \textbf{ \begin{tabular}[l] {ll} \# Tuples in \\Bayes Net  \end{tabular} }& \textbf{ \begin{tabular}[l] {ll} \# Bayes Net \\ Parameters   \end{tabular} } & \textbf{\begin{tabular}[l] {ll}Para. \\Learning\\ Time (s) \end{tabular} }\\
    \hline
    Movielens &   72    %& 12
    & 292 & 0.57  \\
    \hline
    Mutagenesis &   124   % & 57
    & 721 & 0.98  \\
    \hline
    UW-CSE &       112 %& 118
   & 241  & 1.14 \\
    \hline
    Mondial &       141 %& 126 
  & 339 & 60.55    \\
    \hline
    Hepatitis &       207%& 142 
  & 569  & 429.15   \\
    \hline
    IMDB  &      195 %& 217  
 & 60,059   & 505.61\\
    \hline
    \end{tabular}%
}  \caption{Model Manager Evaluation.}
  \label{tab:model}%
\end{table}%

Figure~\ref{fig:test-timing} compares computing predictions on a test set using an instance-by-instance loop, with a separate SQL query for each instance, vs. a single SQL query for all test instances as a block. The blocked access method is 1,000-10,000 faster. 


\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
 \centering
\resizebox{0.4\textwidth}{!}{
 \includegraphics[width=0.5\textwidth]{test-timing.pdf} 
} 
\caption{Times (s) for Computing Predictions on Test Instances. The right red column shows the time for looping over single instances using the Single Access Query of Table~\ref{table:target-query}. The left blue column shows the time for the Blocked Access Query of Table~\ref{table:target-query}.
}
 \label{fig:test-timing}
\end{figure}

%\begin{table}[htbp]
%  \centering
%      \resizebox{0.5\textwidth}{!}{ \begin{tabular}{|l|r|r|r|r|}
%    \hline
%     \textbf{ Dataset} %&  \textbf{ \begin{tabular}[l] {ll}\# Tuples In \\  RRVD \end{tabular} } 
%&  \textbf{ \begin{tabular}[l] {ll} \# Tuples in \\Bayes Net  \end{tabular} }& \textbf{ \begin{tabular}[l] {ll} \# Bayes Net \\ Parameters   \end{tabular} } & \textbf{\begin{tabular}[l] {ll}Para. \\Learning\\ Time (s) \end{tabular} }\\
%    \hline
%    Movielens &   72    %& 12
%    & 292 & 0.57  \\
%    \hline
%    Mutagenesis &   124   % & 57
%    & 721 & 0.98  \\
%    \hline
%    UW-CSE &       112 %& 118
%   & 241  & 1.14 \\
%    \hline
%    Mondial &       141 %& 126 
%  & 339 & 60.55    \\
%    \hline
%    Hepatitis &       207%& 142 
%  & 569  & 429.15   \\
%    \hline
%    IMDB  &      195 %& 217  
% & 60,059   & 505.61\\
%    \hline
%    \end{tabular}%
%}  \caption{Model Manager Evaluation.}
%  \label{tab:model}%
%\end{table}%


Table~\ref{tab:othersrl} reports result for the complete learning of a Bayesian network, structure and parameters. It
benchmarks MRLBase against functional gradient boosting, a state-of-the-art  multi-relational learning approach that construct a graphical model with parameter estimates\cite{Natarajan2012}. MLN\_Boost learns a Markov Logic Network, and RDN\_Boost a Relational Dependency Network. 
We used the Boostr implementation \cite{Khot2013}. To make the results easier to compare across databases and systems, we divide the total running time by the number of random variables for the database (Table~\ref{tab:rvd}). Table~\ref{tab:othersrl} shows that structure learning with MRLBase is fast: even the large complex database IMDB requires only around 8 minutes/node. Compared to the boosting methods, MRLBase shows excellent scalability: the competitor methods do not terminate on IMDB database, and while RDN\_Boost terminates on the MovieLens database, it is almost 5,000 times slower than MRLBase. While the Learn-and-Join algorithm is more efficient than the boosting ensemble search, much of its speed is due to quick computation of sufficient statistics. As the last column of Table~\ref{tab:othersrl} shows, on the larger datasets MRLBase spends about 80\% of computation time on gathering sufficient statistics. This suggests that a large  speedup for the boosting algorithms could be achieved if they used the MRLBase approach. 


We do not report accuracy results due to space constraints and because predictive accuracy is not the focus of this paper. On the standard conditional log-likelihood metric, as defined by Equation~\ref{eq:classify}, the BN learned by MRLBase performs better than the boosting methods on all databases. This is consistent with the results of previous studies \cite{Schulte2012}.

\begin{table}[htbp]
  \centering
      \resizebox{0.5\textwidth}{!}{
\begin{tabular}{|l|r|r|r|r|}\hline
Dataset  & RDN\_Boost  & MLN\_Boost  & MRLBase & MRLBase-CT \\\hline
MovieLens & 92.7min  & N/T & 1.12 & 0.39 \\\hline
Mutagenesis  & 118 & 49 & 1 & 0.15 \\\hline
UW-CSE & 15 & 19 & 1 & 0.27 \\\hline
Mondial  & 27 & 42 & 102 & 61.82 \\\hline
Hepatitis  & 251 & 230 & 286 & 186.15 \\\hline
IMDB & N/T & N/T & 524.25 & 439.29 \\\hline
\end{tabular}
}  \caption{Learning Time Comparison with other multi-relational learning systems. Unless otherwise noted, times are in seconds.}
  \label{tab:othersrl}%
\end{table}%

{\em Conclusion.} MRLBase leverages RDBMS capabilities for scalable management of statistical analysis objects. It efficiently constructs and stores large numbers of sufficient statistics and parameter estimates. 
%MRLBase can integrate information from different statistical objects when they are maintained as first-class citizens in the RDBMS. 
The RDBMS support for multi-relational learning translates into orders of magnitude improvements in speed and scalability.
% compared to other state-of-the-art methods.


\section{Conclusion and Future Work} 
Compared to traditional learning with a single data table, learning for multi-relational data requires new system capabilities. In this paper we described MRLBase, a system that leverages the existing capabilities of an SQL-based RDBMS to support multi-relational learning. Representational tasks include specifying metainformation about structured random variables that are appropriate for multi-relational data, and storing the structure of a learned model. Computational tasks include storing and constructing sufficient statistics (event counts), and computing parameter estimates and model selection scores based on the sufficient statistics. 
We showed that SQL scripts can be used to implement these capabilities, with multiple advantages. These advantages include: 1) Fast program development through high-level SQL constructs for complex table and count operations. 2) Managing large and complex statistical objects that are too big to fit in main memory. For instance, some of our benchmark databases require storing and querying millions of sufficient statistics. Empirical evaluation on six benchmark databases showed significant scalability advantages from utilizing the RDBMS capabilities: Both structure and parameter learning scaled well to millions of data records, beyond what previous multi-relational learning systems can achieve. For the important task of computing prediction scores for test instances, block access provided by the RDBMS runs orders of magnitude faster than a loop through test instances. 

{\em Future Work.} On the RDBMS side, our implementation has used simple SQL plus indexes. Further optimizations are likely possible, especially for view materialization and the key scalability bottleneck of computing multi-relational sufficient statistics. An important direction is to integrate MRLBase with probabilistic inference systems that also utilize RDBMS capabilities, such as BayesStore and Tuffy. Further potential application areas for MRLBase include managing massive numbers of aggregate features for classification \cite{Popescul2007}, and collective matrix factorization \cite{Singh2008}. 
%Another research direction is to exploit the capabilities of other advanced data management system for multi-relational learning, in addition to multi-relational databases. These include OLAP systems for hierarchical structures and Hadoop systems for distributed computations. 

In sum, we believe that the succesful use of SQL presented in this paper shows that relational algebra can play the same role for multi-relational learning as linear algebra for single-table learning: a unified language for both representing statistical objects and for computing with them.
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{master,master1,master2,master3,master4} 



%ACKNOWLEDGMENTS are optional
%\section{Acknowledgments}
%
%This research was supported by a Discovery grant to Oliver Schulte by the Natural Sciences and Engineering Research Council of Canada. 
%Zhensong Qian was supported by a grant from the China Scholarship Council.
%We thank the organizers for providing a discussion venue, and the anonymous reviewers for constructive criticism.




 % vldb_sample.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references

%\subsection{References}

%%APPENDIX is optional.
%% ****************** APPENDIX **************************************
%% Example of an appendix; typically would start on a new page
%\newpage
%\pagebreak
%
\begin{appendix}

\section{The Random Variable Database Layout}
We provide details about the Schema Analyzer. Table~\ref{table:rvdb1} shows the relational schema of the Random Variable Database. Figure~\ref{fig:rv_db1} shows dependencies between the tables of this schema. 

\begin{table}[htbp]
  \centering
\resizebox{0.5\textwidth}{!}{
    \begin{tabular}{|r|r|r|r|r|r|}
    \hline
    \multicolumn{2}{|c|}{Table Name} & \multicolumn{4}{c|}{Schema}  \\
    \hline
    \multicolumn{2}{|l|}{AttributeColumns} & \multicolumn{4}{l|}{\begin{tabular}{l}TABLE\_NAME,  COLUMN\_NAME   \end{tabular}}  \\
    \hline
    \multicolumn{2}{|l|}{Domain} & \multicolumn{4}{l|}{\begin{tabular}{l}COLUMN\_NAME, VALUE   \end{tabular}}  \\
    \hline
    \multicolumn{2}{|l|}{Pvariables} & \multicolumn{4}{l|}{\begin{tabular}{l}Pvid,  TABLE\_NAME  \end{tabular}}  \\
    \hline
    \multicolumn{2}{|l|}{1Variables} & \multicolumn{4}{l|}{\begin{tabular}{l}1VarID,  COLUMN\_NAME,  Pvid 
\end{tabular} }  \\
    \hline
    \multicolumn{2}{|l|}{2Variables } & \multicolumn{4}{l|}{\begin{tabular}{ll} 2VarID,  COLUMN\_NAME,  Pvid1,  Pvid2, \\ TABLE\_NAME \end{tabular}}  \\
    \hline
    \multicolumn{2}{|l|}{Relationship} & \multicolumn{4}{l|}{\begin{tabular}{lll}RVarID,  TABLE\_NAME, Pvid1,  Pvid2, \\ COLUMN\_NAME1,  COLUMN\_NAME2  \end{tabular}}  \\
    \hline
    \end{tabular}%
}
  \caption{Schema for Random Variable Database}
  \label{table:rvdb1}%
\end{table}%


\begin{figure}[htbp]
\begin{center}
\resizebox{0.5\textwidth}{!}{
\includegraphics[width=0.5\textwidth]{rv_db.pdf} %move to the appendix
%\includegraphics[width=0.5\textwidth]{rv_db_tables.pdf}
}
\caption{Tables  Dependency in the Random Variable Database $\RVD$.
\label{fig:rv_db1}}
\end{center}
\end{figure}

\section{MySQL Script for creating default random variables.}

\begin{scriptsize}
\begin{alltt}
/*AchemaAnalyzer.sql*/
DROP SCHEMA IF EXISTS @database@_AchemaAnalyzer; 
CREATE SCHEMA  @database@_AchemaAnalyzer;

CREATE SCHEMA  if not exists @database@_BN;
CREATE SCHEMA  if not exists @database@_CT;

USE @database@_AchemaAnalyzer;
SET storage_engine=INNODB;

CREATE TABLE Schema_Key_Info AS SELECT TABLE_NAME, COLUMN_NAME,
REFERENCED_TABLE_NAME, REFERENCED_COLUMN_NAME, CONSTRAINT_NAME FROM
INFORMATION_SCHEMA.KEY_COLUMN_USAGE WHERE (KEY_COLUMN_USAGE.TABLE_SCHEMA =
'@database@') ORDER BY TABLE_NAME;

CREATE TABLE Schema_Position_Info AS SELECT COLUMNS.TABLE_NAME,
COLUMNS.COLUMN_NAME,
COLUMNS.ORDINAL_POSITION FROM
INFORMATION_SCHEMA.COLUMNS,
INFORMATION_SCHEMA.TABLES
WHERE
(COLUMNS.TABLE_SCHEMA = '@database@'
    AND TABLES.TABLE_SCHEMA = '@database@'
    AND TABLES.TABLE_NAME = COLUMNS.TABLE_NAME
    AND TABLES.TABLE_TYPE = 'BASE TABLE')
ORDER BY TABLE_NAME;

CREATE TABLE NoPKeys AS SELECT TABLE_NAME FROM
Schema_Key_Info
WHERE
TABLE_NAME NOT IN (SELECT 
        TABLE_NAME
    FROM
        Schema_Key_Info
    WHERE
        CONSTRAINT_NAME LIKE 'PRIMARY');

CREATE table NumEntityColumns AS
SELECT 
    TABLE_NAME, COUNT(DISTINCT COLUMN_NAME) num
FROM
    Schema_Key_Info
WHERE
    CONSTRAINT_NAME LIKE 'PRIMARY'
        OR REFERENCED_COLUMN_NAME IS NOT NULL
GROUP BY TABLE_NAME;

CREATE TABLE TernaryRelations as SELECT TABLE_NAME FROM
NumEntityColumns
WHERE
num > 2;


CREATE TABLE AttributeColumns AS SELECT TABLE_NAME, COLUMN_NAME FROM
Schema_Position_Info
WHERE
(TABLE_NAME , COLUMN_NAME) NOT IN (SELECT 
        TABLE_NAME, COLUMN_NAME
    FROM
        KeyColumns)
    and TABLE_NAME NOT IN (SELECT 
        TABLE_NAME
    FROM
        NoPKeys)
    and TABLE_NAME NOT IN (SELECT 
        TABLE_NAME
    FROM
        TernaryRelations);

ALTER TABLE AttributeColumns ADD PRIMARY KEY (TABLE_NAME,COLUMN_NAME);

CREATE TABLE InputColumns AS SELECT * FROM
KeyColumns
WHERE
CONSTRAINT_NAME = 'PRIMARY'
ORDER BY TABLE_NAME;

CREATE TABLE ForeignKeyColumns AS SELECT * FROM
KeyColumns
WHERE
REFERENCED_COLUMN_NAME IS NOT NULL
ORDER BY TABLE_NAME;

ALTER TABLE ForeignKeyColumns ADD PRIMARY KEY (TABLE_NAME,COLUMN_NAME,
REFERENCED_TABLE_NAME);

CREATE TABLE EntityTables AS SELECT distinct TABLE_NAME, COLUMN_NAME 
FROM
KeyColumns T
WHERE
1 = (SELECT 
        COUNT(COLUMN_NAME)
    FROM
        KeyColumns T2
    WHERE
        T.TABLE_NAME = T2.TABLE_NAME
            AND CONSTRAINT_NAME = 'PRIMARY');

ALTER TABLE EntityTables ADD PRIMARY KEY (TABLE_NAME,COLUMN_NAME);

CREATE TABLE SelfRelationships AS SELECT DISTINCT RTables1.TABLE_NAME 
AS TABLE_NAME,
RTables1.REFERENCED_TABLE_NAME AS REFERENCED_TABLE_NAME,
RTables1.REFERENCED_COLUMN_NAME AS REFERENCED_COLUMN_NAME FROM
KeyColumns AS RTables1,
KeyColumns AS RTables2
WHERE
(RTables1.TABLE_NAME = RTables2.TABLE_NAME)
    AND (RTables1.REFERENCED_TABLE_NAME = RTables2.REFERENCED_TABLE_NAME)
    AND (RTables1.REFERENCED_COLUMN_NAME = RTables2.REFERENCED_COLUMN_NAME)
    AND (RTables1.ORDINAL_POSITION < RTables2.ORDINAL_POSITION);

ALTER TABLE SelfRelationships ADD PRIMARY KEY (TABLE_NAME);

CREATE TABLE Many_OneRelationships AS SELECT KeyColumns1.TABLE_NAME FROM
KeyColumns AS KeyColumns1,
KeyColumns AS KeyColumns2
WHERE
(KeyColumns1.TABLE_NAME , KeyColumns1.COLUMN_NAME) IN (SELECT 
        TABLE_NAME, COLUMN_NAME
    FROM
        InputColumns)
    AND (KeyColumns2.TABLE_NAME , KeyColumns2.COLUMN_NAME) IN (SELECT 
        TABLE_NAME, COLUMN_NAME
    FROM
        ForeignKeyColumns)
    AND (KeyColumns2.TABLE_NAME , KeyColumns2.COLUMN_NAME) NOT IN (SELECT 
        TABLE_NAME, COLUMN_NAME
    FROM
        InputColumns);

CREATE TABLE PVariables AS SELECT CONCAT(EntityTables.TABLE_NAME, '0') AS Pvid,
EntityTables.TABLE_NAME,
0 AS index_number FROM
EntityTables 
UNION 
SELECT 
CONCAT(EntityTables.TABLE_NAME, '1') AS Pvid,
EntityTables.TABLE_NAME,
1 AS index_number
FROM
EntityTables,
SelfRelationships
WHERE
EntityTables.TABLE_NAME = SelfRelationships.REFERENCED_TABLE_NAME
    AND EntityTables.COLUMN_NAME = SelfRelationships.REFERENCED_COLUMN_NAME ;

ALTER TABLE PVariables ADD PRIMARY KEY (Pvid);

CREATE TABLE RelationTables AS SELECT DISTINCT ForeignKeyColumns.TABLE_NAME,
ForeignKeyColumns.TABLE_NAME IN (SELECT 
        TABLE_NAME
    FROM
        SelfRelationships) AS SelfRelationship,
ForeignKeyColumns.TABLE_NAME IN (SELECT 
        TABLE_NAME
    FROM
        Many_OneRelationships) AS Many_OneRelationship FROM
ForeignKeyColumns;

ALTER TABLE RelationTables ADD PRIMARY KEY (TABLE_NAME);

CREATE TABLE 1Variables AS SELECT CONCAT('`', COLUMN_NAME, '(', Pvid, ')', '`') AS 1VarID,
COLUMN_NAME,
Pvid,
index_number = 0 AS main FROM
PVariables
    NATURAL JOIN
AttributeColumns;

ALTER TABLE 1Variables ADD PRIMARY KEY (1VarID);
ALTER TABLE 1Variables ADD UNIQUE(Pvid,COLUMN_NAME);

CREATE TABLE ForeignKeys_pvars AS SELECT ForeignKeyColumns.TABLE_NAME,
ForeignKeyColumns.REFERENCED_TABLE_NAME,
ForeignKeyColumns.COLUMN_NAME,
Pvid,
index_number,
ORDINAL_POSITION AS ARGUMENT_POSITION FROM
ForeignKeyColumns,
PVariables
WHERE
PVariables.TABLE_NAME = REFERENCED_TABLE_NAME;

ALTER TABLE ForeignKeys_pvars ADD PRIMARY KEY (TABLE_NAME,Pvid,
ARGUMENT_POSITION);

CREATE table Relationship_MM_NotSelf AS
SELECT 
    CONCAT('`',
            ForeignKeys_pvars1.TABLE_NAME,
            '(',
            ForeignKeys_pvars1.Pvid,
            ',',
            ForeignKeys_pvars2.Pvid,
            ')',
            '`') AS orig_RVarID,
    ForeignKeys_pvars1.TABLE_NAME,
    ForeignKeys_pvars1.Pvid AS Pvid1,
    ForeignKeys_pvars2.Pvid AS Pvid2,
    ForeignKeys_pvars1.COLUMN_NAME AS COLUMN_NAME1,
    ForeignKeys_pvars2.COLUMN_NAME AS COLUMN_NAME2,
    (ForeignKeys_pvars1.index_number = 0
        AND ForeignKeys_pvars2.index_number = 0) AS main
FROM
    ForeignKeys_pvars AS ForeignKeys_pvars1,
    ForeignKeys_pvars AS ForeignKeys_pvars2,
    RelationTables
WHERE
    ForeignKeys_pvars1.TABLE_NAME = ForeignKeys_pvars2.TABLE_NAME
        AND RelationTables.TABLE_NAME = ForeignKeys_pvars1.TABLE_NAME
        AND ForeignKeys_pvars1.ARGUMENT_POSITION <
        ForeignKeys_pvars2.ARGUMENT_POSITION
        AND RelationTables.SelfRelationship = 0
        AND RelationTables.Many_OneRelationship = 0;

CREATE table Relationship_MM_Self AS
SELECT 
    CONCAT('`',
            ForeignKeys_pvars1.TABLE_NAME,
            '(',
            ForeignKeys_pvars1.Pvid,
            ',',
            ForeignKeys_pvars2.Pvid,
            ')',
            '`') AS orig_RVarID,
    ForeignKeys_pvars1.TABLE_NAME,
    ForeignKeys_pvars1.Pvid AS Pvid1,
    ForeignKeys_pvars2.Pvid AS Pvid2,
    ForeignKeys_pvars1.COLUMN_NAME AS COLUMN_NAME1,
    ForeignKeys_pvars2.COLUMN_NAME AS COLUMN_NAME2,
    (ForeignKeys_pvars1.index_number = 0
        AND ForeignKeys_pvars2.index_number = 1) AS main
FROM
    ForeignKeys_pvars AS ForeignKeys_pvars1,
    ForeignKeys_pvars AS ForeignKeys_pvars2,
    RelationTables
WHERE
    ForeignKeys_pvars1.TABLE_NAME = ForeignKeys_pvars2.TABLE_NAME
        AND RelationTables.TABLE_NAME = ForeignKeys_pvars1.TABLE_NAME
        AND ForeignKeys_pvars1.ARGUMENT_POSITION <
        ForeignKeys_pvars2.ARGUMENT_POSITION
        AND ForeignKeys_pvars1.index_number < ForeignKeys_pvars2.index_number
        AND RelationTables.SelfRelationship = 1
        AND RelationTables.Many_OneRelationship = 0;

CREATE table Relationship_MO_NotSelf AS
SELECT 
    CONCAT('`',
            ForeignKeys_pvars.REFERENCED_TABLE_NAME,
            '(',
            PVariables.Pvid,
            ')=',
            ForeignKeys_pvars.Pvid,
            '`') AS orig_RVarID,
    ForeignKeys_pvars.TABLE_NAME,
    PVariables.Pvid AS Pvid1,
    ForeignKeys_pvars.Pvid AS Pvid2,
    KeyColumns.COLUMN_NAME AS COLUMN_NAME1,
    ForeignKeys_pvars.COLUMN_NAME AS COLUMN_NAME2,
    (PVariables.index_number = 0
        AND ForeignKeys_pvars.index_number = 0) AS main
FROM
    ForeignKeys_pvars,
    RelationTables,
    KeyColumns,
    PVariables
WHERE
    RelationTables.TABLE_NAME = ForeignKeys_pvars.TABLE_NAME
        AND RelationTables.TABLE_NAME = PVariables.TABLE_NAME
        AND RelationTables.TABLE_NAME = KeyColumns.TABLE_NAME
        AND RelationTables.SelfRelationship = 0
        AND RelationTables.Many_OneRelationship = 1;

CREATE table Relationship_MO_Self AS
SELECT 
    CONCAT('`',
            ForeignKeys_pvars.REFERENCED_TABLE_NAME,
            '(',
            PVariables.Pvid,
            ')=',
            ForeignKeys_pvars.Pvid,
            '`') AS orig_RVarID,
    ForeignKeys_pvars.TABLE_NAME,
    PVariables.Pvid AS Pvid1,
    ForeignKeys_pvars.Pvid AS Pvid2,
    KeyColumns.COLUMN_NAME AS COLUMN_NAME1,
    ForeignKeys_pvars.COLUMN_NAME AS COLUMN_NAME2,
    (PVariables.index_number = 0
        AND ForeignKeys_pvars.index_number = 1) AS main
FROM
    ForeignKeys_pvars,
    RelationTables,
    KeyColumns,
    PVariables
WHERE
    RelationTables.TABLE_NAME = ForeignKeys_pvars.TABLE_NAME
        AND RelationTables.TABLE_NAME = PVariables.TABLE_NAME
        AND RelationTables.TABLE_NAME = KeyColumns.TABLE_NAME
        AND PVariables.index_number < ForeignKeys_pvars.index_number
        AND RelationTables.SelfRelationship = 1
        AND RelationTables.Many_OneRelationship = 1;

CREATE TABLE Relationship AS SELECT * FROM  
Relationship_MM_NotSelf    
UNION SELECT            
*                   
FROM
Relationship_MM_Self 
UNION SELECT 
*
FROM
Relationship_MO_NotSelf 
UNION SELECT 
*
FROM
Relationship_MO_Self;

ALTER TABLE Relationship ADD PRIMARY KEY (orig_RVarID);
ALTER TABLE `Relationship` ADD COLUMN `RVarID` VARCHAR(10) NULL , 
ADD UNIQUE INDEX `RVarID_UNIQUE` (`RVarID` ASC) ; 


CREATE TABLE 2Variables AS SELECT CONCAT('`',
        COLUMN_NAME,
        '(',
        Pvid1,
        ',',
        Pvid2,
        ')',
        '`') AS 2VarID,
COLUMN_NAME,
Pvid1,
Pvid2,
TABLE_NAME,
main FROM
Relationship
    NATURAL JOIN
AttributeColumns;

ALTER TABLE 2Variables ADD PRIMARY KEY (2VarID);</p>\end{alltt}

%\section{Final Thoughts on Good Layout}
%Please use readable font sizes in the figures and graphs. Avoid tempering with the correct border values, and the spacing (and format) of both text and captions of the PVLDB format (e.g. captions are bold).
%
%At the end, please check for an overall pleasant layout, e.g. by ensuring a readable and logical positioning of any floating figures and tables. Please also check for any line overflows, which are only allowed in extraordinary circumstances (such as wide formulas or URLs where a line wrap would be counterintuitive).
%
%Use the \texttt{balance} package together with a \texttt{\char'134 balance} command at the end of your document to ensure that the last page has balanced (i.e. same length) columns.
\end{scriptsize}

\end{appendix}

\end{document}



%The databases are fairly complex, so the experiments are computationally demanding, especially the Alchemy inference component, which needs to be applied to all groundings of all descriptive attributes to compute average predictive performance. The databases and their main characteristics are as follows. 
% and on-line sources such as \cite{bib:jbnsite}.
%In this paper we report the average result over all subdatabases in this paper and leave the evaluation of how models should evolve based on the size of data to an extension of the work in a journal paper. 
%
%\textbf{to do}
%Also, we may emphasize the difference of such datasets in terms of self-relationships and same type relationships.
%
%\noindent\textbf{University Database.} We manually created a small dataset, based on the schema given in Figure~\ref{fig:university-schema}.% omitting the $\it{Teaches}$ relationship for simplicity. 
%The dataset is small and is used as a testbed for the correctness of our algorithms.
%
%\noindent\textbf{MovieLens Database } 
%%A dataset from the UC Irvine machine learning repository. The data are organized in 3 tables (2 entity tables, 1 relationship table, and 7 descriptive attributes). 
%
%This is a standard dataset from the UC Irvine machine learning repository.  \textbf{really? I can NOT find it online.}
%%user:6039; Movie: 3883; rating: 1000129
%%age, 7 bins based on the original MovieLens design
%
%% \cite{Schulte2012}.
%%The schema for the dataset is shown in Table \ref{}.
%It contains two tables representing entity sets: User with 6,039 tuples and Item (Movies) with 3,883 tuples.
%The User table has 2 descriptive attributes, $\age$ and $\it{gender}$. We discretized the attribute $\age$ into three equal-frequency bins. 
%The table Item represents information about the movies. 
%%It has 17 Boolean attributes that indicate the genres of a given movie. 
%There is one relationship table Rated corresponding to a Boolean predicate. 
%The Rated contains Rating as descriptive attribute; 1,000,129 ratings are recorded.  
%We performed a preliminary data analysis following \cite{Schulte2012}.
%%and omitted genres that have only weak correlations with the rating or user attributes, leaving a total of three genres (Drama, Horror, Action).
%
%%
%%The full dataset contains 170,143 ground atoms and is too big for Alchemy to perform learning. We made small subsamples to make the experiments feasible. Subsampling 100 Users and 100 Items transforms to an Alchemy input file with 3,485 ground atoms. Structure learning with Alchemy takes around 30 min.
%%Subsampling 300 Users and 300 Items transforms to an Alchemy input file with 27,134 ground atoms. Structure learning with Alchemy takes about 2 days to run.
%%The full table with 100,000 ratings exceeded the memory limits of Tetrad, so we randomly picked 40\% of the ratings of the relationship table as input data.
%
%\noindent\textbf{Mutagenesis Database.}  This dataset is widely used in Inductive Logic Programming research \cite{Srinivasan1996}. %It contains 4 tables total to 15218 tuples. 
%We used a previous discretization \cite{Schulte2012}.
%Mutagenesis has two entity tables, Atom with 3 descriptive attributes, and Mole (decribing molecules), with 5 descriptive attributes. 
%%including two attributes that are discretized into ten values each (logp and lumo).
%There are two relationship tables, MoleAtom, indicating which atoms are parts of which molecules, and Bond, which relates two atoms and has 1 descriptive attribute. 
%%The full dataset, with 35,973 ground atoms, crashed Alchemy with both structure  and parameter learning. A subsample with 5,017 ground atoms did not terminate for structure learning, but weight learning was feasible. The computational difficulties of Alchemy compared to the MovieLens dataset are  due to the high number of descriptive attributes.
%%%another subsample with
%%Representing a relationship between entities from the same table in a parametrized Bayes net requires using two or more variables associated with the same population (e.g., $\it{Bond}(\A_{1},\A_{2}))$.
%%(Techreport 2009) describes a straightforward extension of Algorithm~\ref{alg:structure} for this case, which we applied to the Mutagenesis dataset.\footnote{Reference omitted for blind review.}
%%We also tested our method on the Financial dataset with similar results, but omit a discussion due to space constraints.
%
%\noindent\textbf{Financial} 
%This dataset stores the loan information about the clients at the associated banks.
%And it's a modified version of the financial dataset from the discovery challenge that was organized at PKDD'99. We adapted the database design to fit the ER model by following the
%modification from CrossMine~\cite{Yin2004} and Graph-NB~\cite{han2009}.
%The data are organized in 7 tables (4 entity tables, 3 relationship tables with 15 descriptive attributes in total).
%
%
%\noindent\textbf{Hepatitis Database.} This data is a modified version of the PKDD02 Discovery Challenge database \cite{Frank2007}. %, which includes removing tests with null values. 
%The database contains information on laboratory examinations of 771 hepatitis B- and C-infected patients, taken between 1982 and 2001. The data are organized in 7 tables (4 entity tables,  3 relationship tables) with 16 descriptive attributes. They contain basic information about the patients, results of biopsy, information on interferon therapy, results of out-hospital examinations, and results of in-hospital examinations. 
%
%\noindent\textbf{IMDB Database.} %, combination?.}
%This is the most challenge dataset in terms of nubmer of total tuples and ER-Digram complexity. %attributes.
%Basically we make a combination of the MovieLens 1M dataset which is a commonly-used rating dataset\footnote{www.grouplens.org}  and the Internet Movie Database (IMDB)\footnote{www.imdb.com, July 2013}.
%We performed a very {\em similar preliminary data processing} with MovieLens dataset. \cite{Peralta2007}
%Two more entity tables representing actors and directors were introduced, and we added more related information as descriptive attributes about the movies from IMDB. 
%In spite of the rating table, we added another two relationship tables. One is to store who directed the movie with genre information, and the other to store which actors participated in which movies.
%So in total, there are four entity tables, three relationship tables.% with more than 1.3M tuples.
%
%%And also includes 3 relationship tables to represent the 5 star rating score given per movie, which actors are involved in given movie and who directed that movie. 
%%4 entity tables : actors (gender 2, quality 6: 98690 ),directors(quality 6,avg\_revenue 5:2201),movies(year 4,country 4,runningtime 4:3832),users(age 7,gender 2,occuption 5 :6039)
%%and 3 relation tables: u2base(rating 5: 996159), movies2actors(cast\_num 4:138349), movies2directos(genre 9:4141)
%
%
%
%\noindent\textbf{Mondial Database.} A geography database, featuring
%one self-relationship, $\it{Borders}$, that indicates which countries border each other. 
%This dataset contains data from multiple geographical web data sources. 
%We follow the modification of She\cite{wangMondial}, and use a subset of the tables and disretized features: 2 entity tables, $\it{Country},\it{Economy}$. 
%The descriptive attributes of Country are continent, government, percentage, majority religion, population size. The descriptive attributes of Economy are inflation, gdp, service, agriculture, industry. 
%Another relationship table is  Economy\_Country specifying which country has what type of economy. %A self-relationship table Borders relates two countries.
%  %$\it{Country},\it{Continent},\it{Economy},\it{Government}$, where the latter three are related to Country by many-one relationships, and one relationship table $\it{Borders}$ that relates two countries. Our dataset includes a self-relationship table Borders that relates two countries.
%
%
%\noindent\textbf{UW-CSE Database.}
%This dataset lists facts about the Department of Computer Science and Engineering at the University of Washington. There're two entity  tables (i.e., $Person$, $Course$) and two relationship tables (i.e. $AdvisedBy$, $TaughtBy$).
%And these two tables both involve a self-relationship relating to two persons \cite{Domingos2007}. 
%%The total number of ground atoms is 4,106,841. The database contained a total of 3380 ground atoms. 
%
%
%\begin{table}[btp] \centering
%%\scalebox{0.7in}{
%\resizebox{0.5\textwidth}{!}{
%\begin{tabular}[c]
%{|l|c|c|c|r|r|}\hline
% \textbf{Dataset} & \textbf{\#Relations} & \textbf{Self} &
% \textbf{SameType}& \textbf{\#Tuples} & \textbf{\#Attribute}  \\\hline
%% \textbf{Dataset} & \textbf{Relationships} & \textbf{\begin{tabular}[l] {ll} Self \\Relationships \end {tabular}} &
%% \textbf{\begin{tabular}[l] {ll} Same Type\\ Relationships \end {tabular}}& \textbf{\#Tuples} & \textbf{\begin{tabular}[l] {ll} \#Attribute  \\Columns \end{tabular}}  \\\hline
%    University&2 & N & N & 171 & 12\\\hline
%    Movielens &1 & N & N & 1,010,051 & 7\\\hline
%%    Movielens(0.1M) &1 & N & N &  83,402 & 7\\\hline
%    Mutagenesis &2 & N & \textbf{Y} & 14,540 & 11\\\hline
%    Financial &3 & N & N &  225,932& 15\\\hline
%   Hepatitis &3 & N & N &12,927  & 19\\\hline
%   IMDB &3 & N &N &1,354,134  & 17\\\hline
%    Mondial &2 & \textbf{Y} & N &  870& 18\\\hline
%    UW-CSE &2 & \textbf{Y} & N & 712 & 14\\\hline
%   
%\end{tabular}
%}
% % end scalebox
%\caption{Real datasets characteristics, including size of datasets in total number of table tuples. 
% \# Attribute should not count the $Rnodes$ ? Here Self means self relationship, and type means same type relationship or not.
% \label{table:datasetsize}}
%\end{table}
%
%
%\subsection{Methods Compared}
%%\paragraph{Methods Compared}
%
%We compared the following methods.
%
%\begin{description}
%\item[Bayes Base Hierarchical Search (B.B.H.) ] The new method that has the potential to find link correlations (Algorithm~\ref{alg:fmt} with the $\ct$-tables instead of natural join tables).
%\item[Flat Search]  Applies the single-table Bayes net learner to the biggest $\ct$-table.
%\item[Complete Graph] fully connected DAG.
%\item[Disconnected Graph] all nodes are disconnected.
%
%\end{description}
%
%To implement Flat Search and the LAJ+ algorithm efficiently, we apply the Fast M\"obius Transform to compute tables of sufficient statistics that involve negated relationships. We discuss the details further in Section~\ref{sec:mobius}.
%
%\subsection{Learning Times: SQL Join Time?}
%
% Table~\ref{table:cttimes} shows the data preprocessing time that the different methods require for table joins. This is the same for all methods, namely the cost of computing the full join table using the fast M\"obius transform described in Section~\ref{sec:mobius}. 
%\begin{table} \centering
%%\scalebox{0.7in}{
%\resizebox{0.5\textwidth}{!}{
%\begin{tabular}
%{|l|r|r|r|r|}\hline 
% \textbf{Dataset} & \textbf{Building Time} & \textbf{Sum(counts)}  & \textbf{\#Tuples} & \textbf{Compress Ratio} \\\hline
%University&1.68/0.05 & 2,280 & 351 &6.5 \\\hline
%Movielens &2.70/703.99 &23,449,437 &252 &93,053.32\\\hline
%%Movielens(0.1M) &0.62 & 1,582,762 &239 &6,622.44 \\\hline
%Mutagenesis &1.67/1096.00 & 905,205 &1,631 &555.00  \\\hline
%Financial &  1421.87/12592.41(N.T.) &149,046,585,349,303 &3,013,011 &49,467,653.90   \\\hline
%Hepatitis &3536.76/6075.39 &17,846,976,000 & 12,374,892 &1,442.19 \\\hline
%IMDB &7467.85/17526.86(N.T.) &5,030,412,758,502,710&15,538,430 & 323,740,092.05 \\\hline
%Mondial &1112.84/132.13&4,655,957&1,746,870&2.67  \\\hline
%UW-CSE &3.84/350.30& 10,201,488&2,828 & 3,607.32\\\hline
%
%\end{tabular}
%}
% % end scalebox
%\caption{Contengency Table (C.T.) Description : Building Time  in seconds. vs Full Relation Table SQL join time? %The Learn-and-Join methods do the same 
% \label{table:cttimes}}
%\end{table}
%
%
%[fill in discussion] 
%
%
%Table~\ref{table:runtimes}
% provides the model search time for each of the link analysis methods. 
%%This does not include the time for computing table joins since this is essentially the same for all methods (the cost of the full join table). 
%On the smaller and simpler datasets, all search strategies are fast, 
%but on the medium-size and more complex datasets (Hepatitis, MovieLens), hierarchical search is much faster due to its use of constraints.
%%Adding prior knowledge as constraints could speed the structure learning substantially.
%%The reason for w LAJ+ starts with the previous LAJ method as the first phase. The edges among attributes that are discovered in the first phase are treated as fixed background knowledge in the second phase. 
%
%
%\begin{table} \centering
%%\scalebox{0.7in}{
%\resizebox{0.5\textwidth}{!}{
%\begin{tabular}[c]
%{|l|r|r|r|r|r|}\hline
% \textbf{Dataset}  & \textbf{BBH} & \textbf{Flat} & \textbf{Compl.} & \textbf{Discon.}\\\hline
%University&1.523&1.486& 0.186 &0.135 \\\hline
%Movielens & 1.528&0.968& 0.101&0.066 \\\hline
%%Movielens(0.1M) &1.178& 0.986& 0.083&0.065 \\\hline
%Mutagenesis & 1.780&1.869& 0.115&0.096 \\\hline
%Financial  &96.308& 1,241.077& 0.203&0.080 \\\hline
%Hepatitis   & 416.704& N.T.& 0.272&0.134 \\\hline
%IMDB   & 551.643 & N.T.&0.286 &0.220 \\\hline
%Mondial & 190.162&1,289.534&0.280 &0.093 \\\hline
%UW-CSE & 2.896&2.367&0.181 &0.100 \\\hline
%\end{tabular}
%}
% % end scalebox
%\caption{Model Structure Learning Time  in seconds.
%% \textbf{Zhensong: may show some realy SQL queries?}
% \label{table:runtimes}}
%\end{table}
%
%
%
%%\paragraph{Performance Metrics }
%\subsection{Performance Metrics }
%
%\textbf{Pseudo Relation Model Selection Scores  \cite{Schulte2011} ?}, advantages \cite{Schulte2013} ?
%
% We report learning time, log-likelihood, Bayes Information Criterion (BIC), and the Akaike Information Criterion (AIC). BIC and AIC are standard scores for Bayes nets \cite{Chickering2003}, defined as follows. We write 
%$$L(\hat{G},\d)$$ for the log-likelihood score,
%where $\hat{G}$ is the BN $\G$ with its parameters instantiated to be the maximum likelihood estimates given the dataset $\d$, and the quantity $L(\hat{G},\d)$ is the log-likelihood of $\hat{G}$ on $\d$. 
%
%The BIC score is defined as follows \cite{Chickering2003,Schulte2011}
%
%$$\mathit{BIC}(\G,\d) = L(\hat{G},\d) - \mathit{par}(\G)/2 \times ln(m)$$
%
%where the data table size is denoted by $m$, and $\mathit{par}(\G)$ is the number of free parameters in the structure $\G$. The AIC score is given by 
%
%$$\mathit{AIC}(\G,\d) = L(\hat{G},\d) - \mathit{par}(\G). $$
%
% AIC is asympotically equivalent to selection by cross-validation, so we may view it as a closed-form approximation to cross-validation,  which is computationally demanding for relational datasets. 
%
%
%
%
%
%%\section{Empirical Evaluation: Statistical Scores}
%
%
%
%
%\begin{table}[hbtp] 
%\begin{center}
%\resizebox{0.5 \textwidth}{!}{
%\begin{tabular}{|l|r|r|r|r| }
%		\hline \textbf{University} &{Pseudo\_BIC}& {Pseudo\_AIC} &{Norm\_log-likelihood} &{\# Para.}\\
%			\hline BBH &-554.01 & -99.50 & -5.50& 94\\
%			\hline Flat  & -3731.88 & -668.20 & -5.20& 663\\
%			 \hline Complete &-386566.28  & -50000.10& -5.10& 49995 \\
%		        \hline Disconnected &-121.44 &-31.89 &-8.89 &23 \\
%			\hline
%		\end{tabular}
%}
%\end{center}
%
%\begin{center}
%\resizebox{0.5 \textwidth}{!}{
%\begin{tabular}{|l|r|r|r|r| }
%\hline \textbf{Movielens(1M) } &{Pseudo\_BIC}& {Pseudo\_AIC} &{Norm\_log-likelihood} &{\# Para.}\\
%
%			\hline BBH &-4927.84& -295.44& -3.44&292\\
%			\hline Flat &-5168.08&-315.44 &-3.44& 312\\
%		      \hline Complete &-11461.86  &-678.44 &-3.44 &675 \\
%		        \hline Disconnected &-179.20 &-19.31 &-4.31 &15 \\
%			
%			\hline
%		\end{tabular}
%}
%\end{center}
%
%%
%%\begin{center}
%%\resizebox{0.5 \textwidth}{!}{
%%\begin{tabular}{|l|r|r|r|r| }
%%		\hline \textbf{Movielens(0.1M) } &{Pseudo\_BIC}& {Pseudo\_AIC} &{Norm\_log-likelihood} &{\# Para.}\\
%%			\hline BBH &-1198.42&-87.10& -3.10&84\\
%%			\hline Flat &-1691.20&-123.10 &-3.10& 120\\
%%			 \hline Complete &-4160.11  &-294.09 &-3.09 &291 \\
%%		        \hline Disconnected &-120.88 &-14.34 & -3.34&11 \\
%%			
%%			\hline
%%		\end{tabular}
%%}
%%\end{center}
%
%\begin{center}
%\resizebox{0.5 \textwidth}{!}{
%\begin{tabular}{|l|r|r|r|r| }
%		\hline \textbf{Mutagenesis  } &{Pseudo\_BIC}& {Pseudo\_AIC} &{Norm\_log-likelihood} &{\# Para.}\\
%			\hline BBH &-9083.88 &-726.96 &-5.96& 721\\
%			\hline Flat & -9224.25 & -1120.59& -5.59& 1115\\
%			 \hline Complete &-5806905.08  &-423374.59 &-5.59 &423369 \\
%		        \hline Disconnected &-292.7 &-43.91 &-7.91 &36 \\
%			\hline
%		\end{tabular}
%}
%\end{center}
%
%\begin{center}
%\resizebox{0.5 \textwidth}{!}{
%\begin{tabular}{|l|r|r|r|r| }
%		\hline \textbf{Financial  } &{Pseudo\_BIC}& {Pseudo\_AIC}&{Norm\_log-likelihood} &{\# Para.}\\
%			\hline BBH &-68562.16 &-2443.74 &-10.74& 2433\\
%			\hline Flat &-188908818.94 &-7206670.64 & -10.66& 7206660\\
%			 \hline Complete &-12218648361.31  &-374400009.12 & -10.67&374399999 \\
%		        \hline Disconnected &-469.21 &-61.79 &-12.79 & 49\\
%			\hline
%		\end{tabular}
%}
%\end{center}
%
%\begin{center}
%\resizebox{0.5 \textwidth}{!}{
%\begin{tabular}{|l|r|r|r|r| }
%		\hline \textbf{Hepatitis  } &{Pseudo\_BIC}& {Pseudo\_AIC} &{Norm\_log-likelihood} &{\# Para.}\\
%			\hline BBH &-10364.93 & -585.58& -16.58& 569\\
%			\hline Flat &N.T. & &  & \\
%			 \hline Complete & N.T.  & & & \\
%		        \hline Disconnected &-473.81 &-76.30 &-18.30 &58 \\
%			\hline
%		\end{tabular}
%}
%\end{center}
%
%
%\begin{center}
%\resizebox{0.5 \textwidth}{!}{
%\begin{tabular}{|l|r|r|r|r| }
%		\hline \textbf{IMDB  } &{Pseudo\_BIC}& {Pseudo\_AIC}&{Norm\_log-likelihood} &{\# Para.}\\
%			\hline BBH &-2072673.77 & -60070.39&-11.39&60059\\
%			\hline Flat & N.T.&  &  &  \\
%			 \hline Complete &N.T.  & & & \\
%		        \hline Disconnected &-647.90 &-66.01 &-12.01 &54 \\
%			\hline
%		\end{tabular}
%}
%\end{center}
%
%\begin{center}
%\resizebox{0.5 \textwidth}{!}{
%\begin{tabular}{|l|r|r|r|r| }
%		\hline \textbf{Mondial  } &{Pseudo\_BIC}& {Pseudo\_AIC} &{Norm\_log-likelihood} &{\# Para.}\\
%			\hline BBH &-3980.41&-357.20&-18.20& 339\\
%			\hline Flat &-8886625.98 &-865255.07 &-14.07  &865241 \\
%			 \hline Complete &N.T.  & & & \\
%		        \hline Disconnected &-336.50 &-74.98 &-19.98 &55 \\
%			\hline
%		\end{tabular}
%}
%\end{center}
%
%\begin{center}
%\resizebox{0.5 \textwidth}{!}{
%\begin{tabular}{|l|r|r|r|r| }
%		\hline \textbf{UW-CSE  } &{Pseudo\_BIC}& {Pseudo\_AIC} &{Norm\_log-likelihood} &{\# Para.}\\
%			\hline BBH & -3187.86& -248.1 & -8.10 & 240\\
%			\hline Flat &-42038.65& -3939.01 & -6.01& 3933 \\
%			 \hline Complete &-356947710.61  &-22118404.95 &-6.02 &22118399 \\
%		        \hline Disconnected &-287.63 &-55.24 &-10.24 &45 \\
%			\hline
%		\end{tabular}
%}
%\end{center}
%\caption{Statistical Performance of different Searching Algorithms by dataset.}
%\textbf{Zhensong: paramete learning  not terminated, mysql, may show some real SQL queries in the appendix?}
%\label{table:result_scores}
%\end{table}
%
%As expected, adding edges between link nodes improves the statistical data fit: 
%the link analysis methods LAJ+ and Flat perform better than the learn-and-join baseline in terms of log-likelihood on all datasets shown in table~\ref{table:result_scores}, except for MovieLens where the Flat search has a lower likelihood. On the small synthetic dataset University, flat search appears to overfit whereas the hierarchical search methods are very close. On the medium-sized dataset MovieLens, which has a simple structure, all three methods score similarly. Hierarchical search finds no new edges involving the single link indicator node (i.e., LAJ and LAJ+ return the same model). 
%
%The most complex dataset, Hepatitis, is a challenge for flat search, which seems to overfit severely with a huge number of parameters that result in a model selection score that is an order of magnitude worse than for hierarchical search. Because of the complex structure of the Hepatitis schema, the hierarchy constraints appear to be effective in combating overfitting.
%
%%
%%\begin{figure*}[htbp] %  figure placement: here, top, bottom, or page
%%   \centering
%%
%%\includegraphics[width=7in,height=0.3\textheight]{unielwin.png}
%%\includegraphics[width=7in,height=0.3\textheight]{movielens.png}
%%\includegraphics[width=7in,height=0.3\textheight]{muta.png}
%%
%%  \caption{Real Bayes Net Examples:.}
%%   \label{fig:BN_Examplse}
%%\end{figure*}
%
%
% The situation is reversed on the Mutagenesis dataset where flat search does well: compared to previous LAJ algorithm, %attribute-only search,
%it manages to fit the data better with a less complex model. 
%Hiearchical search performs very poorly compared to flat search (lower likelihood yet many more parameters in the model). Investigation of the models shows that the reason for this phenomenon is a special property of the Mutagenesis dataset: The two relationships in the dataset, Bond and MoleAtm, involve the same descriptive attributes. The hierarchical search learns two separate Bayes nets for each relationship, then propagates both sets to the final result. However, the union of the two graphs may not be a compact model of the associations that hold in the entire database. A solution to this problem would be to add a final pruning phase where redundant edges can be eliminated. We expect that with this change, hierarchical search would be competitive with flat search on the Mutagenesis dataset as well.
%
%
%


%%\section{Computing Relational Contingency Tables} \label{sec:cta}
%\section{Computing Contingency Tables For Positive Relationships} \label{sec:cta}
%
%
%%The learning algorithms described in this paper rely on the  availability of the $\ct$-tables (see Figure~\ref{fig:university-tables}). 
%%Computing the contingence tables raises two key challenges that we address in this paper. 
%If a conjunctive query involves only positive relationships, then it can be computed using SQL's sum aggregate function applied to a table join. The SQL sum query represents the positive part of the $\ct$-table much like a view definition represents a relation. 
%The challenge is generality: If the machine learning algorithm is developed without advance knowledge of the database schema, it needs to compute the SQL sum query from metainformation. 
%We propose accomplishing this via a \textbf{metaquery}.
%%: Using the meta-information in the random variable database, we construct an SQL query for each relationship chain. 
%%The SQL query represents a table join followed by a sum aggregation. Executing this query constructs the positive relationship part of the contingency table. 
%
%%In the next section we proposed a new approach for the second challenge.
%%
%%The second challenge is computing event counts that involve negated relationships. This is infeasible using standard table joins [explain: firends vs un-firends]. 
%%We approach this problem in two steps. 
%%First, we show that event counts using $m+1$ negated relationships can be computed from two event counts that each involve at most $m$ relationships. 
%%We state this result as a relational algebra identity in a new extension of relational algebra that we term \textbf{contingency table algebra}. A dynamic programming algorithm applies the algebraic identify repeatedly to efficiently build up a complete contingency table from partial tables that involve fewer negated relationships. 
%
%\subsection{Metaqueries for Utilizing Schema Information} 
%%As the top level of  Figure~\ref{fig:flow} illustrates, 
%The required counts involving only true relationships can be computed using the standard SQL constructs COUNT(*) and GROUP BY. 
%The general form of these operations is the same for every input database. 
%The varying part is the list of columns to be included, which depends on the input database. 
%For a fixed database, the column lists can be hard-coded. 
%To achieve a general solution when the column list is not known in advance, we introduce a new approach that we refer to as an SQL \textbf{meta query}. 
%
%An SQL meta query is an SQL query that takes as input schema information as recorded in the random variable database,
%and produces four kinds of tables: the Select, From, Where and Group By tables. The Select table lists the entries in the Select clause of the target query, the From table lists the entries in the From clause, and similar for Where and GROUP BY tables. %\footnote{basically the Group BY table is same as Select table without $\qcount$ column}. 
%%Thus an SQL meta query maps schema information to the components of another SQL query. 
%Given the four query tables, the corresponding SQL query can be easily executed in an application or stored procedure to produce the $\ct$-table entries.
%Figure~\ref{fig:meta-query} shows an example of meta queries for the university database.
%
%\begin{figure}[htb]
%\begin{center}
%\resizebox{0.5\textwidth}{!}{
%\includegraphics[width=0.5\textwidth]{figures/meta-query.png}
%}
%\caption{Example of Metaqueries and Entries for relationship table $\ra$ (i.e. rnid=$\ra$) based on university database.
%The entries of the Group By table (not shown) are the same as Select table but without $\qcount$ column.
%Here the $\it{@database@}$ is a variable denoting the given database schema.
%~\label{fig:meta-query} }
%\end{center}
%\end{figure}

%
%\section{Computing Contingency Tables For Negated Relationships} \label{sec:cta}
%
%Computing sufficient statistics that involve negated relationships is infeasible using standard table joins. 
%%and can be carried out by regular table joins or optimized virtual joins~\cite{Yin2004}. 
%%
%%Computing joint probabilities for a family containing one or more negative relationships is harder. 
%Standard join tables materialize new data tables that enumerate tuples of objects that are {\em not} related. %(see Figure~\ref{fig:university-tables}).
%%and then apply existing counting methods to the new tables. 
%However, the number of unrelated tuples is too large to make this scalable (think about the number of user pairs who are {\em not} friends on Facebook). 
%%A numerical example will illustrate why this is not feasible. Consider a university database with 20,000 Students, 1,000 Courses and 2,000 TAs. If each student is registered in 10 courses, the size of a $\it{Registered}$ table is 200,000. So the number of complementary student-course pairs is $2 \times 10^{7}-2 \times 10^{5}$, which is too big for most database systems. 
%%If we consider joins, complemented tables are even more difficult to deal with: suppose that each course has at most 3 TAs. Then  the number of satisfying instantiations of a positive relationship only formula such as $\it{Registered}(\S,\C) = \true,\it{TA}(\T,\C) = \true)$ is less than $6 \times 10^{5}$, whereas with negations the number of instantiations of the expression $\it{Registered}(\S,\it{course}) = \false, \it{TA}(\T,\it{course}) = \false)$ is on the order of $4 \times 10^{10}$. 
%%\subsection{The M\"obius parametrization.} 
%%To compute frequencies involving negated relationships, we would like to use the optimized algorithms for table join frequencies as an oracle/black box. 
%%Can we instead reduce the computation of sufficient statistics that involve negated relationships to the computation of sufficient statistics that involve existing (positive) relationships only? 
%%In their work on learning Probabilistic Relational Models with existence  uncertainty, Getoor et al. 
%%provided a subtraction method for the special case of estimating counts with only a single negated relationship \cite[Sec.5.8.4.2]{Getoor2007c}. 
%%They did not treat contingency tables with multiple negated relationships, which we consider next.
%
%We describe a ``virtual join'' algorithm that computes the required data tables without the quadratic cost of materializing a cross-product. Our experiments below compare the virtual joins with materializing table joins. 
%%We approach this problem in two steps. 
%First, we show that query counts using $m+1$ negated relationships can be computed from two query counts that each involve at most $m$ relationships. 
%We state this result as a relational algebra identity in a new extension of relational algebra that we term \textbf{contingency table algebra}. Second, a dynamic programming algorithm applies the algebraic identify repeatedly to efficiently build up a complete contingency table from partial tables.
%% that involve fewer negated relationships. 
%
%%Our first naive implementation constructs this tables using standard joins. 
%%While this was sufficient for our experiments, the cross-products carry a \textbf{quadratic} costs for binary relations, and therefore do not scale to large datasets. 
%%Moreover, the hierarchical search requires joins of the extended tables. 
%
%
%\textbf{to do: check page 19 of plan.pptx}
%%
%%Our starting point is the observation that a statistical learning algorithm like a Bayes net learner does not require an enumeration of individuals tuples, but only {\em sufficient statistics} \cite{Heckerman1995,Schulte2011}. 
%%These can be represented in {\em contingency tables} as follows \cite{Moore1998}. 
%%Consider a fixed list of relationship nodes $\R_{1}, R_{2},\ldots,R_{m}$, and attribute nodes $\functor_{1},\ldots,\functor_{j}$. 
%%A \textbf{query} is a set of $(node = value)$ pairs where each value is of a valid type for the node. 
%%The \textbf{result set} of a query in a database $\D$ is the set of instantiations of the population variables such that the query evaluates as true in $\D$.
%%For example, in the database of Figure~\ref{fig:university-tables} the result set for the query 
%%$(\it{intelligence}(\S) = 2$, $\it{rank}(\S) = 1$, $\it{rating}(\C) = 3$, $\it{Diff}(C) = 1$, $\reg(\S,\C) = F)$ 
%%is the singleton $\{\langle \it{kim}, \it{101}\rangle\}$. 
%%The \textbf{count} of a query is the cardinality of its result set. 
%%Each subset of nodes $\set{V} = v_{1},\ldots,v_{n}$ has an associated \textbf{contingency table} denotes by $\cttable(\set{V})$. %$CT(\set{V})$. 
%%This is a table with a row for each of the possible assignments of values to the nodes in $\set{V}$, and a special integer column called $\qcount$. 
%%The value of the $\qcount$ column in a row corresponding to $V_{1} = v_{1},\ldots,V_{n} = v_{n}$ records the count of the corresponding query. 
%%Figure~\ref{fig:ct} shows a partial contingency table from University database.
%
%
%
%
%
%\subsection{Contingency Table Algebra} 
%%\subsection{Definition} 
%First we introduce some notation.
%If $\dtable$ denotes a generic database table, then $\ColumnList(\dtable)$ is a list of columns in table $\dtable$; the list order is irrelevant. For a CT-table $\ct$, the expression $\ColumnList(\ct)$ denotes a list of columns other than the count column.
%
%none $\qcount$ columns in table $\dtable$, 
%and column sets $\set{V}$, $\set{U}$ are the union of $ \qcount $ column with $\ColumnList({\dtable})$ for different given $\dtable$. 
%Suppose ${\dtable}_{1}$, ${\dtable}_{2}$ be two union-compatible %contingency 
%tables with the same column headers, and ${\dtable}_{3}$ is another table,
%then $\ColumnList({\dtable}_{1}) =V_{1}, \ldots,\ V_{k}$, $\ColumnList({\dtable}_{2}) =V_{1}, \ldots,\ V_{k}$
%and $\ColumnList({\dtable}_{3}) = U_{1}, \ldots,\ U_{m}$.
%We define $\ColumnList({\dtable}_{1}) = \ColumnList({\dtable}_{2})$ by 
%${\dtable}_{1}.V_{1} ={\dtable}_{2}.V_{1}, \ldots, {\dtable}_{1}.V_{k} = {\dtable}_{2}.V_{k}$.
%
%
%\textbf{to do: %update the $CL$, $\cup$,
% check the consistency with algorithm part?}
%
%\subsubsection{Unary Operators} \label{sec:unary}
%\begin{description}
%\item[Selection] $\sigma_{\selectcond}  \ct$ selects a subset of the rows in the  $\ct$-table  that satisfy condition $\selectcond$. This is the standard relational algebra operation except that the selection condition $\selectcond$ may not involve the $\qcount$ column.
%\item[Projection]  %$\project_{\set{V}}  
%$\project_{\V_{1},\ldots,\V_{k}} \ct$ selects a subset of the  columns in the  $\ct$-table, excluding the count column. 
%The counts in the projected subtable are the sum of counts of rows that satisfy the query in the subtable. 
%The  $\ct$-table projection  $\project_{\V_{1},\ldots,\V_{k}} \ct$ can be defined by the following pseudo-SQL code:
%
%\begin{quote}
%SELECT SUM(COUNT) AS COUNT, $V_{1}, \ldots,\ V_{k}$ \\
%FROM $\cttable$ \\
%GROUP BY $V_{1}, \ldots,\ V_{k}$
%\end{quote}
%%\begin{quote}
%%SELECT SUM($\qcount$) AS $\qcount$, $\ColumnList({\ct})$ \\
%%FROM $\ct$ \\
%%GROUP BY $\ColumnList({\ct})$
%%\end{quote}
%
%
%\item[Conditioning]  $\condition_{\selectcond}  \ct$ returns a conditional contingency table. Ordering the columns as $(V_{1},\ldots,V_{i}, \ldots,\V_{i+j}$),  suppose that the selection condition is a conjunction of values of the form $C = (V_{i+1} = v_{i+1},\ldots, V_{i+j} = v_{i+j})$.  Conditioning can be defined in terms of selection and projection by the equation:
%\begin{equation}
%\condition_{\selectcond}  \ct = \project_{\V_{1},\ldots,\V_{i}} (\select_{\selectcond}  \ct) \nonumber
%\end{equation}
%\end{description}
%
%\subsubsection{Binary Operators} \label{sec:bin}
%
%\begin{description}
%\item[Cross Product]  %Let %$\ct_{1}(\V_{1}, \ldots,\ V_{m}),\ct_{3}(U_{1}, \ldots,\ U_{k})$
%%$\ct_{1}(\set{V}),\ct_{3}(\set{U})$ 
%% be two contingency tables %that do not share any papulation variable. 
%The \textbf{cross-product} of $\ct_{1}(\set{U}),\ct_{2}(\set{V})$ is the Cartesian product of the rows, where the product counts are the products of count. The cross-product can be defined by the following pseudo-SQL code:
%% \begin{quote}
%%SELECT \\($\ct_{1}.COUNT*\ct_{2}.COUNT$) AS COUNT,  $U_{1}, \ldots,\ U_{k}, \V_{1}, \ldots,\ V_{m}$ \\
%%FROM  $\ct_{1},\ct_{2}$
%%\end{quote}
%\begin{quote}
%SELECT \\($\ct_{1}.\qcount *\ct_{2}.\qcount$) AS $\qcount$,  $\ColumnList({\ct_{1}}), \ColumnList({\ct_{2}}) $\\
%FROM  $\ct_{1},\ct_{2}$
%\end{quote}
%
%
%\item[Addition] 
% The \textbf{count addition} $\ct_{1}(\set{V}) + \ct_{2}(\set{V})$ adds the counts of matching rows, as in the following pseudo-SQL query.
%\begin{quote}
%SELECT % $\ct_{1}$.COUNT+$\ct_{2}$.COUNT 
%$\ct_{1}.\qcount$+$\ct_{2}.\qcount$ AS $\qcount$, $\ColumnList({\ct_{1}})$ \\%$\ct_{1}.V_{1} , \ldots, \ct_{1}.V_{k} $ \\
%FROM  $\ct_{1},\ct_{2}$\\
%%WHERE $\ct_{1}.V_{1} = \ct_{2}.V_{1}, \ldots, \ct_{1}.V_{k} = \ct_{2}.V_{k}$
%WHERE $\ColumnList({\ct_{1}}) = \ColumnList({\ct_{2}})$
%\end{quote}
%
%If a row appears in one $\ct$-table but not the other, we include the row with the count of the table that contains the row. Note that  $\ct_{1}(\set{V})$ and $\ct_{2}(\set{V})$ are union-compatible because they have the same  column headers.
%
%\item[Subtraction] %Let $\ct_{1}(\set{V}),\ct_{2}(\set{V})$ be two union-compatible contingency tables with the same column headers. 
%The \textbf{count difference} $\ct_{1}(\set{V}) - \ct_{2}(\set{V})$ equals $\ct_{1}(\set{V}) + (- \ct_{2}(\set{V}))$ where $- \ct_{2}(\set{V})$ is the same as $\ct_{2}(\set{V})$ where the counts are negative. 
%Table subtraction is defined only if (i) without the $\qcount$ column, the rows in $\ct_{1}$ are a superset of those in $\ct_{2}$, and (ii) for each row that appears in both tables, the count in $\ct_{1}$ is at least as great as the count in $\ct_{2}$.
%
%
%\end{description}
%
%
%%\subsection{Implementing the Contingency Table Operators}\label{sec:imp}
%%Everything true: database optimization. General tricks, e.g., omit 0 counts. Specific tricks, e.g. merge-sort.
%\subsubsection{Implementation}\label{sec:imp}
%Our implementation is based on SQL queries, whose execution is well optimized by a RDBMS such as MySQL.   
%A RDBMS offers built-in options for improving query planning, such as using covering index  to speed up the conditioning.
%%[give examples: e.g, indixes]. 
%For some operations where MySQL chooses a suboptimal query plan, we implemented our own method.
%
%%\subsubsection{Unary Operators}
%%Suppose we already know the required columns list $\ColumnList({\dtable})$ for each query clause. 
%The selection operator can be implemented  using SQL as with standard relational algebra. 
%Project with $\ct$-tables requires use of the GROUP BY construct as shown in Section~\ref{sec:unary}. 
%%The list of columns to be projected at a point in the algorithm can be found by a meta query. 
%%\subsubsection{Binary Operators}
% % big * smaller, faster enough.
%%The required column lists can be found by a meta query. 
%
%The most difficult operation to implement efficiently is 
%For addition/subtraction, simply executing the SQL query shown above %shown in Section~\ref{sec:bin} 
%may lead to a \textbf{quadratic} cost if a nested-loops join is used, and therefore does not scale to large datasets.
%The most efficient algorithm is a sort-merge join \cite{Ullman1982}. 
%Given two union-compatible $\ct$-tables, each row in one matches at most one other on the non-count columns. 
%As is well known, with unique matches, the cost of a sort-merge join is $\it{size}(table1) + \it{size}(table2) +$ the cost of sorting both tables. 
%Our experiments below are based on a sort-merge join (our own implementation in Java). The cross product is easily implemented in SQL as shown in Section~\ref{sec:bin}. However, the cross product size is quadratic in the size of the input tables, so a quadratic cost is unavoidable.
%%A general trick to shrink the size $\ct$-table is remove the 0 counts in $\ct$-table.
%%But for a general input database, usually the user can not know the column lists in advance. 
%%So in next section, we propose a new method to compute these.
%
%
%

%
%\subsection{Latttice Computation of Contingency Tables} \label{sec:mobius}
%%\textbf{older material}
%%Our flat search algorithm can be implemented by computing the contingency table for all functor nodes, then presenting it to a Bayes net learner. 
%%The hierarchical search algorithms can be implemented by computing the contingency tables for each relationship chain in the lattice (Figure~\ref{fig:big-lattice}). 
%%For a relationship set, the nodes in the contingency table comprise (1) the descriptive attributes of the entities involved in the relationship set, 
%%(2) the descriptive attributes of the relationships, and (3) a Boolean relationship node for each member of the set. 
%This section describes a method for computing the contingency tables level-wise in the relationship chain lattice. We start with a contingency table algebra equivalence that allows us to compute counts for rows with negative relationships from rows with positive relations.
%
%%
%%\textbf{old material} So long as a database probability involves only positive relationships,
%%%formula only contains positive relationships, 
%%%the computation is straightforward. 
%%%For example, in 
%%%$P_{\D}(\it{gender}(\X) = M, \it{Friend}(\X,\Y) = \true)$, the value  $\grounds_{\D}(\it{gender}(\X) = M, \it{Friend}(\X,\Y) = \true)$, the count of friendship pairs $(x,y)$ where $x$ is male and the
%%%{\em Friend} relationship is true, 
%%and can be carried out by regular table joins or optimized virtual joins~\cite{Yin2004}. 
%%%
%%Computing joint probabilities for a family containing one or more negative relationships is harder. 
%%A naive approach would explicitly construct new data tables that enumerate tuples of objects that are {\em not} related. %(see Figure~\ref{fig:university-tables}).
%%%and then apply existing counting methods to the new tables. 
%%However, the number of unrelated tuples is too large to make this scalable (think about the number of user pairs who are {\em not} friends on Facebook). 
%%A numerical example will illustrate why this is not feasible. Consider a university database with 20,000 Students, 1,000 Courses and 2,000 TAs. If each student is registered in 10 courses, the size of a $\it{Registered}$ table is 200,000. So the number of complementary student-course pairs is $2 \times 10^{7}-2 \times 10^{5}$, which is too big for most database systems. 
%%If we consider joins, complemented tables are even more difficult to deal with: suppose that each course has at most 3 TAs. Then  the number of satisfying instantiations of a positive relationship only formula such as $\it{Registered}(\S,\C) = \true,\it{TA}(\T,\C) = \true)$ is less than $6 \times 10^{5}$, whereas with negations the number of instantiations of the expression $\it{Registered}(\S,\it{course}) = \false, \it{TA}(\T,\it{course}) = \false)$ is on the order of $4 \times 10^{10}$. 
%%\subsection{The M\"obius parametrization.} 
%%To compute frequencies involving negated relationships, we would like to use the optimized algorithms for table join frequencies as an oracle/black box. 
%%Can we instead reduce the computation of sufficient statistics that involve negated relationships to the computation of sufficient statistics that involve existing (positive) relationships only? 
%%In their work on learning Probabilistic Relational Models with existence  uncertainty, Getoor et al. 
%%provided a subtraction method for the special case of estimating counts with only a single negated relationship \cite[Sec.5.8.4.2]{Getoor2007c}. 
%%They did not treat contingency tables with multiple negated relationships, which we consider next.
%
%\textbf{need to define *. Point to example in diagram}
%
%\begin{proposition}%[Pivot_CT]
%\label{PivotCT}
%Let $R$ be a relationship node and let $\set{R}$ be a set of relationship nodes. Let $\Nodes$ be a set of nodes that %(1) must contain all $\eatts$ of $\R$, and may contain any other nodes, as long as (2) $\Nodes$ 
%does not contain $\R$ nor any of the $\ratts$ of $\R$. Let  $\X_{1},\ldots, \X_{l}$ be the population variables that appear in $\R$ but not in $\Nodes$ where ${l}$ is a number could be zero. Then we have
%\begin{flalign}
%\label{update}
%&\ct(\Nodes \cup \eatts(R)|\set{R} = \true, R = F) = & \\ %\nonumber\\
%& \ct(\Nodes|\set{R} = \true, R =*) \times \ct(\X_{1}) \times \cdots \times \ct(\X_{l}) \nonumber & \\
%& -\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = T). \nonumber&
%\end{flalign}
%If $l = 0$, the equation holds without  the %$ \ct(\X_{1}) \times \cdots \times \ct(\X_{k})$ 
%cross-product term.
%
%\end{proposition}
%
%
%\begin{proof}
%The equation 
%\begin{align}
%&\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = *) = &\label{eq:update}  \\ 
%&\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = T)  + & \nonumber \\ 
%&\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = F) & \nonumber
%\end{align}
%holds because the set $\Nodes \cup \eatts(R)$ contains all population variables in $R$ by the assumption that each population variable is associated with at least one $\eatt .$ % $\it{1Node}$. 
%
%%Using table subtraction Equation~\eqref{eq:update} implies
% Equation~\eqref{eq:update} implies
%\begin{align} 
%&\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = \false) =& \label{eq:table-subtract} \\ 
%&\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = *) -&\nonumber  \\
% & \ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = \true).& \nonumber
%\end{align}
%
%To compute the CT-table conditional on the relationship $\R$ being unspecified, we use the equation
%\begin{align}
%&\ct(\Nodes  \cup \eatts(R)|\set{R} = \true, R = *) =  &\label{eq:table-multiply}\\
%&\ct(\Nodes|\set{R} = \true, R =*) \times \ct(\X_{1}) \times \cdots \times \ct(\X_{l})& \nonumber
%\end{align}
%which holds because if the set $\Nodes$ does not contain a population variable of $\R$, then the counts of the associated $\eatts(\R)$ are independent of the counts for $\Nodes$. 
%If $l = 0$, this means there's no new papulation variable introduced, Equations~\eqref{eq:table-multiply} holds without  the cross-product term.
%
%Together Equations~\eqref{eq:table-subtract} and~\eqref{eq:table-multiply} establish the proposition \ref{PivotCT}.
%\end{proof}
%
%Algorithm ~\ref{Pivot_CT_Computation} provides pseudo-code for applying Equation~\eqref{eq:update} to compute a complete CT-table given a partial table where a specified relationship node $\Relation$  is true and another partial table that does not contain the relationship node. We refer to $\Relation$ as the \textbf{pivot} node. For extra generality, we apply the Equation with a condition that lists a set of relationship nodes fixed to be true. 
%
%Algorithm~\ref{level-wise-subtract} shows how the Pivot operation can be applied repeatedly to find all contigency tables in the relationship lattice. 
%
%\begin{enumerate}
%\item Compute $\ct$-tables for entities (lines 1--3).
%\item Compute $\ct$-tables for each single relationship node (lines 4--8).
%\begin{enumerate}
%\item Where the relationship value is unspecified, the $\ct$-table is the cross-product of the $ct$-tables for the entity tables involved in the relationship (line 5).
%\item Where the relationship value is specified as true, the $\ct$-table can be found  using an SQL metaquery (line 6).
%\item Apply the Pivot function to construct the  complete $\ct$-table from the previous two tables (line 7).
%\end{enumerate}
%\item Compute $\ct$-tables for all relationship chains of length $>1$ (lines 9--23).
%\begin{enumerate}
%\item For each relationship chain, find the conditional $\ct$-table where all relationship nodes are specified as true (line 11). 
%\item Order the relationship nodes in the chain arbitrarily. Make each relationship node in order the Pivot node $\Relation_{i}$.
%\item For the current Pivot node $\Relation_{1}$, find the conditional $\ct$-table where $\Relation_{i}$ is unspecified, and the following relations $\Relation_{j}$ with $j>i$ are true (lines 14--19). (This can be computed from a $ct$-table for a shorter chain that has been constructed already.)
%\item The $ct$-table condition on $\Relation_{i}$ and the following relations being true has been constructed already (see loop invariant). Apply the Pivot function to construct the  complete $\ct$-table, conditional on the following relations being true (line 20).
%\end{enumerate}
%\end{enumerate}
%
%
%
%
%
%Algorithm ~\ref{Pivot_CT_Computation} and~\ref{level-wise-subtract} present the fast computation for given length of relationship chain.
%
%The Virtual Join algorithm requires support for two basic tasks: 1) compute initial $\ct$-tables from the base data tables, 
%and 2) apply CT-algebra operations to form new CT-tables from existing ones. %We discuss these in turn.
%
%Figure~\ref{fig:flow} illustrates the computation using  contingency table algebra when there's only one relationship.


%
%
%\begin{figure*}[tb]
%\begin{center}
%\resizebox{0.9\textwidth}{!}{
%%%!%\includegraphics[width=1\textwidth]{pbn}
%%\includegraphics{figures/subtraction-flow.pdf}
%\includegraphics[width=0.9\textwidth]{figures/sub.jpg}
%}
%\caption{Examples of the contingency table algebra operations  following Proposition~\ref{PivotCT} %and equations
%based on partial University database with SQL queries. %[Column-List(TABLE)].
%$\ColumnList({p})= Pop, Teach; \ColumnList({s}) = Intel, Rank;\ColumnList({RA})= Cap, Sal; \ColumnList({CT_{*}}) = \ColumnList({temp}) = Pop, Teach, Intel, Rank; \ColumnList({CT_{T}}) = Pop, Teach, Intel, Rank, Cap, Sal;\ColumnList({CT_{F}}) = Pop, Teach, Intel, Rank;$
%$\ColumnList({CT}) =  \ColumnList(CT_{\false}^{+})  = \ColumnList(CT_{\true}^{+}) = Pop, Teach, Intel, Rank, Cap, Sal, RA.$ to do: change the place?
%\label{fig:flow}}
%\end{center}
%\end{figure*}
%
%
%\begin{algorithm*}[htbp]
%\label{Pivot_CT_Computation}
%%\linesnumbered
%\SetKwData{KwCalls}{Calls}
%\SetKwData{KwCondition}{Precondition:}
%\KwIn{Two conditional contingency tables   $CT_{\true} :=\ct(\it{Nodes},\it{\ratts}(R_{\it{pivot}})|R_{\it{pivot}}=\true$$,\vec{R}=\true)$ and  $CT_{*} :=\ct(\it{Nodes}|R_{\it{pivot}} = *$$,\vec{R}=\true)$ .}
%\KwCondition  %$\it{Nodes}:= \eatts(\R_{1},\ldots,\R_{\ell}) \cup \ratts(\R_{1},\ldots,\R_{\ell}) \cup (\R_1,\ldots,\R_{\ell}) - R_{\it{pivot}} - \ratts(R_{\it{pivot}}) $ \;
% The set $\it{Nodes}$ does not contain the relationship node $R_{\it{pivot}}$ nor any of its descriptive attributes $\ratts(R_{\it{pivot}}$).
%
%% {The set $\it{Nodes}$ contains $\eatts(\R_{1},\ldots,\R_{\ell}) \cup \ratts(\R_{1},\ldots,\R_{\ell}) \cup (\R_1,\ldots,\R_{\ell})$ but not the relationship node $R_{\it{pivot}}$ nor any of its descriptive attributes $\ratts(R_{\it{pivot}}$) \;}
%
%\KwOut{The conditional contigency table $ \ct(\it{Nodes},\it{\ratts}(R_{\it{pivot}}),R_{\it{pivot}}|$$\vec{R}=\true)$ .}
%\begin{algorithmic}[1]
%%\STATE $CT_{\false} := \ct(\it{Nodes}|R_{\it{pivot}} = *$$,\vec{R}=\true) - \pi_{\it{Nodes}} \ct(\it{Nodes},\it{\ratts}(R_{\it{pivot}})|R_{\it{pivot}}=\true$$,\vec{R}=\true)$.
%\STATE $CT_{\false} := CT_{*} - \pi_{\it{Nodes}}CT_{\true}$.
%
%\COMMENT{ Implements the algebra Equation~\ref{update}  in proposition~\ref{PivotCT}. }
%\STATE $CT_{\false}^{+}$ := extend  $CT_{\false}$ with columns $R_{\it{pivot}}$ everywhere false and $\it{\ratts}(R_{\it{pivot}})$ everywhere $n/a$.
%%\STATE $CT_{\true}^{+}$ := extend  $\ct(\it{Nodes},\it{\ratts}(R_{\it{pivot}})|R_{\it{pivot}}=\true$$,\vec{R}=\true)$ with columns $R_{\it{pivot}}$ everywhere true.
%\STATE $CT_{\true}^{+}$ := extend  $CT_{\true}$ with columns $R_{\it{pivot}}$ everywhere true.
%\STATE \Return $CT_{\false}^{+} \cup CT_{\true}^{+}$
%\end{algorithmic}
%\label{alg:pivot}
%\caption{The Pivot function returns a conditional contingency table for a set of attribute nodes and all possible values of the relationship $R_{\it{pivot}}$, including $R_{\it{pivot}} = \false$. %It implements the algebra Equation~\eqref{eq:update}.
% The set of conditional relationships $\vec{R} =(\R_{pivot},\ldots,\R_{\ell})$ %=\true$
% may be empty in  which case the Pivot computes an unconditional CT-table. }
%\end{algorithm*}
%
%
%
%
%\begin{algorithm*}[htbp]
%\label{level-wise-subtract}
%%\linesnumbered
%\SetKwData{KwCalls}{Calls}
%\SetKwData{Notation}{Notation}
%\KwIn{A relational database $\D$; a set of functor nodes}
%\KwOut{A contingency table that lists the count in the database $D$ for each possible assignment of values to each functor node.}
%%\KwCalls  Pivot
%\begin{algorithmic}[1]
%\FORALL{population variables $\X$}
%\STATE compute $\ct(\eatts(\X))$ using SQL queries.
%\ENDFOR
%
%\FORALL{relation node $\R$}
%\STATE $CT_{*} := \ct(\X) \times \ct(\Y)$ where $\X$,$\Y$ are the population variables in $\R$.
%\STATE $CT_{\true} := \ct(\eatts(\R)|\R = \true)$ using SQL joins.
%\STATE Call  $\it{Pivot}(CT_{\true},CT_{*})$ to compute $\ct(\eatts(\R),\ratts(\R),\R)$.
%% with the two tables $CT_{*}$ and $CT_{\true}$  
%\ENDFOR
%
%\FOR{Rchain length $\ell=2$ to $m$}
%\FORALL{Rchain $\R_{1},\ldots,\R_{\ell}$}
%
%\STATE $Current\_CT :=  \ct(\eatts(\R_{1},\ldots,\R_{\ell}), \ratts(\R_{1},\ldots,\R_{\ell})|\R_{1}=\true,\ldots,\R_{\ell}=\true)$ using SQL joins.
%%\STATE $CT_{*} := \ct(\eatts(\R_{2},\ldots,\R_{\ell}), \ratts(\R_{2},\ldots,\R_{\ell})|
%%\R_{1}=*,\R_{2} = \true,\ldots,\R_{\ell}=\true) \times \ct(\X)$ where $\X$ is the population variable in $\R_{1}$, if any, that does not appear in $\R_{2},\ldots,\R_{\ell}$\; 
%%\COMMENT{$CT_{*}$ can be computed from a CT-table for a Rchain of length $\ell-1$.}
%%\STATE  $Current\_CT :=  \it{Pivot}(Current\_CT,CT_{*})$ \;
%%\COMMENT{The table $Current\_CT$ equals $ \ct(\eatts(\R_{1},\ldots,\R_{\ell}), \ratts(\R_{1},\ldots,\R_{\ell})
%%,\R_{1}|\R_{2} = \true,\ldots,\R_{\ell}=\true)$.}
%\FOR{$i=1$ to $\ell$} \label {reine:innerloop}
%\IF{ $i$ equals  1}
%\STATE $CT_{*} := \ct(\eatts(\R_{2},\ldots,\R_{\ell}), \ratts(\R_{2},\ldots,\R_{\ell})|
%\R_{1}=*,\R_{2} = \true,\ldots,\R_{\ell}=\true) \times \ct(\X)$ where $\X$ is the population variable in $\R_{1}$, if any, that does not appear in $\R_{2},\ldots,\R_{\ell}$\; 
%\COMMENT{$CT_{*}$ can be computed from a CT-table for a Rchain of length $\ell-1$.}
%\ELSE
%\STATE $\eatts_{\bar{i}} := \eatts(\R_{1},\ldots,\R_{i-1},\R_{i+1},\ldots,\R_{\ell})$.
%\STATE $\ratts_{\bar{i}} := \ratts(\R_{1},\ldots,\R_{i-1},\R_{i+1},\ldots,\R_{\ell})$.
%%\STATE $\ratts_{i} := \ratts(\R_{1},\ldots,\R_{i-1},\R_{i+1},\ldots,\R_{\ell})$.\
%%\STATE $\it{Nodes_{i}} := \eatts \cup \ratts_{i} \cup \{\R_{1},\ldots,\R_{i-1}\}$.
%%\STATE $\vec{\R} := \{\R_{i+1},\ldots,\R_{\ell}\}$.
%\STATE $CT_{*} := \ct(\eatts_{\bar{i}}, \ratts_{\bar{i}},\R_{1},\ldots,\R_{i-1})|
%\R_{i}=*,\R_{i+1} = \true,\ldots,\R_{\ell}=\true) \times \ct(\Y)$ where $\Y$ is the population variable in $\R_{i}$, if any, that does not appear in $\vec{\R}$. 
%\ENDIF \\
%%\COMMENT{$\vec{R}$ is an Rchain of length $\ell-i$, so the CT-table $ \ct(\eatts,\ratts($$\vec{\R}))$ has been computed already.}
%%\IF{ $i$ equals to 1}
%%\STATE   $CT_{\true} = \ct(\it{Nodes_{i}},\ratts(\R_{i})|\R_{i} = \true,$$\vec{\R} = \true)$ using SQL joins.
%%\ELSE
%%\STATE   $CT_{\true} :=CT_{i-1}$.
%%\COMMENT{updating the $CT_{\true}$ with the $CT_{i-1}$ just computed in previous loop.}
%%\ENDIF \\
%\STATE $Current\_CT :=  \it{Pivot}(Current\_CT,CT_{*})$.
%% with the two tables $CT_{*}$ and $CT_{\true}$  to compute $\ct(\it{Nodes_{i}},\ratts(\R_{i}),\R_{i}|$$\vec{\R} = \true)$.
%\ENDFOR
%
%\COMMENT{Loop Invariant: After  iteration $i$, the table $Current\_CT$ equals $ \ct(\eatts(\R_{1},\ldots,\R_{\ell}), \ratts(\R_{1},\ldots,\R_{\ell})
%,\R_{1},\ldots,\R_{i}|
%\R_{i+1} = \true,\ldots,\R_{\ell}=\true)$}
%%After $i = \ell$, we have computed  $\ct(\eatts(\R_1,\ldots,\R_{\ell}),\ratts(\R_1,\ldots,\R_{\ell}),\R_1,\ldots,\R_{\ell})$}
%
%\ENDFOR
%
%\COMMENT{Loop Invariant: The CT-tables for all Rchains of length $\ell$ have been computed.}
%
%\ENDFOR 
%
%\STATE \Return the CT-table for the Rchain involves all the relationship nodes.
%\end{algorithmic}
%\label{alg:fmt}
%\caption{Virtual Join Algorithm for Computing the Contingency Table for Input Database}%Dynamic Algorithm for Computing  {\em Sufficient Statistics} given one relational database }
%\end{algorithm*}
%
%
%\subsubsection{Complexity Analysis} 
%to do: \textbf{check the consistency of $\cttable$ v.s $\ct$ ??}
%
%The key point about the Virtual Join Algorithm %(VJA) %~\ref{alg:fmt} 
%is that it avoids the generally infeasible enumeration of tuples of entities that are not related.
%{\em The algorithm accesses  only \textbf{existing} tuples, never constructs nonexisting tuples.}
%
%To  analyze the computational cost, we examine the total number of $\ct$-algebra operations performed by the Virtual Join Algorithm. 
%In the next section we discuss the cost of a single  $\ct$-algebra operation.
%
%We provide two upper bounds, one in terms of the number of relationship nodes  $m$, and another in terms of  the number of rows $\row$  in the $\ct$-table which is the output of the algorithm. 
%For these parameters we show that
%$$\it{\# \ct\_\it{ops}} = O(r \cdot \log r) = O(m \cdot 2^{m}) .$$
%
%This shows the efficiency of our algorithm for the following reasons.
%(i) Since the time cost of any algorithm must be at least as great as the time for writing the output, which is as least as great as $\row$, 
%the Virtual Join Algorithm adds at most a logarithmic factor to this lower bound. 
%(ii)  In practice the number $m$ is on the order of the number of tables in the database, 
%which is very small compared to the number of tuples in the tables\footnote{For general $m$, the problem of computing a $\ct$ table in a relational structure is \#P-complete \cite[Prop.12.4]{Domingos2007}.}.
%
%\paragraph{Derivation of Upper Bounds}
%to do : \textbf{give a label for specific line algorithm ?? reference for the identity: maybe some textbook
%}
%
%For a given relationship chain of length $\ell$, the Virtual Join Algorithm goes through the chain linearly (Algorithm~\ref{alg:fmt} inner for loop line ~\ref {reine:innerloop}%12
%). 
%% check line number
%At each iteration, it computes a $\ct_{*}$ table with a single cross product, then performs a single Pivot operation.
% Each Pivot operation requires three  $\ct$-algebra operations. 
%Thus overall, the number of  $\ct$-algebra operations for a relationship chain of length $\ell$ is $6 \cdot \ell = O(\ell)$. For a fixed length $\ell$, there are at most $\binom{m}{\ell}$ relationship chains. Using the known identity
%\begin{align} 
%\sum_{\ell=1}^{m} {m\choose \ell} \cdot \ell = m \cdot  2^{m-1} \label{eq:upperbound}
%\end{align}
%we obtain the $O(m \cdot  2^{m-1}) = O(m \cdot  2^{m})$ upper bound. 
%With the Binomial theorem the identity ~\eqref{eq:upperbound} is easy to prove, we omit further details due to space constraints.
%
%For the upper bound in terms of $\ct$-table rows $\row$, we note that the output $\ct$-table can be decomposed into $2^{m}$ subtables, one for each assignment of values to the $m$ relationship nodes. 
%Each of these subtables contains the same number of rows $d$ , one for each possible assignment of values to the attribute nodes. 
%Thus the total number of rows is given by $r = d \cdot 2^m.$ 
%Therefore we have 
%$m \cdot 2^{m} = \log (r/d) \cdot r/d \leq \log(r) \cdot r.$
%Thus the total number of $\ct$-algebra operations is $O(r \cdot \log(r))$.
%%$\it{\# \ct\_\it{ops}} = O(r \cdot \log r) .$
%
%From this analysis we see that both upper bounds are overestimates. 1) Because relationship chains must be linked by foreign key constraints, the number of valid relationship chains of length $\ell$ is usually much smaller than the number of all possible subsets ${m\choose \ell}$. 2) The constant factor $d$ grows exponentially with the number of attribute nodes, so $log(r) \cdot r$ is a loose upper bound on $log (r/d) \cdot r/d$. 
%We conclude that the number of $\ct$-algebra operations is not the critical factor for scalability, but rather the cost of processing tuples, 
%or in other words, the cost of carrying out a single $\ct$-algebra operation. 
%%In Section~\ref{sec:cta} we discussed how to perform these operations efficiently.
%%
%%Here we give a quick proof of the identity ~\eqref{eq:upperbound} %or put it in the appendix
%%\begin{proof}
%%With the Binomial theorem, for any non-negative integers $l,m$, the binomial formula of two variables $a,b$ is 
%%$$ f(a,b)=\sum_{\ell=0}^{m}\binom{m}{\ell} \cdot a^{m-\ell} \cdot b^\ell = (a+b)^m. $$
%%Suppose we substitute $a$ with one, for any $b$, we have 
%%$$
%%f(1,b)= \sum_{\ell=0}^m\binom{m}{\ell} \cdot b^\ell=(1+b)^m.
%%$$
%%And then we could compute the partial derivative with respect to $b$
%%$$
%%f'_{b}(1,b)= \sum_{\ell=1}^m\binom{m}{\ell}\cdot \ell \cdot b^{\ell-1}=m \cdot (1+b)^{m-1}.$$
%%By substituting $b$ with one, we have 
%%$$
%%f'_{b}(1,1)=  \sum_{\ell=1}^m\binom{m}{\ell}\cdot \ell \cdot 1^{\ell-1}= \sum_{\ell=1}^m\binom{m}{\ell}\cdot \ell = m\cdot(2)^{m-1}.$$
%%
%%That means the following identity
%%$$\sum_{\ell=1}^{m} {m\choose \ell}\cdot \ell = m * 2^{m-1}  $$ holds.
%%\end{proof}
%

%\section{Bayes net Structure Learning }
%
%
%Thus the lattice structure defines a {\em multinet} rather than a single Bayes net. Multinets are a classic Bayes net formalism for modelling {\em context-sensitive} dependencies among variables. They have been applied  for modelling diverse domains, such as sleep disorders, eye diseases, and turbines that generate electricity. 
%Geiger and Heckerman contributed a standard reference article for the multinet formalism  \cite{Geiger1996}.
%
%This Learn-and-Join Algorithm (LAJ)  takes as input a relational database and constructs a Bayes multinet for each relationship set in the multinet lattice. We provide a brief summary; for a detailed description please see \cite{Schulte2011}. The algorithm applies a standard single-table Bayes net learner, which can be chosen by the user, to the contingency table for each lattice point. Computing the contingency tables is the conceptually and computationally most challenging part of our system. We describe a dynamic programming algorithm for precomputing the contingency tables in Section~\ref{sec:cta} below. In this section we describe the structure learning algorithm on the assumption that the contingency tables have been precomputed.
%
% The algorithm proceeds level-wise by considering relationship sets of length $s = 0,1, 2, \ldots$. After Bayes nets have been learned for sets of length $s$, the learned edges are propagated to sets of length $s+1$. In the initial case of single relationship tables where $s=0$, the edges are propagated from Bayes nets learned for entity tables. Propagated edge constraints are of two types: Required and forbidden edges. 
%
%Suppose that $\rchain$ at level $s$ is a parent of $\rchain'$ at level $s+1$.  For required edges, then if an edge $\node \rightarrow \node'$ is present in the Bayes net $\B_{\rchain}$ for the parent chain $\rchain$, the edge must also contained in the Bayes net $\B_{\rchain'}$ for the child  chain $\rchain'$. In other words, edges present in smaller relationship chains are inherited by larger relationship chains. For forbidden edges, if an edge $\node \rightarrow \node'$ is absent from the Bayes nets of all parent chains of $\rchain'$, it must also be absent from the Bayes net of $\rchain'$. In other words, the absence of edges in smaller relationship chains is inherited by larger relationship chains.
%
%{\em Example.} 
%If the $\student$ Bayes net contains an edge $$\it{intelligence}(\S) \rightarrow \it{ranking}(\S),$$ then the Bayes net associated with the relationship $\it{\ra(\S,\C)}$ must also contain this edge (see Figure~\ref{fig:university-tables}). 
%If the $\prof$ Bayes net does not contain an edge $$\(\C) \rightarrow \it{difficulty}(\C)$$, it must also be absent in the Bayes net associated with the relationship $\ra(\S,\C)}$.
%If the $\it{Course}$ Bayes net does not contain an edge $$\it{rating}(\C) \rightarrow \it{difficulty}(\C)$$, it must also be absent in the Bayes net associated with the relationship $\it{Registered(\S,\C)}$.


%{\em Examples.} 
%Consider the lattice shown in Figure~\ref{fig:big-lattice}. 
%Suppose that the graph associated with the relationship $\reg(\S,\C)$ contains an edge $$\it{difficulty}(\C) \rightarrow \it{intelligence}(\S),$$ and that the graph associated with the relationship $\it{TA}(\S,\C)$ does {\em not} contain the edge $\it{difficulty}(\C) \rightarrow \it{intelligence}(\S)$. 
%%(The latter will be the case since the relationship set $\it{TA}(\S,\C)$ does not contain the variable $\S$.)
%% actually, the answer is that the main functor constraint overrides the inheritance constraint.
%Then the edge $\it{difficulty}(\C) \rightarrow \it{intelligence}(\S)$ must be present in the Bayes net associated with the larger relationship set $\reg(\S,\C)$, $\it{TA}(\S,\C)$. If the edge is contained  in neither of the graphs associated with $\it{Registered}(\S,\C)$, and $\it{TA}(\S,\C)$, it must not be present in the graph associated with the $\it{Registered}(\S,\C)$, $\it{TA}(\S,\C)$.
%
%
%Algorithm~\ref{alg:learn} provides pseudo-code. The next section discusses how the edge constraint propagation can be implemented compactly using SQL. 
%
%
%\begin{algorithm}[htbp]
%\label{alg:learn}
%%\linesnumbered
%\SetKwData{KwCalls}{Calls}
%\SetKwData{KwNotation}{Notation}
%\KwIn{A lattice comprising relationship chains and population variables.}
%\KwOut{A Bayes net model for each point in the lattice.}
%\KwNotation $\BNLearn$: Any propositional Bayes net learner that accepts edge constraints and a single table of cases as input.
%%\KwNotation $\BNLearn(\cttable,\it{Econstraints})$ denotes the output DAG of PBN. 
%\BlankLine
%\begin{algorithmic}[1]
%\FORALL{population variables $\X$}
%\STATE Call $\BNLearn(\ct(\X),\emptyset)$ to compute a Bayes net using the pre-computed $\ct$-table for $\X$ (and no constraints).
%\ENDFOR
%
%\FOR{Rchain length $\ell=1$ to $m$}
%\FORALL{Rchain = $\R_{1},\ldots,\R_{\ell}$}
%\STATE Econstraints = Forbidden + Required Edges for  Rchain.%$\R_1,\ldots,\R_\ell$.
%%\BlankLine
%\STATE $\cttable$ := the precomputed CT-table for  Rchain.% $\R_1,\ldots,\R_\ell$.
%\STATE Call $\BNLearn(\cttable,\it{Econstraints})$ to compute a Bayes net for  Rchain.%$\R_1,\ldots,\R_\ell$. 
%\ENDFOR
%\ENDFOR
%\end{algorithmic}
%\label{alg:fmt}
%\caption{Pseudocode for Hierarchical Bayes Net Structure Learning}
%\end{algorithm}
%
%\begin{figure}[htbp]
%\begin{center}
%\resizebox{0.5\textwidth}{!}{
%\includegraphics[width=0.3\textwidth]{figures/path-bn.png}
%\includegraphics[width=0.3\textwidth]{figures/inherited-view.png}
%\includegraphics[width=0.3\textwidth]{figures/view-flow.png}
%}
%\caption{Path bayes net and Edge Propagation
%\label{fig:path-bn}}
%\end{center}
%\end{figure}
%
%\subsection{The Model Manager: Edge Propagation} 
%%\paragraph{The Model Manager: Implementing Hierarchical Model Search in SQL}
%Figure~\ref{figure:flow} illustrates the system flow on part of the relationship lattice. The Bayes net learner receives a CT-table and edge constraints as input, and returns a Bayes net. The view mechanism propagates the results as constraints of learning to larger relationship chains, without the need for any further computation.
%
%The Bayes multinet can be stored in a master table called {\em PathBayesNet}, as shown in Figure \ref{fig:path-bn}. An entry such as $$\langle [\it{Registered}(\S,\C),\it{RA}(\S,\P)],\it{ranking}(\S),\it{intelligence}(\S)\rangle$$ means that the 
%Bayes net graph for the relationship chain $\it{Registered}(\S,\C),\it{RA}(\S,\P)$ contains an edge $\it{ranking}(\S)\leftarrow \it{intelligence}(\S)$. 

%
%\begin{figure}[htbp]
%\begin{center}
%\resizebox{0.5\textwidth}{!}{
%\includegraphics[width=0.3\textwidth]{figures/inherited-view.png}
%\includegraphics[width=0.3\textwidth]{figures/view-flow.png}
%}
%\caption{Edge Propagation.
%\label{figure:flow}}
%\end{center}
%\end{figure}


%
%The SQL View mechanism propagates edges compactly, as shown in Figure~\ref{fig:path-bn}. 
%%The advantage of the view mechanism is that each time that the Bayes net learner completes learning for a lattice point, the view  update mechanism propagates the results as constraints on learning for the children of the lattice point. 
%The {\em LatticeRel} table records which relationship chains are parents of which others. This auxilliary table makes it easy to write a query that finds, for a given child chain, all the parent chains and inserts their edges as entries for the child chain. This view implements the propagation of required edges. The view definition for forbidden edges is similar: the only difference is that we first need to find the set of complement edges that are {\em not} included in the Bayes net of a given relationship chain. The complement edges are also computed by a view (not shown). 
%


%
%
%
%\section{Introduction} 
%Scalable link analysis for relational data with multiple link types is a challenging problem in network science.
% We describe a method for learning a Bayes net that captures simultaneously correlations between link types, link features, and attributes of nodes. Such a Bayes net provides a succinct graphical representation of complex statistical-relational patterns. A  Bayes net model supports powerful probabilistic reasoning for answering ``what-if'' queries about the probabilities of uncertain outcomes conditional on observed events.
%Previous work on learning Bayes nets for relational data was restricted to correlations among attributes given the existence of links \cite{Schulte2012}. The larger class of correlations examined in our new algorithms includes two additional kinds:
%
%\begin{enumerate}
%\item Dependencies between  different types of links.
%\item Dependencies among node attributes given the {\em absence} of a link between the nodes.
%\end{enumerate}
%
%Discovering such dependencies is useful for several applications. 
%
%\begin{description}
%\item[Knowledge Discovery] Dependencies provide valuable insights in themselves. For instance, a web search manager may wish to know whether if a user searches for a video in Youtube for a product, they are also likely to search for it on the web. 
%\item[Relevance Determination] Once dependencies have been established, they can be used as a relevance filter for focusing further network analysis only on statistically significant associations. For example, the classification and clustering methods of Sun and Han \cite{Sun2012} for heterogeneous networks assume that a set of ``metapaths'' have been found that connect link types that are associated with each other. 
%\item[Query Optimization] The Bayes net model can also be used to estimate relational statistics, the frequency with which statistical patterns occur in the database \cite{Schulte2012b}. This kind of statistical model can be applied for database query optimization \cite{Getoor2001}, \cite{Tzoumas_VLDBJ2013}.
%\end{description}
%
%%\paragraph{Approach}
%\subsection{Approach}
%
%We consider three approaches to multiple link analysis with Bayes nets. 
%
%\begin{description}
%\item[Flat Search] Applies a standard Bayes net learner to a single large join table. This table is formed as follows: (1) take the cross product of entity tables. (An entity table lists the set of nodes of a given type.) (2) For each tuple of entities, add a relationship indicator whose value ``true'' or ``false'' indicates whether the relationship holds among the entities. 
%\item[Hierarchical Search] Conducts bottom-up  search
% through the lattice of table joins hierarchically. Dependencies (Bayes net edges) discovered on smaller joins are propagated to larger joins. 
%The different table joins include information about the presence or absence of relationships as in the flat search above. 
%This is an extension of the current state of the art Bayes net learning algorithm for relational data \cite{Schulte2012}.
%\end{description}
%
%
%%(1) Baseline: The learn-and-join algorithm is the state of the art method for learning Bayes nets that capture correlations among attributes of entities or nodes. It conducts a hierarchical search
%% through the lattice of table joins. Dependencies discovered on smaller joins are propagated to larger joins.  The current version of the learn-and-join method considers only correlations between attributes and link types, not correlations between link types. (2) {\em Hierarchical Search With Link Types.} We extend the learn-and-join algorithm to consider correlations among link types. This is done by adding a new feature for each relationship table that indicates for each tuples of entities, whether they are related or not. 
%%(3) {\em Flat search}: Form a single big join table that combines different relationships with the new relationship indicator feature. Then apply a standard Bayes net learner on the join table. 
%
%%\paragraph{Evaluation.}
%\subsection{Evaluation.} 
% We compare the learned models using standard scores (e.g., Bayes Information Criterion, log-likelihood). %
%These results indicate that both flat search and hierarchical search are effective at finding correlations among link types. 
%Flat search can on some datasets achieve a higher score by exploiting attribute correlations that depend on the absence of relationships. 
%Structure learning time results indicate that hierarchical search is substantially more scalable.
%
%The main contribution of this paper is extending the current state-of-the-art  
%Bayes net learner to model correlations among different types of links, with a comparison of a flat 
%and a hierarchical search strategy.
%
%% \paragraph{Contributions} 
%% 
%% \begin{enumerate}
%% \item To our knowledge this is the first application of Bayes net learning to modelling correlations among different types of links.
%% \item Extension of a lattice search strategy for link type modelling, with a comparison to a flat search join approach.
%% \end{enumerate}
%
%%\paragraph{Paper Organization}
%\subsection{Paper Organization}
% We describe Bayes net models for relational data (Poole's Parametrized Bayes Nets). Then we present the learning algorithms, first flat search then hierarchical search. We compare the models on four databases from different domains.
%


%Han: rchain = metapath
%
%\textbf{\cite{MCDB_SIGMOD_2008} initial attemp to design and prototype a MC-based system for managing uncertain data : probabilistic databae }
%\textbf{\cite{Wick_VLDB_2010} probabilistic databases, graphical modes with mcmc sampling }
%
%\textbf{\cite{Wang_Sigmod_2011}in-database implementations of a wide variety of inference algorithms (PostgreSQL) }
%
%\textbf{\cite{MLbase_ICDR_2013} higher level libary of a bunch of ml algorithms }
%\textbf{\cite{MADlib_VLDB_2012} higher level machine learning algorithms library (could cite the part, why databases?) }
%\textbf{\cite{Feng_SIGMOD_2012} single unified architecture, in-database analytics, consider data analytics techniques that are modeled as convex programming problems }
%\textbf{\cite{Wang2008} BayesStore: managing large, uncertain data repositories with probabilistic 	graphical models}
%\textbf{\cite{Deshpande_VLDB2007} Probabilistic graphical models and their role in databases}
%
%\textbf{\cite{Graepel_CIKM13}Automated Probabilistic Modelling for Relational Data }
%
%
%\textbf{\cite{Domingos_MRDM2003}Prospects and challenges for multi-relational data mining }
%
%Approaches to structure learning for directed graphical models with link uncertainty have been previously described, such as \cite{Getoor2007c}. However
%to our knowledge, no implementations of such structure learning algorithms for directed graphical models are available. Our system builds on the state-of-the-art Bayes net learner for relational data, whose code is available at \cite{bib:jbnsite}.
%Implementations exist  for other types of graphical models, specifically Markov random fields (undirected models) \cite{Domingos2009} 
%and dependency networks (directed edges with cycles allowed) \cite{Natarajan2012}. 
%Structure learning programs for Markov random fields are provided by Alchemy \cite{Domingos2009} and Khot et al \cite{Khot2013}. Khot et al. use boosting to provide a state-of-the-art dependency network learner. None of these programs are able to return a result on half of our datasets because they are too large. For space reasons we restrict the scope of this paper to directed graphical models and do not go further into undirected model. For an extensive comparison of the learn-and-join Bayes net learning algorithm with Alchemy please see \cite{Schulte2012}.
%



%\section{Background and Notations}
%\subsection{Relational Databases }
% \begin{table}[tbp] \centering
%\begin{tabular}
%[c]{|l|}\hline\\
%$\student$(\underline{$student\_id$}, $\intelligence$, $ranking$)\\
%$\course$(\underline{$course\_id$}, $\diff$, $rating$)\\ 
%$\prof$ (\underline{$professor\_id$}, $teaching\_ability$, $popularity$)\\
%$\reg$ (\underline{$student\_id$, $course\_id$}, $grade$, $satisfaction$)\\
%$\ra$ (\underline{$student\_id$, $prof\_id$}, $salary$, $capability$)\\
%$\it{Teaches} $(\underline{$professor\_id$, $course\_id$})\\
%\\
%\hline
%\end{tabular}
%\caption{Replace by a ER Diagram: A relational schema for a university domain. Key fields are underlined. An instance for this schema is given in Figure \ref{fig:university-tables}.
%\label{table:university-schema}} 
%\end{table}

%\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
% \centering
%\resizebox{0.4\textwidth}{!}{
% \includegraphics[width=0.5\textwidth]{figures/university-schema.png} 
%} 
%\caption{A relational ER Design for a university domain.}
% \label{fig:university-schema}
%\end{figure}
%
% We begin with a standard \textbf{relational schema} containing a set of tables, each with key fields, %typically
%descriptive attributes, and possibly foreign key pointers. 
%A \textbf{database instance} specifies the tuples contained in the tables of a given database schema. 
%We assume that tables in the relational schema can be divided into {\em entity tables} and {\em relationship tables.} 
%This is the case whenever a relational schema is derived from an entity-relationship model (ER model) \cite[Ch.2.2]{Ullman1982}. 
%The functor formalism is rich enough to represent the constraints of an ER schema by the following translation: Entity sets correspond to types, descriptive attributes to functions, relationship tables to predicates, and foreign key constraints to type constraints on the arguments of relationship predicates.  
%Assuming an ER design, a relational structure can be visualized as a complex network \cite[Ch.8.2.1]{Russell2010}: individuals are nodes, attributes of individuals are node labels, relationships correspond to (hyper)edges, and attributes of relationships are edge labels. Conversely, a complex  network can be represented using a relational database schema.


%Table \ref{table:university-schema} shows a relational schema for a database related to a university.
%In this example, there are three entity tables: a $\student$ table, a $\course$ table and a $\prof$ table.  
%The relationship table $\reg$ with foreign key pointers to the $\student$ and $\course$ tables whose tuples indicate which students have registered in which courses.
%
%The other two relationship tables have the similar foreign key constraints involving different entity tables. 
%
%Figure \ref{fig:university-tables} displays a small database instance for this schema together with a Parametrized Bayes Net (only considering the $\reg$ relationship for simplicity.) 
%%To keep the schema simple, we introduce only a limited number of attributes for each entity class. 
%
% 
%\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
% \centering
%\resizebox{0.5\textwidth}{!}{
% \includegraphics[width=0.5\textwidth]{figures/university-tables.png} 
%} 
%\caption{Database Table Instances: (a) $\student$, (b) $\course$, (c) $\reg$.  (d)A sample Parametrized Bayes Net based on the university schema.  % The $\cttable(\reg)$ table  is the contingency table for $\reg$ which we will introduce in Section~\ref {sec:mobius}. (e) 
%}
% \label{fig:university-tables}
%\end{figure}
%


%\subsection{Propositional Machine Learning}
%bla,bla,

%\subsection{Bayes Nets for Relational Data}
%Poole introduced the Parametrized Bayes net (PBN) formalism that combines Bayes nets with logical syntax for expressing relational concepts \cite{Poole2003}. We adopt the PBN formalism, following Poole's presentation.
%A \textbf{population} is a set of individuals.
%Individuals are denoted by lower case expressions (e.g., $\it{bob}$). 
%%A \textbf{population variable} is capitalized. 
%A \textbf{functor} represents a mapping
%$\functor: \population_{1},\ldots,\population_{a} \rightarrow \outdomain_{\functor}$
%where $\functor$ is the name of the functor, each $\population_{i}$ is a population, and $\outdomain_{\functor}$ is the output type or \textbf{range} of the functor. 
%In this paper we consider only functors with a finite range, disjoint from all populations.  If $\outdomain_{\functor} = \{\true,\false\}$, the functor $\functor$ is a (Boolean) \textbf{predicate}. A predicate with more than one argument is called a \textbf{relationship}; other functors are called \textbf{attributes}. We use uppercase for predicates and lowercase for other functors.
%
%
%
%%the populations associated with 
% %the variables are of the appropriate type for the functor.
%%A \textbf{Parametrized random variable} (PRV) is of the form $\functor(\X_{1},\ldots,\X_{a})$, where each $\X_{i}$ is a 1st-order variable. 
%Each 1st-order variable is associated with a population that specifies its domain or type. 
%We refer to the 1st-order variables as population variables to distinguish them from the parametrized random variables. %that appear in a Bayes net model. 
%
%
%Figure \ref{fig:university-tables} displays a small database instance for this schema together with a Parametrized Bayes Net (only considering the $\ra$ relationship for simplicity.) 
%%To keep the schema simple, we introduce only a limited number of attributes for each entity class. 
%
% 
%\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
% \centering
%\resizebox{0.4\textwidth}{!}{
% \includegraphics[width=0.5\textwidth]{figures/university-tables.png} 
%} 
%\caption{Database Table Instances: (a) $\student$, (b) $\prof$, (c) $\ra$.  (d) A sample Parametrized Bayes Net based on the university schema.  % The $\cttable(\reg)$ table  is the contingency table for $\reg$ which we will introduce in Section~\ref {sec:mobius}. (e) 
%}
% \label{fig:university-tables}
%\end{figure}



%\textbf{OS: make table for random variable database}
%move  to the appendix
%\begin{table}[htbp]
%  \centering
%\resizebox{0.5\textwidth}{!}{
%    \begin{tabular}{|r|r|r|r|r|r|}
%    \hline
%    \multicolumn{2}{|c|}{Table Name} & \multicolumn{4}{c|}{Schema}  \\
%    \hline
%    \multicolumn{2}{|l|}{AttributeColumns} & \multicolumn{4}{l|}{\begin{tabular}{l}TABLE\_NAME,  COLUMN\_NAME   \end{tabular}}  \\
%    \hline
%    \multicolumn{2}{|l|}{Domain} & \multicolumn{4}{l|}{\begin{tabular}{l}COLUMN\_NAME, VALUE   \end{tabular}}  \\
%    \hline
%    \multicolumn{2}{|l|}{Pvariables} & \multicolumn{4}{l|}{\begin{tabular}{l}Pvid,  TABLE\_NAME  \end{tabular}}  \\
%    \hline
%    \multicolumn{2}{|l|}{1Variables} & \multicolumn{4}{l|}{\begin{tabular}{l}1VarID,  COLUMN\_NAME,  Pvid 
%\end{tabular} }  \\
%    \hline
%    \multicolumn{2}{|l|}{2Variables } & \multicolumn{4}{l|}{\begin{tabular}{ll} 2VarID,  COLUMN\_NAME,  Pvid1,  Pvid2, \\ TABLE\_NAME \end{tabular}}  \\
%    \hline
%    \multicolumn{2}{|l|}{Relationship} & \multicolumn{4}{l|}{\begin{tabular}{lll}RVarID,  TABLE\_NAME, Pvid1,  Pvid2, \\ COLUMN\_NAME1,  COLUMN\_NAME2  \end{tabular}}  \\
%    \hline
%    \end{tabular}%
%}
%  \caption{Schema for Random Variable Database}
%  \label{table:rvdb}%
%\end{table}%




%To build a table with definitions of nodes, we represent a node as a tuple of the form $\langle \functor, \it{pv}_1, \ldots, \it{pv}_\ell \rangle$; in this paper we assume that  $\ell = 1,2$. Table~\ref{table:translation} shows examples.
%Adding population variables to functors generally requires only utilizing the primary key information to find which entities are associated with each functor and then using the population variables constructed in the previous step. If we find a self-relationship, we add one population variable for each role in the relationship, and one set of 1Nodes for each population variable. 

%\begin{quote}
%CREATE TABLE 1VARIABLES AS SELECT \\
%CONCAT($'`'$, COLUMN\_NAME, $'(', $pvid,$ ')', '`')$ AS 1VarID,\\
%COLUMN\_NAME,    pvid\\
%%    index\_number = 0 AS main \\
%FROM\\
%PVariables        NATURAL JOIN    AttributeColumns;
%\end{quote}
%\subsection{The Random Variable Database}
%The information about the random variables is stored in the \textbf{random variable database}.  
%%This database provides metainformation for machine learning analysis analogous to the way in which the system catalog provides metainformation for database queries. The definitions of the random variables are stored in tables in the random variable database, together with useful metainformation about the random variables, such as their domains and the sizes of their domains. 
%%
%The Virtual Join algorithm distinguishes three types of PRVs, 1Nodes, RNodes, and 2Nodes. 
%For each attribute of an entity, there is a corresponding attribute node called a \textbf{1Node} for short. The domain of a 1Node is the same as that of the corresponding attribute.  There is a Boolean random variable for each relationship set, called a \textbf{relationship node} or \textbf{RNode} for short.
%Throughout this paper we assume that all relationships are binary, though this is not essential for our algorithm. 
%For each attribute of an relationship, there is a corresponding attribute node called a \textbf{2Node} for short. The domain of a 2Node is the same as that of the corresponding attribute. 
%%In logical terms, nodes are interpreted as functions that return a value for a given input entity tuple \cite{Milch2005}.
%Table~\ref{table:translation} illustrates %the mapping from the ER design to 
%the logical functor notation and its relationship to the database ER diagram. 

%
%The basic set of random variables is derived from the elements of an ER design.
%Table~\ref{table:translation} illustrates the mapping from the ER design to the logical functor notation. 
%There is a random variable for each entity set, called a \textbf{population variable}. 
%The domain of population variables comprises the entities in the associated entity set. 
%There is a Boolean random variable for each relationship set, called a \textbf{relationship node} or \textbf{RNode} for short.
%Throughout this paper we assume that all relationships are binary, though this is not essential for our system. 
%For each attribute of an entity, there is a corresponding attribute node called a \textbf{1Node} for short. The domain of a 1Node is the same as that of the corresponding attribute.  
%For each attribute of an relationship, there is a corresponding attribute node called a \textbf{2Node} for short. The domain of a 2Node is the same as that of the corresponding attribute. 
%In logical terms, nodes are interpreted as functions that return a value for a given input entity tuple \cite{Milch2005}.
%As the table shows, we refer to the attributes of a population variable $\A$ collectively as $1Nodes(\A)$ and to the attributes of a relationship node $\R$ collectively as $2Nodes(\R)$. %\cite{BLOG}. 

%The value of a relationship attribute is undefined for entities that are not related.
% For example, if student $\s$ is not registered in course $\c$, the value of $\it{grade}(\s,\c)$ is undefined. 
%Following \cite{Milch2005}, 
%%\cite{BLOG}, 
%we indicate this by writing $\it{grade}(\s,\c) = n/a$ for a reserved constant $\it{n/a}$. 
%The assertion $\it{grade}(\s,\c) = n/a$ is therefore equivalent to the assertion that $\reg(\s,\c) = \false$.
 



%\begin{figure}[htbp]
%\begin{center}
%\resizebox{0.5\textwidth}{!}{
%\includegraphics[width=0.5\textwidth]{figures/translation.png}
%}
%\caption{Translation from ER Diagram of Relational Database to Bayes Nets.
%\label{fig:translation}}
%\end{center}
%\end{figure}

%\begin{table}[btp] \centering
%\resizebox{0.5\textwidth}{!}{
%\begin{tabular}[c]
%{|l|l|l|l|}\hline
% ER Design &Bayes Net & Example &Term Notation\\\hline
%    Entity Tables&Population Variables  & Student, Course & S, C \\\hline
%    Relation Tables &RNodes &Registered & Registered(C,S) \\\hline
%   Entity Attributes &1Nodes & intelligence, ranking & \{intelligence(S), ranking(S)\} = 1Nodes(S) \\\hline
%   Relationship Attributes &2Nodes & satisfaction, grade & \{satisfaction(C, S), grade(C,S)\} = 2Nodes(Registered(C,S)) \\\hline
%   
%\end{tabular}
%}
% % end scalebox
%\caption{Translation from ER Design of Relational Database to Bayes Nets.
% \label{table:translation}}
%\end{table}
%\begin{table}[btp] \centering
%\resizebox{0.4\textwidth}{!}{
%\begin{tabular}[c]
%{|l|c|l|l|}\hline
% ER Design &Type & Functor &PRV \\\hline
%   % \begin{tabular}{l}Entity \\Tables\end{tabular}&\begin{tabular}{l}Population \\Variables \end{tabular} & Student, Course & S, C \\\hline
%    \begin{tabular}{l}Relation \\Tables \end{tabular}&RNodes &RA & RA(P,S) \\\hline
%   \begin{tabular}{l}Entity \\Attributes \end{tabular}&1Nodes & intelligence, ranking &\begin{tabular}{l} \{intelligence(S), ranking(S)\}  \\=1Nodes(S) \end{tabular} \\\hline
%  \begin{tabular}{l} Relationship \\Attributes \end{tabular}&2Nodes & teachingability, salary &\begin{tabular}{l}   \{teachingability(P,S), salary(P,S)\}  \\= 2Nodes(RA(P,S))\end{tabular}\\\hline
%   
%\end{tabular}
%}
% % end scalebox
%\caption{Translation from ER Diagram to Relational Random Variables.
% \label{table:translation}}
%\end{table}

%{\em Database Representation.} 
%The definitions of the three types of PRVs are listed in three tables in the
% Random Variable Database. 
%This database also provides information about the random variables that is required for learning, such as their domains and the sizes of their domains. We refer to this as \textbf{metainformation}. Utilizing the metainformation allows us to develop general schema-independent learning algorithms. 
%
%\subsection{Generating Default PRVs} Defining a set of PRVs for a given database requires some user expertise in translating domain knowledge into the functor representation and metainformation.
%% terms of understanding the functor representation and extracting metainformation. 
%An alternative is to exploit the database schema catalog to generate default random variable database.
%We use this default in the experiments reported below.  
%Given an ER-diagram, it is straightforward to generate functors as illustrated in Table~\ref{table:translation}.
% Given a database information schema, we can compute an implicit ER diagram by reversing the standard translation from ER-diagrams to SQL \cite{Ullman1982}.
%For instance, if a table contains a single primary key column and no foreign key pointers, we treat it as representing an entity set and associate a population variable with it. 
%Since the database information schema is itself stored in table form, the default random variable database can be created using an SQL script. 
%We omit further details due to space constraints.
%
%The main complication arises when the database contains a self-relationship \cite{Heckermann} that relates two entities of the same type.
%%, when we need to introduce more than one relationship node for the relationship functor. 
%For example, the Mondial database contains a self-relationship $\it{Borders}$ that relates two countries. 
%%In the ER diagram, there are two lines from the $\it{Countries}$ entity set to the $\it{Borders}$ relationship set 
%%The ring structure may be represented with two different population variables that each refer to the $\it{Countries}$ entity set. 
%The corresponding relationship node is $\it{Borders}(\C_{1},\C_{2})$ . The different positions of the 1st-order variables can represent different roles in the relationship. 
%%Since we introduce two population variables referring to the same entity set, we need to duplicate the 1Nodes for that entity set. 
%%This allows the logical language to represent that attributes of neighboring countries influence each other, 
%%%Using a second relationship node with the $\it{Borders}$ functor allows us to represent such influences, 
%%for instance in the following rule
%%$$\it{continent}(\C_{1}) = \it{Europe}, \it{Borders}(\C_{1}, \C_{2}) = \true \rightarrow \it{continent}(\C_{2}).$$
%If the database schema contains a self-relationship, our script adds one population variable for each role in the relationship in the default database. For each population variable there is a separate set of corresponding 1Nodes. In the Mondial example, there are two 1Nodes $\it{continent}(\C_{1})$ and $\it{continent}(\C_{2})$.
%
%
%To build a table with definitions of nodes, we represent a node as a tuple of the form $\langle \functor, \it{pv}_1, \ldots, \it{pv}_\ell \rangle$; in this paper we assume that  $\ell = 1,2$. Table~\ref{table:translation} shows examples.
%Adding population variables to functors generally requires only utilizing the primary key information to find which entities are associated with each functor and then using the population variables constructed in the previous step. If we find a self-relationship, we add one population variable for each role in the relationship, and one set of 1Nodes for each population variable. 
%
%We first discuss computing a default set of population variables and functors from the schema information, then combining the variables with the functors to form terms.
%
%\begin{algorithm}[htbp]
%\caption{Generating a default set of functors for modelling a target database. Can be implemented with SQL queries to the database information schema.}
%\label{alg:functors}
%%\linesnumbered
%\SetKwData{KwCalls}{Calls}
%\SetKwData{KwCondition}{Precondition:}
%\KwIn{Database Information Schema; Target Database $\D$}
%\KwOut{Tables with meta information about functors, stored in the random variable database $\D_{\RV}$.}
%\begin{algorithmic}[1]
%\STATE Find tables in $\D$ with a single primary key field. Store table names in $\D_{\RV}.\it{EntityTables}$.
%\STATE For each entity table, %.
%add one population variable entry $(\it{pv})$ in $\D_{\RV}.\it{Pvariables}$.
%\STATE For each entity table, for each column that is not part of the primary key, add an entry $(\it{column\_name})$ in $\D_{\RV}.\it{1Functors}$.
%%\STATE For each entity table, for each column that is not part of the primary key, add an entry $(\it{column\_name},\it{pv})$ in $\D_{\RV}.\it{1Functors}$.
%\STATE Find tables in $\D$ with two primary key columns that reference entity tables. Store table names in $\D_{\RV}.\it{RelationTables}$.
%% \STATE For each relation table, add an entry $(\it{table\_name}, \it{pv}_{1}, \it{pv}_{2})$ to $\D_{\RV}.
% \STATE For each relation table, add an entry $(\it{table\_name})$ to $\D_{\RV}
%\it{RFunctors}$. 
%\STATE For each relation table , for each column that is not part of the primary key, add an entry $(\it{column\_name})$  in $\D_{\RV}.\it{2Functors}$.
%%\STATE For each relation table , for each column that is not part of the primary key, add an entry $(\it{column\_name}, \it{pv}_{1}, \it{pv}_{2})$  in $\D_{\RV}.\it{2Functors}$.
%\end{algorithmic}
%\end{algorithm}
%
%
%\subsection{Generating Nodes}
%\textbf{duplicate part?}
%There are many statistical dependencies that functors by themselves are not sufficient to express. 
%These concern relational autocorrelations \cite{jensen}, where the attribute values of related entities influence each other. This type of dependency occurs when the ER diagram contains a self-relationship.
%%, when we need to introduce more than one relationship node for the relationship functor. 
%For example, the Mondial database contains a self-relationship $\it{Borders}$ that relates two countries. In the ER diagram, there are two lines from the $\it{Countries}$ entity set to the $\it{Borders}$ relationship set that form a ring \cite{ullmann}. \textbf{role in relationship? ring?}
%The ring structure may be represented with two different population variables that each refer to the $\it{Countries}$ entity set. The corresponding relationship node is $\it{Borders}(\C_{1},\C_{2})$ . The different positions of the variables can represent different roles in the relationship. 
%%Since we introduce two population variables referring to the same entity set, we need to duplicate the 1Nodes for that entity set. 
%This allows the logical language to represent that attributes of neighboring countries influence each other, 
%%Using a second relationship node with the $\it{Borders}$ functor allows us to represent such influences, 
%for instance in the following rule
%$$\it{continent}(\C_{1}) = \it{Europe}, \it{Borders}(\C_{1}, \C_{2}) = \true \rightarrow \it{continent}(\C_{2}).$$
%
%To build a table with definitions of nodes, we represent a node as a tuple of the form $\langle \functor, \it{pv}_1, \ldots, \it{pv}_\ell \rangle$; in this paper we assume that  $\ell = 1,2$. Table~\ref{table:translation} shows examples.
%Adding population variables to functors generally requires only utilizing the primary key information to find which entities are associated with each functor and then using the population variables constructed in the previous step. If we find a self-relationship, we add one population variable for each role in the relationship, and one set of 1Nodes for each population variable. 
%-unielwin: pvariable, rnodes,1nodes, 2nodes.... show the whole setup database.
%- Pseudocode for explaining the metadata script. in Figure \ref{fig:rv_db}
%- mention self-relationship detail





%Relational learning algorithms explore longer and longer chains of relationships that may link statistically dependent objects. 
%Chains of relationships form a natural lattice structure. 
%%Conceptually, the lattice view simplifies describing and developing learning algorithms. Computationally, 
%The Virtual Join algorithm computes contingency tables by using the results for smaller relationships for larger relationship chains. In terms of the lattice, this corresponds to moving from the bottom to the top. 
%%The lattice diagram facilitates the computation of contingency tables that summarize the sufficient database statistics for learning.
%%
%%\subsection{The Relationship Lattice}
%%
%
%A relationship set is a \textbf{chain} if it can be ordered as a list $[\Relation_{1}(\argterms_{1}),\ldots,\Relation_{k}(\argterms_{k})]$ 
%%is a \textbf{chain} if 
%such that each functor $\Relation_{i+1}(\argterms_{i+1})$ shares at least one population variable with the preceding terms $\Relation_{1}(\argterms_{1}),\ldots,\Relation_{i}(\argterms_{i})$.
%%\footnote{Essentially the same concept is called a slot chain in PRM modelling \cite{Getoor2007c}.}
%%A relationship set forms a chain if the corresponding list is a chain. 
%All sets in the lattice are constrained to form a chain.
%%
%For instance, in the University schema of Figure~\ref{fig:university-schema}, a %relationship 
%chain is formed by the two relationship nodes
%\[\reg(\S,\C),\ra(\P,\S).\]
%%list \[[\it{RA}(\P,\S),\it{Registered}(\S,\C)].\] 
%If relationship node $\it{Teaches}(\P,\C)$ is added,
%%to record which student is a TA for which course,
%we may have a three-element chain \[\reg(\S,\C),\ra(\P,\S),\it{Teaches}(\P,\C).\] 
%The subset relation defines a lattice on relationship sets/chains. 
%Figure~\ref{fig:big-lattice} illustrates the  lattice for the relationship nodes in the university schema. 
%For reasons that we explain below, entity tables are also included in the lattice and linked to relationships that involve the entity in question. 
%
%\begin{figure}[htbp]
%\begin{center}
%\resizebox{0.5\textwidth}{!}{
%\includegraphics[width=0.8\textwidth]{figures/uni-big-lattice.png}
%}
%\caption{A lattice of relationship sets for the university schema of Figure~\ref{fig:university-schema}.
% Links from entity tables to relationship tables correspond to foreign key pointers. 
%%The list representation of the sets is determined by the functor ordering $\it{Registered} < \it{TA} < \it{Teaches}$. 
%\label{fig:big-lattice}}
%\end{center}
%\end{figure}
%
%
%
%
%With each relationship set $\set{\Relation}$ is associated a $\ct$-table $\ct_{\set{\Relation}}$. 
%For a relationship chain $\set{\Relation}$ the nodes in the $\ct$-table  $\ct_{\set{\Relation}}$%$\B_{\set{Relation}}$
% comprise: the Rnodes in the set $\set{\Relation}$, the 1Nodes and 2Nodes associated with each of the Rnodes. To describe these, we introduce the following notation.
%%Let's first introduce the notations for all kinds of functor nodes in order to extend relational algebra operators for manipulating $\qcount$ in contingency tables as follows.
%%Let $R$ be a relationship node and let $\set{R}$ be a set of relationship nodes.
%
%\begin{itemize}
%\item $\eatts(R)$ denotes the set of entity attribute nodes for the population variables involved in the relationship $R$. 
%\item $\eatts(\set{R})$ is the union of the entity attributes for each relationship $R \in \set{R}$.
%\item $\ratts(R)$ denotes the set of relationship attribute nodes for %the population variables involved in 
%the relationship $R$.
%\item $\ratts(\set{R})$ is the union of the relationship attributes for each relationship $R \in \set{R}$.
%\item $\atts(R)$ is the set of both entity and relationship attributes for relationship $R$, similarly for $\atts({\set{R}})$.
%\item  $1Nodes(\A)$ denotes the attributes of a population variable $\A$ collectively
%%\item $R_{i}$ is the Boolean relationship node.
%%\item $\etable$ denotes a entity table.
%%\item $Column\_List({\etable})$ is the set of non-count columns in entity table $\etable$.
%\end{itemize}
%
%In this notation, the nodes in the $\ct$-table  $\ct_{\set{\Relation}}$  are denoted as $\set{\Relation} \cup \atts({\set{R}})$. 
%
%\emph{Implementation.} The Random Variable Database specifies the set of relationship nodes for building the lattice. Building a lattice of relationship chains can be done using any standard technique, such as those developed for enumerating itemsets in association rule  mining \cite{Agrawal1994}. 
%We create a table {\em Lattice} in the Bayes net database with one row for each relationship chain. We also create two auxilliary tables: One that lists which relationship chains are children of which others. A child contains exactly one more relationship node (cf. Figure~\ref{fig:big-lattice}). 
%%For example, the relationship chain $\it{Registered}(\S,\C),\it{RA}(\S,\P)$ is a child of the chain $\it{Registered}(\S,\C)$. Another auxilliary table lists which relationship nodes are members of which relationship chain. For example, the relationship chain $\it{Registered}(\S,\C),\it{RA}(\S,\P)$ has two members. The auxilliary tables are used during learning as described below.
%
%The goal of the Virtual Join Algorithm is to compute a contingency table for each point in the lattice. 
%In the example of Figure~\ref{fig:big-lattice}, the algorithm computes 10 contingency tables. The $\ct$-table for the top element of the lattice is the $\ct$-table for the entire database. 
%It is useful to distinguish two types of rows within each $\ct$-table: rows where all the relationship nodes are assigned the value $\true$---positive relationships only---and rows where one or more relationship nodes are assigned the value $\false$---some negative relationships. 
%We begin with the case of positive relationships.



You can use an appendix for optional proofs or details of your evaluation which are not absolutely necessary to the core understanding of your paper. 

CREATE TABLE KeyColumns AS SELECT * FROM
(Schema_Key_Info
NATURAL JOIN Schema_Position_Info)
WHERE
TABLE_NAME NOT IN (SELECT 
        TABLE_NAME
    FROM
        NoPKeys)
    AND TABLE_NAME NOT IN (SELECT 
        TABLE_NAME
    FROM
        TernaryRelations);


\section{Other Applications}

In this section we outline how the MRLBase framework can be applied to support other machine learning applications. This discussion is mean to illustrate the potential of MRLBase for other machine learning applications beyond Bayesian network learning.

\subsection{Markov Networks} Markov networks or undirected graphical models have been applied extensively to relational data \cite{taskar,domingos}. An  undirected graphical model defines a log-linear equation for classification. Markov Logic Networks (MLNs) define a widely applied logic-based formalism for specifying the random variables in a Markov network \cite{domingos}. An MLN structure is defined as a set of first-order logic formulas. Typically these formulas are conjunctive queries of the type that define relational contingency tables. 

One option for learning a Markov Logic Network structure is to first learn a Bayesian network structure, then transform the BN structure to an MLN using a standard conversion method (moralization) \cite{MLJ}. This means that the MRLBase method we have described for learning Bayesian networks can be directly applied for learning Markov Logic Networks with RDMBS support. For complex relational schemas, the MRLBase approach to learning MLNs is much more scalable than previous MLN learning methods (scales to millions of records rather than hundreds of thousands). The accuracy of the Bayesian network approach is competive if not greater \cite{}.

An MLN structure can be represented in a database table just like a BN structure. A conjunctive formula is a list $\node_{1} = \value_{1},\ldots,\node_{k} = \value_{k}$. A list of formulas can be represented in a table $\MLN\_\it{Structure}$ with four columns $\it{Index},\it{Variable},\it{Value}$, where $\it{Index}$ stores the index $i$ of a formula, $\it{Variable}$ stores the ID of a relational random variable in formula $i$, and $\it{Value}$ stores the value assigned to the variable in formula $i$. Given a database representation of a BN structure, and the variable domain information in the random variable database, it is straightforward to implement moralization to convert the BN structure table to a corresponding MLN structure table. 


\subsection{Classification With Aggregate Functions}

A prominent alternative to log-linear models for relational classification is to use aggregate functions \cite{prms,survey,treeliker}. The general idea is to ``flatten'' or ``propositionalize'' the relational structure by summarizing the information about links and linked entities in terms of aggregate features of the target individuals. These aggregate features can be visualized as new columns added to the entity table representing target individuals. Classification can then be performed using any standard single-table classifier (SVM, logistic regression). In our university example, we may compute the average capability score of a student and his or her average grade in courses. This would generate two new features of students that can be used for predicting the intelligence of a student.  

The search space of potentially useful aggregate features is large. Popescul et al. propose representing each such feature as an SQL query, and using a refinement ordering on SQL queries to structure the feature search. On the Citeseer dataset, their method generates x features requiring y CPU time. To scale to complex datasets, this approach requires support for managing and evaluating large feature sets. As we discussed in Section~\ref{sec:metaquery}, SQL queries can be represented in database tables. Assuming that aggregate features are computed by SQL queries stored in a table, a metaquery could be used to compute a contingency table for the aggregate feature. The contingency table can then be used to compute a predictive accuracy score for the aggregate feature, similar to the model score computation described in Section~\ref{sec:model-score}.

%\subsection{Collective Matrix Factorization}
%- collective matrix factorization, tensor factorization. 
%1M parameters. 
%



% Table generated by Excel2LaTeX from sheet 'count-manager'
\begin{table}[htbp]
  \centering
     \resizebox{0.5\textwidth}{!}
{  \begin{tabular}{|l|r|r|r|r|r|r|}
    \hline
    Scoring Method & Movielens & Mutagenesis & Mondial & UW-CSE & Hepatitis & IMDB \\
    \hline
    Blocked Instances & 54.44 & 61.79 & 109.2 & 80.18 & 693.42 & 5.21 h \\
    \hline
    Instance-by-instance & 5544.01 & 4052.43 & 957.37 & 1064.12 & 23210.52 & >20 h \\
    \hline
    \end{tabular}%
}
  \caption{Test Timing for Model Predictions}
  \label{tab:addlabel}%
\end{table}%


