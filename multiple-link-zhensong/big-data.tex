\documentclass{article}
% The file ijcai13.sty is the style file for IJCAI-13 (same as ijcai07.sty).
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{ijcai13}

\input{preamble-stuff}
\newcommand{\ct}{\mathit{ct}}



\begin{document}
\title{Learning Bayes Nets with Link Uncertainty for Relational Data Sets}
\author{Oliver Schulte and Zhensong Qian\\
\\ School of Computing Science\\ Simon Fraser University\\Vancouver-Burnaby, Canada}
\date{\today}
\maketitle

\begin{abstract} Many if not most big data sets are maintained in relational databases. Bayes net learning has the potential to discover knowledge about correlations among link types and node attributes in big relational data. The current state of the art algorithm for learning relational Bayes nets captures only correlations among entity attributes {\em given} the existence of links among entities. The models described in this paper capture a wider class of correlations that involve uncertainty about the link structure. A key challenge for relational learning that scales with data size is to compute event counts in a relational database (sufficient statistics), especially when these involve negated relationships. We describe how the fast M\"obius transform provides a scalable solution for this problem.
%We provide a lattice-based implementation of the fast Mšbius transform that efficiently computes sufficient statistics across multiple tables. The lattice M¬\"obius transform leads to efficient multi-relational Bayes net structure learning algorithms.
\end{abstract}


\section{Introduction} Many if not most big data sets are maintained in relational databases. Such datasets represent complex heterogenous networks \cite{Sun2012}. Scalable link analysis for heterogenous networks with multiple link types is a challenging problem in network science. We describe a method for learning a Bayes net that captures simultaneously correlations between link types, link features, and attributes of nodes. 

\paragraph{Motivation} 


Building a Bayes net model is useful for big data analysis because such models provide a compact summary of the statistical relationships in the data. The model supports both descriptive and predictive analytics. Correlations are presented to the user in a graphical way, and queries about probabilistic relationships can be answered quickly using Bayes net inference rather than via database queries run against a large dataset. Applications supported by Bayes net learning include the following:

\begin{description}
\item[Knowledge Discovery] Dependencies provide valuable insights. For instance, a web search manager may wish to know whether if user searches for a video in Youtube for a product, they are also likely to search for it on the web. 
\item[Probabilistic queries] Once the model has been built, well-researched Bayes net methods can be used for probabilistic inference and ``what-if'' queries. For instance, a university administrator what wish to know how raising the average SAT test score of applicants would affect the attrition rate of students.
\item[Database Query Optimization] The Bayes net model can also be used to estimate relational statistics, the frequency with which statistical patterns occur in the database \cite{Schulte2012b}. This kind of statistical model can be applied for database query optimization \cite{Getoor2001}.
\end{description}

Previous work on learning Bayes nets for relational data was restricted to correlations among attributes given the existence of links \cite{Schulte2012}. The larger class of correlations examined in our new algorithms includes two additional kinds:

\begin{enumerate}
\item Dependencies between two different types of links.
\item Dependencies among node attributes given the {\em absence} of a link between the node.
\end{enumerate}


\paragraph{Approach}

We consider three approaches to multiple link analysis with Bayes nets. 

\begin{description}
\item[Flat Search] Apply a standard Bayes net learner to a single large join table. This table is formed as follows: (1) take the cross product of entity tables. (An entity tables lists the set of nodes of a given type.) (2) For each tuple of entities, add a relationship indicator whose value ``true'' or ``false'' indicates whether a certain relationship holds among the entities. 
\item[Hierarchical Search] Conducts bottom-up  search
 through the lattice of table joins hierarchically. Dependencies (Bayes net edges) discovered on smaller joins are propagated to larger joins. 
The different table joins include information about the presence of absence of relationships as in the flat search above. 
This is an extension of the current state of the art Bayes net learning algorithm for relational data \cite{Schulte2012}.
\end{description}

%(1) Baseline: The learn-and-join algorithm is the state of the art method for learning Bayes nets that capture correlations among attributes of entities or nodes. It conducts a hierarchical search
% through the lattice of table joins. Dependencies discovered on smaller joins are propagated to larger joins.  The current version of the learn-and-join method considers only correlations between attributes and link types, not correlations between link types. (2) {\em Hierarchical Search With Link Types.} We extend the learn-and-join algorithm to consider correlations among link types. This is done by adding a new feature for each relationship table that indicates for each tuples of entities, whether they are related or not. 
%(3) {\em Flat search}: Form a single big join table that combines different relationships with the new relationship indicator feature. Then apply a standard Bayes net learner on the join table. 

\paragraph{Evaluation.} We compare the learned models using standard scores (e.g., Bayes Information Criterion, log-likelihood). 
These results indicate that both flat search and hierarchical search are effective at finding correlations among link types. 
Flat search can on some datasets achieve a higher score by exploiting attribute correlations that depend on the absence of relationships. 
Structure learning time results indicate that hierarchical search is substantially more scaleable.

\paragraph{Contributions} 

\begin{enumerate}
\item To our knowledge this is the first application of Bayes net learning to modelling correlations among different types of links.
\item Extension of a lattice search strategy for link type modelling, with a comparison to a flat search join approach.
\item Using the lattice M\"obius transform to make the computation of the empirical frequencies of the relations imposed by negated relational links tractable. 
\end{enumerate}

\paragraph{Paper Organization} We describe Bayes net models for relational data (Poole's Parametrized Bayes Nets). 
Then we present the learning algorithms, first flat search then hierarchical search. 
We compare different model search strategies on four databases from different domains. The last part of this paper turns from statistical to computation issues. We propose the M\"obius transform to efficiently compute sufficient database statistics involving any number of negated relationships.

\section{Related Work} 
To our knowledge, there are no implementations of structure learning algorithms for directed graphical models that consider correlations among different link types, 
let alone together with node attributes. 
Such implementations exist, however, for other types of graphical models, specifically Markov random fields (undirected models) \cite{Domingos2009} and dependency networks (directed edges with cycles allowed) \cite{Natarajan2012}. 
Structure learning programs for Markov random fields are provided by Alchemy \cite{Domingos2009} and Khot et al \cite{Khot2013}. Khot et al. use boosting to provide a state-of-the-art dependency network learner. 
None of these programs are able to return a result on half of our datasets because they are too large. For space reasons we restrict the scope of this paper to directed graphical models and do not go further into undirected model. For an extensive comparison of the learn-and-join Bayes net learning algorithm with Alchemy please see \cite{Schulte2012}.

\section{Background and Notation} 
Poole introduced the Parametrized Bayes net (PBN) formalism that combines Bayes nets with logical syntax for expressing relational concepts \cite{Poole2003}. 
We adopt the PBN formalism, following Poole's presentation.


\subsection{Bayes Nets for Relational Data}
A \textbf{population} is a set of individuals. Individuals are denoted by lower case expressions (e.g., $\it{bob}$). A \textbf{population variable} is capitalized. A \textbf{functor} represents a mapping
$
\functor: \population_{1},\ldots,\population_{a} \rightarrow \outdomain_{\functor}
$
where $\functor$ is the name of the functor,  and $\outdomain_{\functor}$ is the output type or \textbf{range} of the functor. In this paper we consider only functors with a finite range, disjoint from all populations.  If $\outdomain_{\functor} = \{\true,\false\}$, the functor $\functor$ is a (Boolean) \textbf{predicate}. A predicate with more than one argument is called a \textbf{relationship}; other functors are called \textbf{attributes}. We use uppercase for predicates and lowercase for other functors.

A {\bf Bayes Net (BN)} is a directed acyclic graph (DAG) whose nodes comprise a set of random variables and conditional probability parameters.
For each assignment of values to the nodes, the joint probability 
is specified by the product of the conditional probabilities, $P(\it{child}|\it{parent\_values}$). 
A \textbf{Parametrized random variable} is of the form $\functor(\X_{1},\ldots,\X_{a})$, where the populations associated with the variables are of the appropriate type for the functor. A \textbf{Parametrized Bayes Net} (PBN) is a Bayes net whose nodes are Parametrized random variables \cite{Poole2003}. If a Parametrized random variable appears in a Bayes net, we often refer to it simply as a node. 

\subsection{Databases and Table Joins}
 
 We begin with a standard \textbf{relational schema} containing a set of tables, each with key fields, %typically
descriptive attributes, and possibly foreign key pointers. A \textbf{database instance} specifies the tuples contained in the tables of a given database schema.
We assume that tables in the relational schema can be divided into {\em entity tables} and {\em relationship tables.} This is the case whenever a relational schema is derived from an entity-relationship model (ER model) \cite[Ch.2.2]{Ullman1982}. 
In our university example, there are two entity tables: a $\student$ table and a $\course$ table.  There is one relationship table $\reg$ with foreign key pointers to the $\student$ and $\course$ tables whose tuples indicate which students have registered in which courses. 

The functor formalism is rich enough to represent the constraints of an ER schema by the following translation: Entity sets correspond to types, descriptive attributes to functions, relationship tables to predicates, and foreign key constraints to type constraints on the arguments of relationship predicates.  
 
Table \ref{table:university-schema} shows a relational schema for a database related to a university.
 %(cf.\cite{prms}). 
 Figure \ref{fig:university-tables} displays a small database instance for this schema together with a Parametrized Bayes Net (omitting the $\it{Teaches}$ relationship for simplicity.) 
%To keep the schema simple, we introduce only a limited number of attributes for each entity class. 
\begin{table}[tbp] \centering
\begin{tabular}
[c]{|l|}\hline
$\student$(\underline{$student\_id$}, $\intelligence$, $ranking$)\\
$\course$(\underline{$course\_id$}, $\diff$, $rating$)\\ 
$\prof$ (\underline{$professor\_id$}, $teaching\_ability$, $popularity$)\\
$\reg$ (\underline{$student\_id$, $\course\_id$}, $grade$, $satisfaction$)\\
%$\ra$ (\underline{$student\_id$, $prof\_id$}, $salary$, $capability$)
$\it{Teaches}(\underline{\it{professor\_id, course\_id}})$
\\
\hline
\end{tabular}
\caption{A relational schema for a university domain. Key fields are underlined. An instance for this schema is given in Figure \ref{fig:university-tables}.
\label{table:university-schema}} 
\end{table}
 
\begin{figure*}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=7in]{figures/university-tables3.png} 
  \caption{Database Table Instances: (a) $\student$, (b) $\reg$ (c) $\course$. To simplify, we added the information about professors to the courses that they teach.  (d) The attribute-relation table $\reg^{+}$ derived from $\reg$, which lists for each pair of entities their descriptive attributes, whether they are linked by $\reg$, and the relationships of a link if it exists. (e) A Parametrized Bayes Net for the university schema.}
   \label{fig:university-tables}
\end{figure*}


 The \textbf{natural table join}, or simply join, of two or more tables contains the rows in the Cartesian products of the tables whose values match on common fields. In logical terms, a join corresponds to a conjunction \cite{Ullman1982}. 
 
 
% For a relationship table $R$, we define the \textbf{full relationship table} $R^{+}$ as follows. Suppose that $R$ is a relationship between the two entity types (populations) $X$ and $Y$. The rows of $R^{+}$ correspond to pairs $(x,y)\in (X \times Y)$, that is, all members of the Cartesian product. Also, $R^{+}$ contains an indicator column $R_{\it{ind}}$ such that a row for $(x,y)$ contains the value $\true$ if and only if the original relationship table contains the pair $(x,y)$. Also, $R^{+}$ contains one column for each descriptive attribute of the relationship $R$. For pairs $(x,y)$ that are not linked by $R$, each attribute column contains the value $\bot$ for ``don't care''. Join $R^{+}$ with the entity tables related by $R$ has the effect of extending $R^{+}$ with the entity information. We refer to this join table as the \textbf{attribute-relation table}.



%
% \begin{figure}[h]
%\begin{center}
%\resizebox{0.5\textwidth}{!}{
%\includegraphics[width=1\textwidth]{figures/database.png}
%%includegraphics[width=1\textwidth]{database.png}
%}
%\caption{Left: A simple relational database instance. Right: The ground atoms for the database, and their values as specified by the database, using functor notation.
%%that are true in the database. 
%\label{fig:db-tables}}
%\end{center}
%\end{figure}


%We assume that a database instance (interpretation) assigns a constant value to each gnode $\f(\set{a})$, which we denote by $
%[f(\set{a})]_{\D}$.
%%Thus a DB instance defines a truth value for each ground atom depending on whether the atom assigns the right function value to the ground functor term.
%The value of descriptive relationship attributes is well defined only for tuples that are linked by the relationship. For example, the value of $\it{grade}(\it{jack},\it{101})$ is not well defined in a university database if $\it{Registered}(\it{jack},\it{101})$  is false. In this case, we follow the approach of Schulte {\em et al.} \cite{Schulte2009c} and assign the descriptive attribute the special value $\bot$ for ``undefined''. Thus the atom $\it{grade}(\it{jack},\it{101}) = \bot$ is equivalent to the atom $\it{Registered}(\it{jack},\it{101}) = \false$. Fierens {\em et al.} \cite{Fierens2005} discuss other approaches to this issue. 

\section{Bayes Net Learning With Link Correlation Analysis}

We outline the two methods we compare in this paper, flat search and hierarchical search. 

\subsection{Flat Search}
The basic idea for flat search is to apply a standard single-table Bayes net learner to a single large join table. 
To learn correlations between link types, we need to provide the Bayes net with data about when links are present {\em and} when they are absent. To accomplish this, we add to each relationship table a \textbf{link indicator column}. This columns contains 1 if the link is present between two entities (specified in the key fields), and 0 if the link is absent. We add rows for all pairs of entities of the right type for the link, and enter 0 or 1 in the link indicator column depending on whether a link exists or not. We refer to relationship tables with a link indicator column as \textbf{extended} tables. Extended tables are readily computed using SQL queries. If we omit the entity Ids from an extended table, we obtain the \textbf{attribute-relation} table that lists all attributes for the entities involved and whether a relationship exists. If the attribute-relation table is derived from a relationship $R$, we refer to it as $R^{+}$. 

The attribute-relation table is readily defined for a set of relationships: take the cross-product of all populations involved, and add a link indicator column for each relationship in the set. For instance, if we wanted to examine correlations that involve both the $\reg$ and the $\it{Teaches}$ relationships, we would form the cross-product of the entity types $\it{Student},\it{Course},\it{Professor}$ and build an attribute-relation table that contains two link indicator columns $\reg(\S,\C)$ and $\it{Teaches}(\P,\C)$. The \textbf{full join} is the attribute-relation table for all relationships in the database.
 
The \textbf{flat search Bayes net learner} takes a standard Bayes net learner and applies it to the full join table to obtain a single Parametrized Bayes net. The results of \cite{Schulte2011} can be used to provide a theoretical justification for this procedure; we outline two key points. (1) The full join table correctly represents the {\em sufficient statistics} of the database: using the full join table to compute the frequency of a joint value assignment for Parametrized Random Variables is equivalent to the frequency with which this assignment holds in the database. (2) Maximizing a standard single-table likelihood score from the full join table is equivalent to maximizing the {\em random selection pseudo likelihood.} The random selection pseudo log-likelihood is the expected log-likelihood assigned by a Parametrized Bayes net when we randomly select individuals from each population and instantiate the Bayes net with attribute values and relationships associated with the selected individuals. 

\subsection{Hierarchical Search}
Khosravi {\em et al.} present the learn-and-join  structure learning algorithm. 
The algorithm upgrades a single-table Bayes net learner for relational learning. 
We describe the fundamental ideas of the algorithm; for further details. 
%The key idea of the algorithm can be explained in terms of the {\em table join lattice.} 
%Recall that the (natural) join of two or more tables
%%, written $\dtable_{1} \Join \dtable_{2} \cdots \Join \dtable_{k}$ 
%is a new table that contains the rows in the Cartesian products of the tables whose values match on common fields. 
The key idea is to build a Bayes net for the entire database by level-wise search through the {\em table join lattice.} The user chooses a single-table Bayes net learner. The learner is applied to table joins of size 1, that is, regular data tables. Then the learner is applied to table joins of size $s,s+1,\ldots$, with the constraint that the absence or presence of learned edges from smaller join tables is propagated to larger join tables. These constraints are implemented by keeping a global cache of forbidden and required edges.  Algorithm~\ref{alg:structure} provides pseudocode for the previous learn-and-join algorithm (LAJ) \cite{Schulte2012c}. 


\begin{figure}[h]
\begin{center}
\resizebox{0.5\textwidth}{!}{
\includegraphics[width=0.8\textwidth]{figures/big-lattice}
}
\caption{A lattice of relationship sets for the University schema of Table~\ref{table:university-schema}. Links from entity tables to relationship tables correspond to foreign key pointers. 
%The list representation of the sets is determined by the functor ordering $\it{Registered} < \it{TA} < \it{Teaches}$. 
\label{fig:big-lattice}}
\end{center}
\end{figure}



To extend the learn-and-join algorithm for link analysis, we replace the natural join in line 7 by the extended join (more precisely, by the attribute-relation tables derived from the extended join). The natural join contains only tuples that appear in all relationship tables. Compared to the extended join, this corresponds to considering only rows where the link indicator columns have the value $\true$. When the propositional Bayes net learner is applied to such a table, the link indicator variable appears like a constant. Therefore the BN learner cannot find any correlations between the link indicator variable and other nodes, nor can it find correlations among attributes conditional on the link indicator variable being $\false$. Thus the previous LAJ algorithm finds only correlations between entity attributes conditional on the existence of a relationship. In sum, hierarchical search with link correlations can be described as follows.

\begin{enumerate}
\item Run the previous LAJ algorithm (Algorithm~\ref{alg:structure}) using natural joins.
\item Starting with the constraints from step 1, extend them with the LAJ algorithm where extended joins replace natural joins. That is, for each relationship set shown in the lattice of Figure~\ref{fig:big-lattice}, apply the single-table Bayes net learner to the extended join for the relationship set.
\end{enumerate}


\begin{algorithm}[htb]
\begin{algorithmic}
{\footnotesize
\STATE {\em Input}: Database $\D$ with $E_1,..E_e$ entity tables, $R_1,... R_r$ Relationship tables, %ER Model ,
\STATE {\em Output}: Bayes Net for $\D$ 
\STATE {\em Calls}: PBN: Any propositional Bayes net learner that accepts edge constraints and a single table of cases as input. 
\STATE {\em Notation}: PBN$(\T,\mbox{Econstraints})$ denotes the output DAG of PBN. Get-Constraints$(\G)$ specifies a new set of edge constraints, namely that all edges in $\G$ are required, and edges missing between variables in $\G$ are forbidden.
} %fnsize
\end{algorithmic}
\begin{algorithmic}[1]
{\footnotesize
	\STATE Add descriptive attributes of all entity and relationship tables as variables to  $G$. Add a boolean indicator for each relationship table to $G$.
	\STATE Econstraints = $\emptyset$ {[Required and Forbidden edges]} %in the G]}
\FOR {m=1 to e}
	\STATE Econstraints += Get-Constraints(PBN($E_m$ , $\emptyset$)) 
	\ENDFOR	
%\FOR {m=1 to r}
%	\STATE Econstraints += Get-Constraints(PBN($R_m$, Econstraints))
%\ENDFOR
\FOR {m=1 to r}
	\STATE $N_m$ :=  natural 
	join of $R_m$ and entity tables linked to $R_m$ 
	\STATE Econstraints += Get-Constraints(PBN($N_m$, Econstraints))
\ENDFOR
\FORALL{$N_i$ and $N_j$ with a foreign key in common}
	\STATE $K_{ij}$ :=  %natural 
	join of $N_i$ and $N_j$ 
	\STATE Econstraints += Get-Constraints(PBN($K_{ij}$, Econstraints))
\ENDFOR
\RETURN Bayes Net defined by Econstraints.
%\FORALL{possible combination of values of a node and its parents} 
%\STATE Add a clause with predicates to MLN input file
%\ENDFOR
%\STATE Run WL on the MLN file
%\FORALL [A relationship node is a parent of a Entity node]{dependencies of kind $X(R_m) \rightarrow Y(E_i)$}
%	%\IF {dependency is of kind $X(R_m) \rightarrow Y(E_i)$ } 
%	\STATE add $R_m \rightarrow Y(E_i)$ to $G$
%	\ENDFOR	
		%\STATE Run dynamic programing algorithm
		} %footnotesize
\end{algorithmic}
%\label{alg:cpt}
\caption{Pseudocode for previous Learn-and-Join Structure Learning for Lattice Search. \label{alg:structure}}
\end{algorithm}

\section{Evaluation} 
All experiments were done on a QUAD CPU Q6700 with a 2.66GHz CPU and 8GB of RAM. Our code and datasets are available on the world-wide web \cite{bib:jbnsite}. We made use of the following single table Bayes Net search implementation:  GES search \cite{Chickering2003} with the BDeu score as implemented in version 4.3.9-0 of CMU's Tetrad package (structure prior uniform, ESS=10; \cite{2008a}).

\paragraph{Methods Compared}

We compared the following methods.

\begin{description}
\item[LAJ] The previous LAJ method without link correlations (Algorithm~\ref{alg:structure}).
\item[LAJ+] The new LAJ method that has the potential to find link correlations (Algorithm~\ref{alg:structure} with the full join instead of natural join).
\item[Flat] Applies the single-table Bayes net learner to the full table join (extended join with all relationship sets in the database).
\end{description}

\paragraph{Performance Metrics} We report learning time, log-likelihood, Bayes Information Criterion (BIC), and the Akaike Information Criterion (AIC). We write 
$$L(\hat{G},\d)$$ for the log-likelihood score, where
where $\hat{G}$ is the BN $\G$ with its parameters instantiated to be the maximum likelihood estimates given the dataset $\d$, and the quantity $L(\hat{G},\d)$ is the log-likelihood of $\hat{G}$ on $\d$. 

The BIC score is defined as follows \cite{Chickering2003,Schulte2011}

$$\mathit{BIC}(\G,\d) = L(\hat{G},\d) - \mathit{par}(\G)/2 \times ln(m)$$

where the data table size is denoted by $m$, and $\mathit{par}(\G)$ is the number of free parameters in the structure $\G$. The AIC score is given by 

$$\mathit{AIC}(\G,\d) = L(\hat{G},\d) - \mathit{par}(\G). $$

BIC and AIC are standard scores for Bayes nets \cite{Chickering2003}. AIC is asympotically equivalent to selection by cross-validation, so we may view it as a closed-form approximation to cross-validation,  which is computationally demanding for relational datasets. 

\begin{table}[btp] \centering
%\scalebox{0.7in}{
%\resizebox{0.5\textwidth}{!}{
\begin{tabular}[c]
{|l|l|l|}\hline
    \textbf{Dataset} & \textbf{\#tuples} \\\hline
    University&662\\\hline
    Movielens &1585385\\\hline
    Mutagenesis &1815488\\\hline
    Hepatitis &2965919\\\hline
    %Mondial &59520\\
    %UW-CSE &2099\\
    Small-Hepatitis & 19827 \\\hline
\end{tabular}
%}
 % end scalebox
\caption{Size of datasets in total number of table tuples and ground atoms. Each descriptive attribute is represented as a separate function, so the number of ground atoms is larger than that of tuples.
% \textbf{Zhensong: needs fixing}
 \label{table:datasetsize}}
\end{table}


%\begin{table}[btp] \centering
%%\scalebox{0.7in}{
%\resizebox{0.5\textwidth}{!}{
%\begin{tabular}[c]
%{|l|l|l|}\hline
% \textbf{Dataset} & \textbf{\#tuples} & \textbf{\#Ground atoms} \\\hline
%University&662&513\\\hline
%Movielens &1585385&170143\\\hline
%Mutagenesis &1815488& 35973 \\\hline
%Hepatitis &2965919&71597 \\\hline
%%Mondial &59520&3366 \\ \hline
%%UW-CSE &2099&3380 \\ \hline
%Small-Hepatitis & 19827 & (not strictly necessary)\\
%\hline
%\end{tabular}
%}
% % end scalebox
%\caption{Size of datasets in total number of table tuples and ground atoms. Each descriptive attribute is represented as a separate function, so the number of ground atoms is larger than that of tuples.
%% \textbf{Zhensong: needs fixing}
% \label{table:datasetsize}}
%\end{table}

\paragraph{Datasets}


We used one synthetic and 
three benchmark real-world databases, with the modifications described by Schulte and Khosravi~\cite{Schulte2012}. See that article for more
details.

% \noindent\textbf{Mondial Database.} A geography database, featuring
% one self-relationship, $\it{Borders}$, that indicates which countries border each other. The data are organized in 4 tables (2 entity tables, 2 relationship tables, 10 descriptive attributes).
\noindent\textbf{University Database.} We manually created a small dataset, based on the schema given in Table~\ref{table:university-schema}. 
The dataset is small and is used as a tested for the correctness of our algorithms.

\noindent\textbf{MovieLens Database.} A dataset from the UC Irvine machine learning repository. The data are organized in 3 tables (2 entity tables, 1 relationship table, and 7 descriptive attributes). 

\noindent\textbf{Mutagenesis Database.} A dataset widely used in ILP research. % \cite{Srinivasan1996}.  
It contains two entity tables and two relationships.

\noindent\textbf{Hepatitis Database.} A modified version of the PKDD'02 Discovery Challenge database. The data are organized in 7 tables (4 entity tables, 3 relationship tables and 16 descriptive attributes). In order to make the learning feasible, we under sampled Hepatitis database to keep the ratio of positive and negative link indicator equal to one. %, following %we adopted the modifications of 
%Frank {\em et al.} \citeyearpar{Frank2007}. %, which includes removing tests with null values. 

%\noindent\textbf{Financial} A dataset from the PKDD 1999 cup. The data are organized in 4 tables (2 entity tables, 2 relationship tables, 13 descriptive attributes).



\begin{table} \centering
%\scalebox{0.7in}{
%\resizebox{0.5\textwidth}{!}{
\begin{tabular}[c]
{|l|l|l|l|}\hline
 \textbf{Dataset} & \textbf{Flat} & \textbf{LAJ+} & \textbf{LAJ}\\\hline
University&1.916&1.183&0.291 \\\hline
Movielens &38.767& 18.204& 1.769\\\hline
Mutagenesis &3.231& 3.448& 0.982\\\hline
Small-Hepatitis &9429.884&8.949&10.617 \\\hline
%Mondial &59520&3366 \\ \hline
%UW-CSE &2099&3380 \\ \hline
\end{tabular}
%}
 % end scalebox
\caption{Model Structure Learning Time  in seconds.
% \textbf{Zhensong: needs fixing}
 \label{table:runtimes}}
\end{table}

%
%\subsection{Hypotheses} [consider hypotheses as formulated in the introduction [guesses at results]]
%
%Our results investigate the following issues.
%
%\begin{enumerate}
%\item Which methods provide the fastest model selection? We expect that because propagating results  along the table join lattice constraints the model search, both types of hierarchical search are faster than flat search.
%\item Which methods provide the best data fit? We expect that the models with link type analysis are statistically more powerful than the attribute-only analysis.
%\end{enumerate}

%
%[also consider copying, e.g. from Journal/laj]
%
%We used one synthetic and 5 benchmark real-world databases.The databases and their main characteristics are as follows. For more details please see the references in \cite{Schulte2012} and on-line sources such as \cite{bib:jbnsite}.
%
%{\em University Database.} We manually created a small dataset, based on the schema given in Table~\ref{table:university-schema}. 
%The dataset is small and is used as a purpose of proofing the correctness of our algorithms. 
%The entity tables contain 38 students, 10 courses, and 6  Professors. The $\reg$ table has 92 rows and the $\it{RA}$ table has 25 rows. %This dataset is translated into 513 ground atoms. 
%
%We assume that tables in the relational schema can be divided into {\em entity tables} and {\em relationship tables.} This is the case whenever a relational schema is derived from an entity-relationship model (ER model) \cite[Ch.2.2]{Ullman1982}. The symbol $\E$ and related symbols like $\E_1,\E_i,\E'$ refer to entity tables, and the symbol $\R$ and related symbols like $\R_1,\R_i,\R'$ refer to relationship table. We use a running example, a small database related to a university,  through the paper to further clarify the introduced concept. The university database has three entity tables:  $\student$ table,  $\course$ table, and $\prof$ table.  There are two relationship tables: $\reg$ with foreign key pointers to the $\student$ and $\course$ tables whose tuples indicate which students have registered in which courses and $\ra$ with foreign key pointers to the $\student$ and $\prof$ tables whose tuples indicate the RAship of students for professors.
%
%
%{\em MovieLens Database.} The second dataset is the MovieLens dataset from the UC Irvine machine learning repository. %The schema for the dataset is shown in Table \ref{}. 
%It contains two entity tables: $\it{User}$ with 941 tuples and $\it{Item}$ with 1,682 tuples, and one relationship table $\it{Rated}$ with 80,000 ratings. The $\it{User}$ table has %key field $\it{user\_id}$ and 
%3 descriptive attributes $\age, \it{gender}, \it{occupation}$. We discretized the attribute age into three bins with equal frequency. The table $\it{Item}$ represents information about the movies. It has 17 Boolean attributes that indicate the genres of a given movie. We performed a preliminary data analysis and omitted genres that have only weak correlations with the rating or user attributes, leaving a total of three genres.
%
%% The full dataset contains 170143 ground atoms and is too big for MLN to do structure learning or parameter learning on. We made small subsamples to make the experiments feasible. Sub sampling 100 Users and 100 Items transforms to a db file with 2505 number of groundings. takes around 30 min to run. Sub sampling 300 Users and 300 Items transforms to a db file with 18040 number of groundings takes around 2 days to run. 
%%The full table with 100,000 ratings exceeded the memory limits of Tetrad, so we randomly picked 40\% of the ratings of the relationship table as input data. 
%
%{\em Mutagenesis Database.} This dataset is widely used in ILP research \cite{Srinivasan1996}. %It contains 4 tables total to 15218 tuples. 
%Mutagenesis has two entity tables, $\it{Atom}$ with 3 descriptive attributes, and $\it{Mole}$, with %188 entries and 
%5 descriptive attributes, including two attributes that are discretized into ten values each (logp and lumo). It features two relationships $\it{MoleAtom}$ indicating which atoms are parts of which molecules, and $\it{Bond}$ which relates two atoms and has 1 descriptive attribute. %The full dataset, with 35973 ground atoms, crashed while doing either parameter learning or structure learning. A subsample with 5017 ground atoms is used was running for 5 days and did not terminate. weight learning was feasible. another subsample with 
%Representing a relationship between entities from the same table in a parametrized BN requires using two or more variables associated with the same population (e.g., $\it{Bond}(\A_{1},\A_{2}))$.  
%
%{\em Hepatitis Database.} This data is a modified version of the PKDDï¿½02 Discovery Challenge database, we adopted the modifications of Frank {\em et al.} \cite{Frank2007}, which includes removing tests with null values. It contains data on the laboratory examinations
%of hepatitis B and C infected patients. The examinations were realized between 1982 and 2001 on 771 patients. The data are organized in 7 tables (4 entity tables,  3 relationship tables and 16 descriptive attributes). They contain basic information about the patients, results of biopsy, information on interferon therapy, results of out-hospital examinations, results of in-hospital examinations. 
%%The data were prepared in cooperation with the Shimane Medical University, School of Medicine and Chiba University Hospital, Japan.
%
%\emph{UW-CSE database.} This dataset lists facts about the Department of Computer Science and Engineering at the University of Washington (UW-CSE), such as entities (e.g., Student, Professor) and their relationships (i.e. AdvisedBy, Publication)\cite{Domingos2007}. 
%%The total number of ground atoms is 4,106,841. The database contained a total of 3380 ground atoms. 
%The dataset was obtained  by crawling pages in the department's Web site (www.cs.washington.edu). Publications and AuthorOf relations were extracted from the BibServ database (www.bibserv.org). 
%
%{\em Mondial Database.} 
%%
%%\textbf{Hassan: which version did you use? The full one from http://www.dbis.informatik.uni-goettingen.de/Mondial/mondial-ER.pdf or Bahareh's?} 
%%
%This dataset contains data from multiple geographical web data sources. We followed the modification of \cite{wangMondial}.
%Our dataset contains 4 entity tables, $\it{Country},\it{Continent},\it{Economy},\it{Government}$, where the latter three are related to Country by many-one relationships, and one relationship table $\it{Borders}$ that relates two countries.
%

%
%\begin{enumerate}
%\item  
%The $X^{2}$ score of a model, which is the ratio of the model's log-likelihood over the log-likelihood of a null hypothesis model. 
%For Bayes nets, the null hypothesis model is the disconnected graph.
%
%Recall that the $X^{2}$ is calculated simply as 
%\[
%\mathit{X^{2}}\equiv \sum \frac{(O_{i}-E_{i})}{E_{i}}
%\]
%where $O_{i}$ and $E_{i}$ are the observered and expected log-likelihood, respectively. 
%Here the observered model stands for the disconnected graph and expected model is learned from data.
%The larger $X^{2}$ has a higher probability lies in the right tail with a given significant level. 
%\item 
%The Bayes Information Criterion (BIC), which is defined as follows. 
%Compared to the $X^{2}$ score, the BIC adds a penalty term for parameters to the model likelihood.
% A single-table model selection score has the form $\score(\G,\datatable)$ where $\G$ is a graphical model and $\datatable$ a data table. 
% We consider scores that trade off data fit against model complexity, and that can be computed given the following quantities.
% We also assume that the score is {\em decomposable}, i.e. can be written as the sum of local scores for each node in the BN.


\subsection{Results} 
% \textbf{Zhensong: let's just use flat search with the schema edges}

\paragraph{Learning Times} Table~\ref{table:runtimes}
 provides the model search time for each of the link analysis methods. 
This does not include the time for computing table joins since this is essentially the same for all methods (the cost of the full table join). 
On the smaller and simpler datasets, all search strategies are fast, 
but on the medium-size and more complex datasets (Hepatitis, MovieLens), hierarchical search is much faster due to its use of constraints.
Adding prior knowledge as constraints could speed the structure learning substantially.

%The reason for the speed-up is that LAJ+ starts with the previous LAJ method as the first phase. The edges among attributes that are discovered in the first phase are treated as fixed background knowledge in the second phase. 

\begin{table}[h]
\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|c|c|c|c|c| }
		\hline \textbf{University} &{BIC}& {AIC} &{log-likelihood} &{\# Parameter}\\
			\hline Flat &-17638.27&-12496.72 & -10702.72& 1767\\
			\hline LAJ+ & -13495.34& -11540.75& -10858.75& 655\\
		        \hline LAJ &-13043.17 & -11469.75&-10920.75 & 522\\
			
			\hline
		\end{tabular}
}
\end{center}

\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|c|c|c|c|c| }
		\hline \textbf{MovieLens} &{BIC}& {AIC} &{log-likelihood} &{\# Parameter}\\
			\hline Flat &-4912286.87&-4911176.01 & -4910995.01& 169\\
			\hline LAJ+ & -4911339.74& -4910320.94& -4910154.94& 154\\
		        \hline LAJ &-4911339.74 & -4910320.94&-4910154.94 & 154\\
			
			\hline
		\end{tabular}
}
\end{center}

\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|c|c|c|c|c| }
		\hline \textbf{Mutagenesis} &{BIC}& {AIC} &{log-likelihood} &{\# Parameter}\\
			\hline Flat &-21844.67&-17481.03 & -16155.03& 1289\\
			\hline LAJ+ & -47185.43& -28480.33& -22796.33& 5647\\
		        \hline LAJ & -30534.26 & -25890.89&-24479.89 & 1374\\
			
			\hline
		\end{tabular}
}
\end{center}

\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|c|c|c|c|c| }
		\hline \textbf{Hepatitis} &{BIC}& {AIC} &{log-likelihood} &{\# Parameter}\\
			\hline Flat &-7334391.72&-1667015.81 & -301600.81& 1365357\\
			\hline LAJ+ & -457594.18& -447740.51& -445366.51& 2316\\
		        \hline LAJ &-461802.76& -452306.05&-450018.05 	 & 2230\\ % updated on April 28th
			
			\hline
		\end{tabular}
}
\end{center}


\caption{Performance of different Searching Algorithms by dataset.  }
\label{table:result_scores}
\end{table}
\paragraph{Statistical Scores}

As expected, adding edges between link nodes improves the statistical data fit: 
the link analysis methods LAJ+ and Flat perform better than the learn-and-join baseline in terms of log-likelihood on all datasets shown in table~\ref{table:result_scores}. On the small synthetic dataset University, flat search appears to overfit whereas the hierarchical search methods are very close. On the medium-sized dataset MovieLens, which has a simple structure, all three methods score similarly. Hierarchical search finds no new edges involving the single link indicator node (i.e., LAJ and LAJ+ return the same model). 

The most complex dataset, Hepatitis, is a challenge for flat search, which seems to overfit severely with a huge number of parameters that result in a model selection score that is an order of magnitude worse than for hierarchical search. Because of the complex structure of the Hepatitis schema, the hierarchical constraints appear to be effective in combating overfitting.

The situation is reversed on the Mutagenesis dataset where flat search does well: compared to attribute-only search, it manages to fit the data better with a less complex model. Hiearchical search performs very poorly compared to flat search 
(lower likelihood yet many more parameters in the model). 
Investigation of the models shows that the reason for this phenomenon is a special property of the Mutagenesis dataset: 
whereas generally relationships are sparse---very few pairs of entities are actually linked---in Mutagenesis most entities whose type allows a link are linked. 
As a result, we find strong correlations between attributes conditional on {\em the absence of relationships}. 
The LAJ+ algorithm is constrained so that it cannot add Bayes net edges between attribute nodes at its second stage, when absent relationships are considered. 
As a result, it can represent attribute correlations conditional on the absence of relationships only indirectly through edges that involve link indicators. 
A solution to this problem would be to add a phase to  the search so that we first learn edges between attributes first conditional on both the existence of relationships, 
then conditional on their nonexistence. The last phase then would consider edges that involve relationship nodes. We expect that with this change, hierarchical search would be competitive with flat search on the Mutagenesis dataset as well.

%
%Between the link analysis methods, flat search often scores higher than hierarchical search (LAJ+). 
%This confirms that it is a statistically sound method. 
%On most datasets flat search and LAJ+ are close, which indicates that LAJ+ offers an attractive trade-off between statistical power and computational tractability. 


%The situation is reversed on the Mutagenesis dataset where 
%The reason is that flat search misses relationships between attributes from related entities. This happens because the join table contains many more rows with absent relationships than with present relationships. Thus a score-based Bayes net learner assigns by far the most weight to the cases where there is no relationship between different entities. In those cases, there is no correlation between their attributes. The LAJ+ system starts with edges learned among attributes and fixes these during link analysis. In probabilistic terms, we can think of the LAJ+ system as first analysing attribute dependencies {\em conditional on the given link structure}, and then analyzing dependencies among link types.


\section{Computing Relational Contingency Tables} \label{sec:mobius}

The learning algorithms described in this paper rely on the  availability of the extended relational tables $R^{+}$ (see Figure~\ref{fig:university-tables}). Our first naive implementation constructs this tables using standard joins. While this was sufficient for our experiments, the cross-products carry a quadratic costs for binary relations, and therefore do not scale to large datasets. Moreover, the hierarchical search requires joins of the extended tables. In this section we describe a ``virtual join'' algorithm that computes the required data tables without the quadratic cost of materializing a cross-product. 

Our starting point is the observation that a statistical learning algorithm like a Bayes net learner does not require an enumeration of individuals tuples, but only {\em sufficient statistics} \cite{Heckerman1995,Schulte2011}. These can be represented in {\em contingency tables} as follows \cite{Moore1998}. Consider a fixed list of relationship nodes $\R_{1}, R_{2},\ldots,R_{m}$, and attribute nodes $\functor_{1},\ldots,\functor_{j}$. A \textbf{query} is a set of $(node = value)$ pairs where each value is of a valid type for the node. The \textbf{result set} of a query in a database $\D$ is the set of instantiations of the population variables such that the query evaluates as true in $\D$. For example, in the database of Figure~\ref{fig:university-tables} the result set for the query\\ $\it{intelligence}(\S) = 2$, $\it{rank}(\S) = 1$, $\it{rating}(\C) = 3$, $\it{Diff}(C) = 1$, $\it{Registered}(\S,\C) = F$\\ is the singleton $\{\langle \it{kim}, \it{101}\rangle\}$. The \textbf{count} of a query is the cardinality of its result set. Each subset of nodes $\set{V} = v_{1},\ldots,v_{n}$ has an associated \textbf{contingency table} denotes by $CT(\set{V})$. This is a table with a row for each of the possible assignments of values to the nodes in $\set{V}$. The row corresponding to $V_{1} = v_{1},\ldots,V_{n} = v_{n}$ records the count of the corresponding query. 
Figure~{fig:ct} shows a contingency table.

\begin{figure}[tb]
\begin{center}
\resizebox{0.5\textwidth}{!}{
%%!%\includegraphics[width=1\textwidth]{pbn}
\includegraphics{figures/ct-table.pdf}
}
\caption{An example contingency table for the attribute-relation table of Figure~\ref{fig:university-tables}, where for illustration we have added counts for another student like Jack and another course like 103.
\label{fig:ct}}
\end{center}
\end{figure}

A \textbf{conditional contingency table}, written $\ct(V_{1},\ldots,V_{k}|V_{k+1} = v_{k+1},\ldots, V_{k+m} = v_{k+m})$
is the contingency table whose column headers are $V_{1},\ldots,V_{k}$ and whose counts are  defined by subset of instantiations that match the conditions to the right of the | symbol. 

Our flat search algorithm can be implemented by computing the contingency table for all functor nodes, then presenting it to a Bayes net learner. The hierarchical search algorithms can be implemented by computing the contingency tables for each relationship chain in the lattice (cf. Figure~\ref{fig:big-lattice}). For a relationship set, the nodes in the contingency table comprise (1) the descriptive attributes of the entities involved in the relationship set, (2) the descriptive attributes of the relationships, and (3) a Boolean relationship node for each member of the set. This section describes a method for computing these contingency tables level-wise.


 
%For example, in Figure~\ref{fig:example} we have $m=2,j=1$ and $\functor_{1} = \it{gender}(\X)$. The sufficient statistics for this set of random variables are the database probabilities
%
%
%\begin{equation} \label{eq:joint-prob}
%P_{\D}(\R_{1} = b_{1}, R_{2} = b_{2},\ldots, R_{m} = b_{m}; \functor_{1} = v_{1},\ldots,\functor_{j} = v_{j})
%\end{equation}
%
%where the $b_{i}$ values are Boolean and each $v_{j}$ is from the domain of $\functor_{j}$. 
%Bayes net algorithms can construct a Bayes net when provided with a table as input that lists these sufficient statistics. In what follows, we suppose that there are $r$ possible assignments of the form~\ref{eq:joint-prob} and therefore $r$ sufficient statistics to be specified.

%In this section we describe algorithms for computing  probability estimates for the parameters $P_{\D}(\it{child}\_value,\it{parent}\_\it{values})$ in a functor Bayes net. The parameters can be estimated independently for each child node. We refer to a joint specification of values $(\it{child}\_value,\it{parent}\_\it{values})$ as a \textbf{family state}. Consider a child node specifying a relationship $\R_{1}$ whose parents comprise relationship nodes $R_{2},\ldots,R_{m}$, and attribute nodes $\functor_{1},\ldots,\functor_{j}$. The algorithms below can be applied in the same way in the case where the child node is an attribute node. The number of family states $r$ is the cardinality of the Cartesian product of the ranges for every node in the family.
% The Bayes net parameters are $r$ conditional probabilities of the form (we separate relationships from attributes by a semicolon)
%\begin{equation} \label{eq:cond-prob}
%P(\R_{1} = b_{1}| R_{2} = b_{2},\ldots, R_{m} = b_{m}; \functor_{1} = v_{1},\ldots,\functor_{j} = v_{j}),
%\end{equation}
%where the $b_{i}$ values are Boolean and each $v_{j}$ is from the domain of $\functor_{j}$. 
%Figure~\ref{fig:example}~(Top) provides an example with $\R_{1} = \it{Friend}(\X,\Y)$, $\R_{2} = \it{Follows}(\X,\Y)$, and $\functor_{1} = \it{gender}(\X)$. In this example, all nodes are binary, so the CP-table requires the specification of $r= 2 \times 2 \times 2 = 8$ values.\footnote{Although only 4 of these values are independent, we take the number of parameters to be $r = 8$ for simplicity of exposition of the lattice Moebius transform in the next section.}



So long as a database probability involves only positive relationships,
%formula only contains positive relationships, 
%the computation is straightforward. 
%For example, in 
%$P_{\D}(\it{gender}(\X) = M, \it{Friend}(\X,\Y) = \true)$, the value  $\grounds_{\D}(\it{gender}(\X) = M, \it{Friend}(\X,\Y) = \true)$, the count of friendship pairs $(x,y)$ where $x$ is male and the
%{\em Friend} relationship is true, 
and can be carried out by regular table joins or optimized virtual joins~\cite{Yin2004}. 
%
Computing joint probabilities for a family containing one or more negative relationships is harder. A naive approach would explicitly construct new data tables that enumerate tuples of objects that are {\em not} related (see Figure~\ref{fig:university-tables}).
%and then apply existing counting methods to the new tables. 
However, the number of unrelated tuples is too large to make this scalable (think about the number of user pairs who are {\em not} friends on Facebook). 
%A numerical example will illustrate why this is not feasible. Consider a university database with 20,000 Students, 1,000 Courses and 2,000 TAs. If each student is registered in 10 courses, the size of a $\it{Registered}$ table is 200,000. So the number of complementary student-course pairs is $2 \times 10^{7}-2 \times 10^{5}$, which is too big for most database systems. 
%If we consider joins, complemented tables are even more difficult to deal with: suppose that each course has at most 3 TAs. Then  the number of satisfying instantiations of a positive relationship only formula such as $\it{Registered}(\S,\C) = \true,\it{TA}(\T,\C) = \true)$ is less than $6 \times 10^{5}$, whereas with negations the number of instantiations of the expression $\it{Registered}(\S,\it{course}) = \false, \it{TA}(\T,\it{course}) = \false)$ is on the order of $4 \times 10^{10}$. 
%\subsection{The M\"obius parametrization.} 
%To compute frequencies involving negated relationships, we would like to use the optimized algorithms for table join frequencies as an oracle/black box. 
%Can we instead reduce the computation of sufficient statistics that involve negated relationships to the computation of sufficient statistics that involve existing (positive) relationships only? 
In their work on learning Probabilistic Relational Models with existence  uncertainty, Getoor et al. provided a subtraction method for the special case of estimating counts with only a single negated relationship \cite[Sec.5.8.4.2]{Getoor2007c}. They did not treat contigency tables with multiple negated relationships, which we consider next.
%this method does not extend to multiple negated relationships.

\subsection{Level-Wise Computation of Contingency Tables: The Subtraction Method} 

We first introduce some notation. Let $R$ be a relationship node and let $\set{R}$ be a set of relationship nodes.

\begin{itemize}
\item $\eatts(R)$ denotes the set of entity attribute nodes for the population variables involved in the relationship $R$. 
\item $\eatts(\set{R})$ is the union of the entity attributes for each relationship $R \in \set{R}$.
\item $\ratts(R)$ denotes the set of relationship attribute nodes for the population variables involved in the relationship $R$.
\item $\ratts(\set{R})$ is the union of the relationship attributes for each relationship $R \in \set{R}$.
\item $\atts(R)$ is the set of both entity and relationship attributes for relationship $R$, similarly for $\atts({\set{R}})$.
\end{itemize}

We next define some basic operations on CT-tables. These are analogues to relational algebra operations on simple tables.

\begin{enumerate}
\item Let $\ct_{1}(\set{V}),\ct_{2}(\set{V})$ be two union-compatible contingency tables with the same column headers, and (hence) with the same rows, except for the count entries. The \textbf{table difference} $\ct_{1} - \ct_{2}$ is a table with the same column headers, such that $$\#_{[\ct_{1} - \ct_{2}]}(\set{V}=\set{v})= \#_{[\ct_{1}]}(\set{V}=\set{v}) - \#_{[\ct_{1}]}(\set{V}=\set{v}).$$
\item Let $\ct_{1}(\set{V}),\ct_{2}(\set{V'})$ be two disjoint contingency tables, i.e., $V \cap V' = \emptyset$. Then the \textbf{cross-product} $\ct_{1}(\set{V}) x \ct_{2}(\set{V'})$ is a table with the column headers $\set{V} \cup \set{V'}$ whose rows form the cross-product of the rows in $\ct_{1}$ and in $\ct_{2}$, and whose count satisfy $$\#_{[\ct_{1} \times  \ct_{2}]}(\set{V}=\set{v},\set{V'}=\set{v'})= \#_{[\ct_{1}]}(\set{V}=\set{v}) x \#_{[\ct_{1}]}(\set{V}=\set{v}).$$
\item Let $\set{V'}$ be a proper subset of columns $\set{V}$ for a contingency table $\ct(\set{V})$. Then the \textbf{marginal} table $\marginalize(\ct,\set{V'})$ is a table whose headers are $\set{V}-\set{V'}$, such that the count $\#_{\marginalize(\ct,\set{V'})}(\set{V}-\set{V'}=\set{v^{*}}$ is the sum of the counts in $\ct(\set{V})$ that satisfy $(\set{V}-\set{V'})=\set{v^{*}}$.
\end{enumerate}

We are now ready to state some contingency algebra equivalences that allows us to compute counts for rows with negative relations from rows with positive relations.

\begin{proposition}
Let $R$ be a relationship node and $\set{R}$ be a set of relationship nodes not containing $R$. Then the following equations hold.

\begin{eqnarray}
\ct(\set{R},\eatts(\set{R}),\ratts(\set{R}),\eatts(R)| R = F) &=& \ct(\set{R},\eatts(\set{R}),\ratts(\set{R}),\eatts(R)| R = *) - \ct(\set{R},\eatts(\set{R}),\ratts(\set{R}),\eatts(R)| R = T)\\
\ct(\set{R},\eatts(\set{R}),\ratts(\set{R}),\eatts(R)| R = *) & = &\ct(\set{R},\eatts(\set{R}),\ratts(\set{R}) \times_{V \in \eatts(R) - \eatts(\set{R})} \ct{V}\\
\ct(\set{R},\eatts(\set{R}),\ratts(\set{R}),\eatts(R)| R = T) & = & \marginalize(\ct(\set{R},\eatts(\set{R}),\ratts(\set{R},\eatts(R),\ratts(R)| R = T),\ratts(R))
\end{eqnarray}
\end{proposition}

The first equation is important because it shows how counts with a false relationship can be computed from counts with the relationship unspecified, and with the relationship value true. The second equation is important because it shows that counts with the relationship unspecified can be computed from ct tables that omit the relationship, multiplied by tables for entity attributes that are involved in the relationship (and that do not already appear in other relationships). The third equation is important because it shows how a table without the descriptive attributes of R can be computed from a complete CT table with R by marginalizing out the descriptive attributes of R.

Figure~ref{fig:table-equation} illustrates the equations. 
\begin{figure}[tb]
\begin{center}
\resizebox{0.5\textwidth}{!}{
%%!%\includegraphics[width=1\textwidth]{pbn}
\includegraphics{figures/subtraction-flow.pdf}
}
\caption{Examples of the table algebra operations and equations.
\label{fig:flow}}
\end{center}
\end{figure}


\marginpar{could these be done probabilistically instead? With one single equation? Maybe do it that way for AI vs. VLDB?}

For a given chain of relationship nodes $\set{R} = R_{1},\ldots,R_{m}$, the equations can be applied as follows. 

\marginpar{make this pseudocode}

\begin{enumerate}
\item Find the CT table for $R_{1}=\true,\ldots,R_{m}=\true$.
\item For $j=1,\ldots,m-1$, apply the proposition to find $\ct(\atts(\set{R}),R_{j+1},\ldots,R_{m}|R_{1} = \true,\ldots,R_{j} = \true$.
\item Apply the proposition one last time to find $\ct(\atts(\set{R}),\set{R}$.
\end{enumerate}


\section{Conclusion} We described different methods for extending relational Bayes net learning to correlations involving links. 
Statistical measures indicate that Bayes net methods succeed in finding relevant correlations. 
There is a trade-off between statistical power and computational feasibility (full table search vs constrained search). 
Hierarchical search often does well on both dimensions, but needs to be extended to handle correlations conditional on the absence of relationships.

Most of the learning time is taken up by forming table joins, whose size is the cross product (Cartesian product) of entity tables. 
These table joins provide the sufficient statistics required in model selection. 
For Bayes net learning to be possible for big data, computing sufficient statistics needs to be feasible for cross product sizes in the millions or more. 
We described a possible solution: virtual join methods that compute sufficient statistics without materializing cross products such as the fast M\"obius transform, and tuple ID propagation \cite{Yin2004}.



\bibliography{master}
\bibliographystyle{plain}
\end{document}


\section{Evaluation} 
All experiments were done on a QUAD CPU Q6700 with a 2.66GHz CPU and 8GB of RAM. Our code and datasets are available on the world-wide web \cite{bib:jbnsite}. We made use of the following single table Bayes Net search implementation:  GES search \cite{Chickering2003} with the BDeu score as implemented in version 4.3.9-0 of CMU's Tetrad package (structure prior uniform, ESS=10; \cite{2008a}).

\paragraph{Methods Compared}

We compared the following methods.

\begin{description}
\item[LAJ] The previous LAJ method without link correlations (Algorithm~\ref{alg:structure}).
\item[LAJ+] The new LAJ method that has the potential to find link correlations (Algorithm~\ref{alg:structure} with the full join instead of natural join).
\item[Flat] Applies the single-table Bayes net learner to the full table join (extended join with all relationship sets in the database).
\end{description}

\paragraph{Performance Metrics} We report learning time, log-likelihood, Bayes Information Criterion (BIC), and the Akaike Information Criterion (AIC). We write 
$$L(\hat{G},\d)$$ for the log-likelihood score, where
where $\hat{G}$ is the BN $\G$ with its parameters instantiated to be the maximum likelihood estimates given the dataset $\d$, and the quantity $L(\hat{G},\d)$ is the log-likelihood of $\hat{G}$ on $\d$. 

The BIC score is defined as follows \cite{Chickering2003,Schulte2011}

$$\mathit{BIC}(\G,\d) = L(\hat{G},\d) - \mathit{par}(\G)/2 \times ln(m)$$

where the data table size is denoted by $m$, and $\mathit{par}(\G)$ is the number of free parameters in the structure $\G$. The AIC score is given by 

$$\mathit{AIC}(\G,\d) = L(\hat{G},\d) - \mathit{par}(\G). $$

BIC and AIC are standard scores for Bayes nets \cite{Chickering2003}. AIC is asympotically equivalent to selection by cross-validation, so we may view it as a closed-form approximation to cross-validation,  which is computationally demanding for relational datasets. 


\begin{table}[btp] \centering
%\scalebox{0.7in}{
\resizebox{0.5\textwidth}{!}{
\begin{tabular}[c]
{|l|l|l|}\hline
 \textbf{Dataset} & \textbf{\#tuples} & \textbf{\#Ground atoms} \\\hline
University&662&513\\\hline
Movielens &1585385&170143\\\hline
Mutagenesis &1815488& 35973 \\\hline
Hepatitis &2965919&71597 \\\hline
%Mondial &59520&3366 \\ \hline
%UW-CSE &2099&3380 \\ \hline
\end{tabular}
}
 % end scalebox
\caption{Size of datasets in total number of table tuples and ground atoms. Each descriptive attribute is represented as a separate function, so the number of ground atoms is larger than that of tuples.
% \textbf{Zhensong: needs fixing}
 \label{table:datasetsize}}
\end{table}

\paragraph{Datasets}


We used one synthetic and 
three benchmark real-world databases, with the modifications described by Schulte and Khosravi~\cite{Schulte2012}. See that article for more
details.

% \noindent\textbf{Mondial Database.} A geography database, featuring
% one self-relationship, $\it{Borders}$, that indicates which countries border each other. The data are organized in 4 tables (2 entity tables, 2 relationship tables, 10 descriptive attributes).
\noindent\textbf{University Database.} We manually created a small dataset, based on the schema given in Table~\ref{table:university-schema}. 
The dataset is small and is used as a purpose of proofing the correctness of our algorithms.

\noindent\textbf{MovieLens Database.} A dataset from the UC Irvine machine learning repository. The data are organized in 3 tables (2 entity tables, 1 relationship table, and 7 descriptive attributes). 

\noindent\textbf{Mutagenesis Database.} A dataset widely used in ILP research. % \cite{Srinivasan1996}.  
It contains two entity tables and two relationships.

\noindent\textbf{Hepatitis Database.} A modified version of the PKDD'02 Discovery Challenge database. The data are organized in 7 tables (4 entity tables, 3 relationship tables and 16 descriptive attributes).
%, following %we adopted the modifications of 
%Frank {\em et al.} \citeyearpar{Frank2007}. %, which includes removing tests with null values. 

%\noindent\textbf{Financial} A dataset from the PKDD 1999 cup. The data are organized in 4 tables (2 entity tables, 2 relationship tables, 13 descriptive attributes).



\begin{table} \centering
%\scalebox{0.7in}{
\resizebox{0.5\textwidth}{!}{
\begin{tabular}[c]
{|l|l|l|l|}\hline
 \textbf{Dataset} & \textbf{Flat} & \textbf{LAJ+} & \textbf{LAJ}\\\hline
University&1.916&1.183&0.291 \\\hline
Movielens &38.767& 18.204& 1.769\\\hline
Mutagenesis &3.231& 3.448& 0.982\\\hline
Hepatitis &9429.884&8.949&10.617 \\\hline
%Mondial &59520&3366 \\ \hline
%UW-CSE &2099&3380 \\ \hline
\end{tabular}
}
 % end scalebox
\caption{Structure Learning Time  in second.
In order to make the learning feasible, we under sampled Hepatitis database to keep the ratio of positive and negative link indicator equals to one.
% \textbf{Zhensong: needs fixing}
 \label{table:runtimes}}
\end{table}

%
%\subsection{Hypotheses} [consider hypotheses as formulated in the introduction [guesses at results]]
%
%Our results investigate the following issues.
%
%\begin{enumerate}
%\item Which methods provide the fastest model selection? We expect that because propagating results  along the table join lattice constraints the model search, both types of hierarchical search are faster than flat search.
%\item Which methods provide the best data fit? We expect that the models with link type analysis are statistically more powerful than the attribute-only analysis.
%\end{enumerate}

%
%[also consider copying, e.g. from Journal/laj]
%
%We used one synthetic and 5 benchmark real-world databases.The databases and their main characteristics are as follows. For more details please see the references in \cite{Schulte2012} and on-line sources such as \cite{bib:jbnsite}.
%
%{\em University Database.} We manually created a small dataset, based on the schema given in Table~\ref{table:university-schema}. 
%The dataset is small and is used as a purpose of proofing the correctness of our algorithms. 
%The entity tables contain 38 students, 10 courses, and 6  Professors. The $\reg$ table has 92 rows and the $\it{RA}$ table has 25 rows. %This dataset is translated into 513 ground atoms. 
%
%We assume that tables in the relational schema can be divided into {\em entity tables} and {\em relationship tables.} This is the case whenever a relational schema is derived from an entity-relationship model (ER model) \cite[Ch.2.2]{Ullman1982}. The symbol $\E$ and related symbols like $\E_1,\E_i,\E'$ refer to entity tables, and the symbol $\R$ and related symbols like $\R_1,\R_i,\R'$ refer to relationship table. We use a running example, a small database related to a university,  through the paper to further clarify the introduced concept. The university database has three entity tables:  $\student$ table,  $\course$ table, and $\prof$ table.  There are two relationship tables: $\reg$ with foreign key pointers to the $\student$ and $\course$ tables whose tuples indicate which students have registered in which courses and $\ra$ with foreign key pointers to the $\student$ and $\prof$ tables whose tuples indicate the RAship of students for professors.
%
%
%{\em MovieLens Database.} The second dataset is the MovieLens dataset from the UC Irvine machine learning repository. %The schema for the dataset is shown in Table \ref{}. 
%It contains two entity tables: $\it{User}$ with 941 tuples and $\it{Item}$ with 1,682 tuples, and one relationship table $\it{Rated}$ with 80,000 ratings. The $\it{User}$ table has %key field $\it{user\_id}$ and 
%3 descriptive attributes $\age, \it{gender}, \it{occupation}$. We discretized the attribute age into three bins with equal frequency. The table $\it{Item}$ represents information about the movies. It has 17 Boolean attributes that indicate the genres of a given movie. We performed a preliminary data analysis and omitted genres that have only weak correlations with the rating or user attributes, leaving a total of three genres.
%
%% The full dataset contains 170143 ground atoms and is too big for MLN to do structure learning or parameter learning on. We made small subsamples to make the experiments feasible. Sub sampling 100 Users and 100 Items transforms to a db file with 2505 number of groundings. takes around 30 min to run. Sub sampling 300 Users and 300 Items transforms to a db file with 18040 number of groundings takes around 2 days to run. 
%%The full table with 100,000 ratings exceeded the memory limits of Tetrad, so we randomly picked 40\% of the ratings of the relationship table as input data. 
%
%{\em Mutagenesis Database.} This dataset is widely used in ILP research \cite{Srinivasan1996}. %It contains 4 tables total to 15218 tuples. 
%Mutagenesis has two entity tables, $\it{Atom}$ with 3 descriptive attributes, and $\it{Mole}$, with %188 entries and 
%5 descriptive attributes, including two attributes that are discretized into ten values each (logp and lumo). It features two relationships $\it{MoleAtom}$ indicating which atoms are parts of which molecules, and $\it{Bond}$ which relates two atoms and has 1 descriptive attribute. %The full dataset, with 35973 ground atoms, crashed while doing either parameter learning or structure learning. A subsample with 5017 ground atoms is used was running for 5 days and did not terminate. weight learning was feasible. another subsample with 
%Representing a relationship between entities from the same table in a parametrized BN requires using two or more variables associated with the same population (e.g., $\it{Bond}(\A_{1},\A_{2}))$.  
%
%{\em Hepatitis Database.} This data is a modified version of the PKDDï¿½02 Discovery Challenge database, we adopted the modifications of Frank {\em et al.} \cite{Frank2007}, which includes removing tests with null values. It contains data on the laboratory examinations
%of hepatitis B and C infected patients. The examinations were realized between 1982 and 2001 on 771 patients. The data are organized in 7 tables (4 entity tables,  3 relationship tables and 16 descriptive attributes). They contain basic information about the patients, results of biopsy, information on interferon therapy, results of out-hospital examinations, results of in-hospital examinations. 
%%The data were prepared in cooperation with the Shimane Medical University, School of Medicine and Chiba University Hospital, Japan.
%
%\emph{UW-CSE database.} This dataset lists facts about the Department of Computer Science and Engineering at the University of Washington (UW-CSE), such as entities (e.g., Student, Professor) and their relationships (i.e. AdvisedBy, Publication)\cite{Domingos2007}. 
%%The total number of ground atoms is 4,106,841. The database contained a total of 3380 ground atoms. 
%The dataset was obtained  by crawling pages in the department's Web site (www.cs.washington.edu). Publications and AuthorOf relations were extracted from the BibServ database (www.bibserv.org). 
%
%{\em Mondial Database.} 
%%
%%\textbf{Hassan: which version did you use? The full one from http://www.dbis.informatik.uni-goettingen.de/Mondial/mondial-ER.pdf or Bahareh's?} 
%%
%This dataset contains data from multiple geographical web data sources. We followed the modification of \cite{wangMondial}.
%Our dataset contains 4 entity tables, $\it{Country},\it{Continent},\it{Economy},\it{Government}$, where the latter three are related to Country by many-one relationships, and one relationship table $\it{Borders}$ that relates two countries.
%

%
%\begin{enumerate}
%\item  
%The $X^{2}$ score of a model, which is the ratio of the model's log-likelihood over the log-likelihood of a null hypothesis model. 
%For Bayes nets, the null hypothesis model is the disconnected graph.
%
%Recall that the $X^{2}$ is calculated simply as 
%\[
%\mathit{X^{2}}\equiv \sum \frac{(O_{i}-E_{i})}{E_{i}}
%\]
%where $O_{i}$ and $E_{i}$ are the observered and expected log-likelihood, respectively. 
%Here the observered model stands for the disconnected graph and expected model is learned from data.
%The larger $X^{2}$ has a higher probability lies in the right tail with a given significant level. 
%\item 
%The Bayes Information Criterion (BIC), which is defined as follows. 
%Compared to the $X^{2}$ score, the BIC adds a penalty term for parameters to the model likelihood.
% A single-table model selection score has the form $\score(\G,\datatable)$ where $\G$ is a graphical model and $\datatable$ a data table. 
% We consider scores that trade off data fit against model complexity, and that can be computed given the following quantities.
% We also assume that the score is {\em decomposable}, i.e. can be written as the sum of local scores for each node in the BN.


\subsection{Results} 
% \textbf{Zhensong: let's just use flat search with the schema edges}

\paragraph{Learning Times} Table~\ref{table:runtimes}
 provides the model search time for each of the link analysis methods. 
This does not include the time for computing table joins since this is essentially the same for all methods (the cost of the full table join). 
On the smaller and simpler datasets, all search strategies are fast, 
but on the medium-size and more complex datasets (Hepatitis, MovieLens), hierarchical search is much faster due to its use of constraints.
Adding prior knowledge as constraints could speed the structure learning substantially.

%The reason for the speed-up is that LAJ+ starts with the previous LAJ method as the first phase. The edges among attributes that are discovered in the first phase are treated as fixed background knowledge in the second phase. 

\begin{table}[h]
\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|c|c|c|c|c| }
		\hline \textbf{University} &{BIC}& {AIC} &{log-likelihood} &{\# Parameter}\\
			\hline Flat &-17638.27&-12496.72 & -10702.72& 1767\\
			\hline LAJ+ & -13495.34& -11540.75& -10858.75& 655\\
		        \hline LAJ &-13043.17 & -11469.75&-10920.75 & 522\\
			
			\hline
		\end{tabular}
}
\end{center}

\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|c|c|c|c|c| }
		\hline \textbf{MovieLens} &{BIC}& {AIC} &{log-likelihood} &{\# Parameter}\\
			\hline Flat &-4912286.87&-4911176.01 & -4910995.01& 169\\
			\hline LAJ+ & -4911339.74& -4910320.94& -4910154.94& 154\\
		        \hline LAJ &-4911339.74 & -4910320.94&-4910154.94 & 154\\
			
			\hline
		\end{tabular}
}
\end{center}

\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|c|c|c|c|c| }
		\hline \textbf{Mutagenesis} &{BIC}& {AIC} &{log-likelihood} &{\# Parameter}\\
			\hline Flat &-21844.67&-17481.03 & -16155.03& 1289\\
			\hline LAJ+ & -47185.43& -28480.33& -22796.33& 5647\\
		        \hline LAJ & -30534.26 & -25890.89&-24479.89 & 1374\\
			
			\hline
		\end{tabular}
}
\end{center}

\begin{center}
\resizebox{0.5 \textwidth}{!}{
\begin{tabular}{|c|c|c|c|c| }
		\hline \textbf{Hepatitis} &{BIC}& {AIC} &{log-likelihood} &{\# Parameter}\\
			\hline Flat &-7334391.72&-1667015.81 & -301600.81& 2037\\
			\hline LAJ+ & -457594.18& -447740.51& -445366.51& 2316\\
		        \hline LAJ &-563439.17 & -472065.07& -470260.07 & 1805\\ %guessed, since the program n/t
			
			\hline
		\end{tabular}
}
\end{center}


\caption{Performance of different Searching Algorithms by dataset.  }
\label{table:result_scores}
\end{table}
\paragraph{Statistical Scores}

As expected, adding edges between link nodes improves the statistical data fit: 
the link analysis methods LAJ+ and Flat performe better than the learn-and-join baseline in terms of log-likelehood on all datasets shown in table~\ref{table:result_scores}.
For samller data sete these three models achieved very similar results on both the BIC and AIC scores.
But for the complicated dataset (i.e. Hepatitis subsampled), the hierarchical search approach does better.

Between the link analysis methods, flat search often scores higher than hierarchical search (LAJ+). 
This confirms that it is a statistically sound method. 
On most datasets flat search and LAJ+ are close, which indicates that LAJ+ offers an attractive trade-off between statistical power and computational tractability. 
Hepatitis (subsampled) is a challenging dataset for flat search, where hierarchical search does much better. 
The situation is reversed on the Mutagenesis dataset where LAJ+ performs very poorly compared to flat search 
(lower likelihood yet many more parameters in the model). 
Investigation of the models shows that the reason for this phenomenon is a special property of the Mutagenesis dataset: 
whereas generally relationships are sparse---very few pairs of entities are actually linked---in Mutagenesis most entities whose type allows a link are linked. 
As a result, we find strong correlations between attributes conditional on {\em the absence of relationships}. 
The LAJ+ algorithm is constrained so that it cannot add Bayes net edges between attribute nodes at its second stage, when absent relationships are considered. 
As a result, it can represent attribute correlations conditional on the absence of relationships only indirectly through edges that involve link indicators. 
A solution to this problem would be to add a phase to  the search so that we first learn edges between attributes first conditional on both the existence of relationships, 
then conditional on their nonexistence. The last phase then would consider edges that involve relationship nodes.

%The reason is that flat search misses relationships between attributes from related entities. This happens because the join table contains many more rows with absent relationships than with present relationships. Thus a score-based Bayes net learner assigns by far the most weight to the cases where there is no relationship between different entities. In those cases, there is no correlation between their attributes. The LAJ+ system starts with edges learned among attributes and fixes these during link analysis. In probabilistic terms, we can think of the LAJ+ system as first analysing attribute dependencies {\em conditional on the given link structure}, and then analyzing dependencies among link types.

\section{Conclusion} We described different methods for extending relational Bayes net learning to correlations involving links. 
Statistical measures indicate that Bayes net methods succeed in finding relevant correlations. 
There is a trade-off between statistical power and computational feasibility (full table search vs constrained search). 
Hierarchical search often does well on both dimensions, but needs to be extended to handle correlations conditional on the absence of relationships.

Most of the learning time is taken up by forming table joins, whose size is the cross product of entity tabes. 
The table joins provide the sufficient statistics required in model selection. 
To improve scalability, computing sufficient statistics needs to be feasible for cross product sizes in the millions or more. 
The proposed LMT algorithm could compute the sufficient statistics across multiple tables efficiently with high scalability in terms of multi-relational Bayes net structure learning for big data.
A promising solution is to utilize virtual join methods that precompute sufficient statistics without materializing table joins such as the Fast M\"obius Transform and tuple ID propagation \cite{Yin2004}.

\bibliography{master}
\bibliographystyle{plain}
\end{document}


\section{undersample notes}
From our current experiments  we observed that after adding the relationship indicator in the relation tables, the learned BN contain more knowledge. 
In other words it has more edges showing the dependency relationship between different link types.
(maybe give an example to show these edges are meaningful and helpfull for understanding the dataset)

However due to the natural of BIC score, which is prefer the simpler model, so laj+ model can not beat the model using flat search.
And also this is casued by the skew distribution problem (citation?) (some datasets the negative and positive ratio is more than 1000).
There is still no general solution people could emply. 
We follow the undersample approach (citation) to keep the ratio as 1:1 and found some very intereting results.


