\documentclass[oribibl]{llncs}
\usepackage{url}
\input{preamble-stuff}
% after reviewer comments:
1) fix typos etc.
2) emphasize performance improvement for count model for CP parameters. Maybe do graph to show pareto-optimality. 
3) emphasize new contribution.
4) check results, do we really do worse in 2 situations?
% Figure ideas: Combine acc and cll graphs, label  bars with standard deviation.
% Experiment Ideas: 1) we have an unfair advantage because we don't try to learn weights that predict links. 2) could use MLN discriminatively successively to learn child|parents. 3) uniform across attributes? 4) testbed: freq smoothes. but i want high values because of strong correlations. Has to do with binary values and with asking only about a single query atom (e.g., not asking about a, b which are pointing in ``opposite'' directions.
% Exposition: maybe put related work at end. Or move Markov network out of the introduction.
% Oh no: the regression equations are messed up because they look like they refer to the entire graph.
%TODO: 
%* check Lafferty on frequencies vs. counts
% EXTENSIONS: 
%1. use decision trees. Or relational regression trees.
%2. consider in more detail having the 0 weight, i.e., the unit clause.
%Add w_{0} weight equiv unit clauses. Normal move. done by domingos. Not yet considered in BN context. Intuition: w_{0} is base rate, w_{i} are changes from w_{0}. Fixes irrelevance problem.  Fixes problem of overweighting prior. Give 2 geometric interpretations. Prove optimization of P-tilde still works.
%3. Implement proper version of method that uses true prior only when faced with no groundings.
%4. Once we predict links, we can't use logs because of 0 problem. Need to use linear shift.
%5. Once I've worked out how to deal with links, maybe use this solution to think about joint distribution. Can it be done with entity join somehow? Or with Gibbs approach?


\begin{document}
\title{A Log-Linear Inference Model for Bayes Nets Applied to Relational Data}
\institute{School of Computing Science,   Simon Fraser University,\\ Burnaby, B.C.,   Canada~V5A~1S6,  
\email{oschulte@cs.sfu.ca,hkhosravi@cs.sfu.ca}}
\author{Oliver Schulte \and Hassan Khosravi \and Tiaxiang Gao \and Yuke Zhu}
%\author{Oliver Schulte\\
%\\ School of Computing Science\\ Simon Fraser University\\Vancouver-Burnaby, Canada}
\date{\today}
\maketitle

\begin{abstract} Log-linear models are widely used for relational data for both generative and discriminative models \cite{Taskar2002,Domingos2009}. They use a weighted sum of variables to define a log-likelihood function.
%, and 
%% where the predictors are instance counts of relational patterns }. 
%are usually derived from Markov net models. 
In this paper we describe a new log-linear relational model class derived from Bayes nets, where the regression weights are log-conditional probabilities. Log-linear Bayes nets are desirable because
conditional probabilities have a natural interpretation and a scalable
maximum likelihood solution for learning. Previous log-linear models used instance counts of relational patterns 
as predictor variables or features. Relational instance counts can be on very different scales  \cite{Domingos2009}; this imbalance problem leads to poor predictive performance when log-conditional probabilities are used as regression weights. We introduce a new log-linear inference model where all variables are scaled to the [0,1] range by using {\em frequencies,} not counts, of relational patterns. 
%The frequency model can be interpreted in terms of the expected value of the prediction that results from a random instantiation of  the Markov blanket of a target node. 
We carried out an empirical comparison on five benchmark databases with (i) weights as log-conditional probabilities vs. (ii) general weights learned with Markov net methods. The conditional probability parameters took seconds to compute  in comparison to hours for Markov net learning. With the frequency scaling, predictive accuracy for the conditional probability weights was much better than with counts, and competitive with the general weights.
\end{abstract}
 
 
\section{Introduction}
Relational data are very common, from enterprise relational databases to network data arising from the world-wide web or social media. Log-linear models are a prominent model class in machine learning that has been widely used with relational data. In a log-linear graphical model, a joint generative distribution is defined via a product of local factors, which is not necessarily normalized. If the model is used discriminatively to make predictions about the value of a specific variable/node, the {\em regression equation} for the unnormalized likelihood $\tilde{P}$ is

\begin{equation} \label{eq:regress}
\tilde{P}(y|\set{x})= exp(\sum_{i} w_{i} x_{i})
\end{equation}

%\begin{equation} \label{eq:regress}
%\tilde{P}(y|\set{x}) = exp(\sum_{i} w_{i} x_{i})
%\end{equation}

where $\tilde{P}(y)$ is the unnormalized 
probability of a target variable or node, and the $x_{i}$ are values of all relevant predictors in the Markov blanket of $y$. The $w_{i}$ are weight parameters representing the (log)-factors associated with the $x_{i}$. 

Log-linear models are usually associated with undirected models, such as Relational Markov networks \cite{Taskar2002} and Markov Logic Networks \cite{Domingos2009}. An MLN is a set of weighted first-order formulas that compactly defines a Markov network comprising ground instances of logical predicates.  In this paper we study log-linear models that are derived from directed Bayes net models rather than from Markov net models. Our motivation for introducing this model class is that (1) maximum likelihood estimation provides a fast and scalable basis for parameter learning, and (2) the parameters have a natural interpretation as conditional probabilities. We summarize the main features of the log-linear Bayes net models.
%Log-linear models %, including the ones we propose for Bayes nets, 
%accommodate cyclic dependencies. 
%
%
%We summarize the main features of the relational log-linear models derived from Bayes nets that are discussed in this paper.


%\paragraph{Cyclic Dependencies.} %Recent research with directed relational models has developed a conversion approach where after learning, a 1st-order Bayes net is converted to a model without acyclicity constraints, such as a Markov network or a dependency network; the converted model is then used for inference [cite Khosravi, Kersting]. The log-linear inference models that we define for Bayes nets can be derived from conversion methods.

%
%
%\paragraph{Cyclic dependencies.} In a standard relational log-linear model, such as those defined by a Markov random field [cite], the predictive features $x_{i}$ are defined by the counts $n_{i}$ of relevant relational patterns defined by the model (i.e., $x_{i} = n_{i}$). For instance, to predict the intelligence $y$ of a student, the model may consider how many A grades she has received, how many B grades, etc. A straightforward log-linear model for Bayes nets uses the same predictors, with log-conditional probabilities as weights. This model can be interpreted as the result of converting the Bayes net to a Markov net using the standard moralization method (connect co-parents, drop edge directions), with conditional probabilities as clique potentials. This conversion is suggested by Domingos and Richardson [cite MLN site, paper].
%Since this model uses the same type of predictors as a relational Markov random field, log-linear inference with cyclic dependencies is handled in the same way as in an undirected model. However, in the presence of cyclic dependencies the resulting likelihood is not normalized. Thus inference with cyclic dependencies is defined in a principled way, but at the cost of requiring a normalization constant.
%

%The normalization constant in a log-linear relational Bayes net model is like the normalization constant for a log-linear Markov net model (partition function). In contrast, a nonrelational Bayes net model does not require an explicit normalization constant, because acyclicity guarantees normalization [cite]. 
%

\paragraph{Parameter Space $\{w_{i}\}$.}
The characteristic feature of a Bayes net model is that its parameters are conditional probabilities of a child node value given an assignment of values to its parents.
In the Bayes net log-linear model, the weights $w_{i}$ are logarithms of these conditional probabilities. Equivalently, the regression equation is a product of conditional probability factors. We refer to models with weights derived from conditional probabilities as Bayes net or {\em CP} log-linear models. We refer to models without constraints on the weights as Markov net or general log-linear models. 
 %of a child node values given an assignment of values to its parents. 
% In relational data, the restriction to conditional probabilities incurs a loss of expressive power.
%%, in that they cannot all probability distributions over relational structures that general log-linear models can represent. 
%For instance, in general rescaling weights and rescaling predictor variables defines equivalent log-linear models, but rescaling conditional log-probability violates the axioms of probability. Despite the loss of modelling power, we believe that conditional probability parameters are well motivated by their advantages for interpretability and scalability.

A standard approach to defining an inference model for a 1st-order Bayes net is to view it as a template for a ground (unrolled) Bayes net \cite{Poole2003}. However, grounding a 1st-order Bayes net often leads to a graph with cycles, which arise from recursive dependencies or autocorrelations, where the value of an attribute for an individual depends on the value of the same attribute for related individuals \cite{Neville2007,Schulte2011b}. While we use the parameters of the 1st-order Bayes net to specify weights for a CP model, inference in a CP model is defined by a log-linear formalism, not with respect to a ground acyclic Bayes net. Recursive dependencies are therefore handled in the same way as with other log-linear formalisms. We discuss interpretations for the CP models in terms of ground graphs in  Section~\ref{sec:interpret}.


%Log-linear models are a prominent model class in machine learning that subsumes both directed and undirected graphical models [cite Bishop]. They are widely used for relational data, both in discriminative and generative learning [cite]. In a relational log-linear model, the predictive features are defined by counts of relevant relational patterns defined by the model. For instance, to predict the intelligence of a student, the model may consider how many A grades they have received, how many B grades, etc. The log-linear Bayes net models that we examine uses the same type of predictors as a relational Markov random field. Therefore, log-linear inference with cyclic dependencies is handled in the same way in both directed and undirected models. However, in the presence of cyclic dependencies the resulting likelihood is not normalized and therefore inferences require a normalization constant. The normalization constant in a relational Bayes net model is like the normalization constant for a Markov net model (partition function); in contrast, a nonrelational Bayes net model does not require an explicit normalization constant, because acyclicity guarantees normalization [cite]. Within the space of log-linear models, Bayes nets are distinguished by the fact that their parameters are of a special form, namely conditional probabilities of a child node value given an assignment of values to its parents. Using parameters that are restricted to conditional probabilities, or simple functions of them (i.e., logarithms), has advantages for interpretability and scalability.

%The learning time gains over local search methods for Markov models are substantial: in our experiments on benchmark databases, the Bayes net parameters are computed in seconds, but in the worst-case database the Alchemy system requires 10 hours to optimize weights.
% rather than facts about specific individuals. 
%
%Sophisticated methods have been developed for optimizing weights for a relational Markov model, especially for Markov Logic Networks [cite]. While these methods find parameters that lead to accurate predictions, they are local search methods in a high-dimensional parameter space and can be a significant computational bottleneck. For instance, Khosravi et al report that MLN parameter optimization on benchmark databases can take hours and in some cases days [cite]. In contrast, the Bayes net parameters can be estimated as the observed conditional frequencies in the input relational database, with optional smoothing methods. The computational cost is only that required for event counts (sufficient statistics) in the database, and therefore scales well both in the size of the database {\em and} the number of parameters in the model. Using frequency estimates can be viewed as a type of lifted learning, which uses only the sufficient statistics in a relational database rather than an iteration over ground facts.
%
%
%We show in experiments that using the observed conditional frequencies leads to accurate predictions. Moreover, these 

%
%\item {\em Structure Learning} often requires repeated parameter estimation, where weights are found for a candidate structure to assess its likelihood, then a new candidate stucture is explored etc. If evaluating candidate structures requires parameter optimization, a fast parameter estimation procedure is essential for scalable learning. Slower optimization methods can be used after structure learning has converged on a final model.
%\item Other common learning problems involve repeated parameter estimation as well, for instance an {\em expectation-maximization} approach to dealing with unobserved variables or missing data.
%\end{itemize}
%
%One of the reasons for the widespread popularity of Bayes nets for nonrelational data is that parameters have a natural interpretation and high-quality estimates can be obtained quickly. 
%We believe that providing users with a relational model class that has similar advantages will encourage applications of statistical-relational learning: Users have the option to carry out an initial data exploration and deploy more complex methods if the results are promising.

%\paragraph{Counts vs. Frequencies} For nonrelational data, restricting the parameter space to conditional probabilities does not entail a loss of expressive power, in the sense that for a graph structure that is adequate for the target generating distribution (an I-map), conditional probability settings can be found that represent the target distribution. An important difference for relational data is that conditional probabilities are restricted in expressive power, in that they cannot all probability distributions over relational structures that general log-linear models can represent. For example, consider rescaling the predictors in a regression model [cite] such that $x'_{i} := x_{i}/k_{i}$. If there is no constraint on the weights, the resulting model is equivalent to one with the original predictors but rescaled weights $w_{i}/k_{i}$. Thus unrestricted the weights can, in effect, change the scale of a predictor, whereas if weights are constrained to be log-probabilities, rescaling typically violates this constraint. While restricting the parameter space to conditional probabilities has the advantages of interpretability and scalability that we discussed, it means that the quality of predictions becomes more sensitive to the form of the model and the scale of the predictors. Researchers have observed previously that relational regression models feature highly unbalanced feature counts [cite Domingos, me]. For example, the number of courses a student has taken may be in the 10s, whereas the number of their friends in a social network may be in the 100s. Moreover, a single attribute of a student (e.g., gender) will be instantiated only once for each student, so the count of this feature is just 1. The effect is that although attributes of the target individual tend to be more informative than attributes of related individuals, the counts associated with related individuals are much higher.

\paragraph{Predictor Space $\{x_{i}\}$.} 
%
In most relational log-linear models, the predictive features $x_{i}$ are defined by the {\em counts} $n_{i}$ of relevant relational patterns defined by the model \cite{Taskar2002,Domingos2009}. For instance, to predict the intelligence $y$ of a student, the model may consider how many A grades she has received, how many B grades, etc. 
%Our first  model uses feature counts.
%; it can be interpreted as the result of converting the Bayes net to a Markov net using the standard moralization method (connect co-parents, drop edge directions), with conditional probabilities as clique potentials. 
%
A problem with counts is that features with more instances have exponentially more influence. 
For example, if the model considers the ranking of a student as well as grades, the grade factors overwhelm the ranking, because a student has just one ranking but many grades. 
Since in a Bayes net model, the weights are on the same scale (log-probability), smaller weights cannot sufficiently scale down the impact of predictors with larger domains. 
%A common solution to unbalanced predictor variables in regression models is standardizing the variables to a common scale \cite{Raina2003}. 
%The second model we consider in this paper 
Therefore we investigate using feature {\em frequencies} $f_{i}$  as predictors $x_{i}$, whose scale is [0,1]. In the intelligence prediction example, we use the percentage of A grades among all grades the student has received, the percentage of B grades, etc. The use of frequency predictors is equivalent to using the {\em geometric mean} rather than the simple product to combine factors. 

%We show that the frequency regression equation can be interpreted as a {\em random regression} as follows. Suppose we randomly sample an instantiation of the Markov blanket of the target node, and compute the log-conditional probability (unnormalized) of the target node given the instantiation. Then the frequency regression equation is equivalent to the expected log-conditional probability. 
%
While this paper focuses on Bayes net models, the distinction between counts vs. frequencies as predictors can be explored  in other log-linear relational models, for example as a different form of potential function for the recent functional gradient boosting approach \cite{Natarajan2012,Khot2011}. 
%The random regression semantics applies to any logic-based model with 1st-order variables.
%For example, functional gradient boosting has recently been shown to be a strong technique for relational regression models; adapting gradient boosting with frequency predictors is a promising topic for future research. 

%The frequency model does not have an interpretation in terms of a generative Markov random field. The reason for this is that the scaling factor for each weight/factor depends on the target node being queried, whereas a clique potential must be fixed independent of a query. However, the frequency model can be interpreted in terms of a dependency network \cite{Heckerman2000,bib:jensen-chapter,Natarajan2012}.  
%Dependency networks are directed graphs that allow cyles; as with undirected models, probabilistic conditional independence corresponds to graph separation. 
%The parameters of dependency networks are conditional probabilities of each node value given an assignment of values to its {\em Markov blanket}, which are specified by the regression equations.

%\paragraph{Evaluation.}
%In experiments on five benchmark databases, using the maximum likelihood conditional probabilities together with the frequency model yields much better estimates than the count model. The frequency model is competitive with a general log-linear model with optimized general weights. We provide empirical evidence that optimized general weights show scaling effects in that the absolute values of weights associated with formulas with high counts tend to be smaller. 
%
%Although the frequency model is intuitively plausible and performs well, it does not appear to have a straightforward interpretation in terms of a generative graphical model. The fundamental reason for this is that {\em the scaling factor for each weight/factor depends on the node being queried}. For instance, the scaling factor will be different if the target node is the intelligence of a student (number of courses taken), or if the target node is the difficulty of a course (number of students in the course). While a uniform scaling factor could be incorporated into the weight parameters of the model, as shown above, the parameters of a Markov random field cannot be changed depending on the query posed. This illustrates another reason for the predictive power of the frequency model: in addition to the global parameters that are applied for predicting a target variable as part of parameter sharing [cite], the scaling factor represents local information, namely the size of the relational neighborhood of the target entity. We discuss the interpretation of the frequency regression equation in detail and make two main observations. 
%
%\begin{enumerate}
%\item The frequency model can be seen as defining a dependency network [cite] rather than a Markov random field. Dependency networks were introduced by Heckerman et al. and have been adapted for relational data. The graphical structure of a dependency network is essentially the same as that of a Markov random field [cite]; in particular, probabilistic conditional independence corresponds to graph separation. The parametrizations, however, are different: While the parameters of Markov random fields are clique potentials, the parameters of dependency networks are conditional probabilities of each node value given an assignment of values to its {\em Markov blanket}. Since the regession equation~\ref{eq:regress} defines this conditional probability, it specifies the parameters of a dependency network.
%\item The frequency regression equation can be interpreted as specifying an expectation as follows. Suppose we randomly sampled an instantiation of the Markov blanket of the target node, and computed the log-conditional probability of the target node given the instantiation. Then the regression equation is the expected log-conditional probability over random instances.
%\end{enumerate}
%
%To illustrate the random selection interpretation, suppose we randomly selected one of Bob's courses and estimated the log-probability of Bob's intelligence given the single selected course only. Then the frequency equation gives the expected log-probability for a randomly selected course. 
%

%An important advantage of log-linear models is their ability to accommodate cyclic dependencies in relational data. 
%A key phenomenon in relational data are cyclic dependencies. 
% Cyclic dependencies are a challenge for Bayes nets \cite{Taskar2002,Domingos2009,Neville2007}, but can be accommodated in the log-linear Bayes net inference models we describe. \marginpar{Hassan} 

\paragraph{Evaluation.} Our experiments use five benchmark databases. For each database, we learn a Bayes net structure, and evaluate four different combinations of parameter/predictor spaces for this fixed structure. 
%We evaluate the predictive accuracy and parameter learning time of the models on five benchmark databases. 
For conditional probability parameters we use the maximum likelihood parameter settings (observed frequencies). 
% Bayes net model we use the empirical conditional frequencies 
%We evaluate the predictive performance of the log-linear Bayes net models with the maximum likelihood parameter settings on five benchmark databases. 
This is compared to general log-linear weight parameters computed by an optimization routine, the default routine in the Alchemy system. The Alchemy package is a state-of-the-art open-source software system for MLNs \cite{Kok2009a}. Using conditional probabilities brings substantial scalability improvements: parameter learning takes seconds, while the Alchemy system requires hours on 3 out of 5 benchmark databases. The predictive performance of conditional probabilities, using frequency predictors, is competitive with the general log-linear model with optimized weights, if not superior. The count model performs worse.
% on the standard conditional log-likelihood metric (basically, average predicted log-likelihood of a database fact given the other database facts). 
%The prediction task is the standard Markov blanket prediction task [cite]: predict in turn the value ground node, given assignments to all the remaining nodes. In database terms, this amounts to predicting the value of a single entry in a database table given all the other entries. We find that the combination of MLE parameters+ counts performs poorly on the average likelihood assigned to the correct value of a node. However, the combination of MLE parameters + frequencies performs quite well, competitive with the optimized general weights. This is evidence that once the predictor variables are transformed to the same scale, restricting the parameter space to conditional probabilities is consistent with good performance. Considering that the computation time of the MLE parameters is orders of magnitude faster than the local search, the MLE + frequency model presents an attractive trade-off between expressive power and computational tractability. We consider synthetic datasets that allow us to systematically vary the scale of feature counts to examine the impact of scale imbalances.

%\paragraph{Limitations and Extensions.} In this paper we consider inference only with respect to predictions given the Markov blanket, which can be evaluated in closed-form. Heckerman et al. \cite{Heckerman2000} show that applying Gibbs sampling with the regression equations defines a stationary joint distribution, hence can be used to answer general queries. Their ordered pseudo-Gibbs sampler has been lifted to the relational setting \cite{Neville2007}. A Gibbs sampler for the frequency model would be able to exploit the ordering constraints provided by the Bayes net. Consider how similar the count and frequency models are, it should also be straightforward to adapt MLN inference methods developed for the count model with the frequency model.



\paragraph{Paper Organization.}
We describe further related work. Then we present background: basic relational graphical models and connections between them. The next section defines the frequency and count regression models. We discuss parameter estimation with conditional probabilities, and the interpretation of the regression models in terms of ground graphical models.
% and shows that conditional probabilities maximize the unnormalized likelihood. We then present the frequency regression model. 
Empirical evaluation compares the frequency and count models with optimized log-linear weights on a number of benchmark databases. 
%We also present simulation results that examine the impact of different scales for feature counts.

\paragraph{Contributions.}
The main contributions of this paper to relational learning may be summarized as follows.

\begin{enumerate}
%\item An empirical evaluation of log-linear Bayes net models with count predictors, together with an analytic solution for the likelihood maxima.
\item A new log-linear regression model for Bayes nets that uses feature frequencies rather than counts, and an empirical comparison of the frequency and count models.
\item A theoretical interpretation of the frequency model in terms of random instantiations of the Markov blanket of the target node.
\item Experiments that indicate that the frequency model with maximum likelihood estimates is competitive with a general log-linear model with optimized weights.
\end{enumerate}


\section{Related Work} \label{sec:related} {\em Moralization Methods.} Several researchers have examined converting a Bayes net relational model to a Markov net, which defines a log-linear model. Richardson and Domingos propose converting a Bayes net to a Markov Logic network using moralization to convert the structure and log-conditional probabilities as clique potentials \cite{Domingos2009}. This is also the standard Bayes net conversion recommended by the Alchemy system \cite{bib:bayes-convert}. The moralization method is equivalent to our log-linear model with counts. Khosravi et al. \cite{Khosravi2010} follow the moralization approach for the model structure, but do not use log-probabilities as parameters for inference. Instead, they use MLN parameter learning methods to obtain weights. To our knowledge, our experiments are the first that evaluate the moralized Bayes net structure with log-probability weights. 

Natarajan et al. \cite{Natarajan2010} consider Bayes nets that have been augmented with combining rules for mapping probabilities obtained from multiple parent instances to a single one.
%(e.g., the set of courses that a student has taken may generate a set of conditional probabilities for the student's intelligence as a child node, one for each course). 
In contrast, we consider tabular Bayes nets whose parameters are CP-table entries only. Combining rules do not generally lead to log-linear models. 
%For instance, if arithmetic mean is used as a combining rule, a product over all child-parent conditional probabilities becomes a product over sums, one sum for each child. 
Natarajan et al. show that for decomposable combining rules, the combining rule can be implemented using additional unobserved random variables (``multiplexers'') \cite{Natarajan2010}. The entire Bayes net structure with observed plus unobserved variables can then be converted to an MLN, which would appear to define a log-linear model in the augmented variable space. Our log-linear model uses only observed features specified in the original Bayes net model. Another difference is that with combining rules, there is no closed form for parameter estimation, so gradient descent methods are applied.

{\em Scaling Predictors.} Scaling predictors to the [0,1] range is a familiar technique in log-linear regression model \cite{Raina2003}. To our knowledge, scaling has not been applied for inference in the generative context. Variants of the Markov net pseudo-likelihood have been proposed that include scaling factors, such as the Weighted Pseudo Log-Likelihood \cite{Domingos2009} and the random selection pseudo-likelihood \cite{Schulte2011}. The key difference is that these scaling factors are used only during {\em learning} to ensure that the learning algorithm optimizes parameters sufficiently for features with low counts. In contrast, we use the scaling parameters during inference. 

The frequency model uses both global shared parameters (conditional probabilities) and local features that depend on the individual target node (scaling factor). Combining rules like the arithmetic mean \cite{Natarajan2010} similarly combine global parameters with a local scaling factor. Our frequency model uses the geometric mean rather than the arithmetic mean. To our knowledge, the geometric mean has not been used in Bayes net models with relational data. Another difference with combining rules is that we apply scaling to the entire Markov blanket of the target node, whereas a combining rule applies only to the parents of the target node.

%\paragraph{Approach} We focus on {\em Markov blanket prediction models}: given an assigment of values to the Markov blanket of a node $\node$, what should be the conditional probability distribution over values of $\node$? This is a simpler question for how to define a generative model, but an adequate substitute for the following reasons: First, Gibbs' classic theorem implies that a specification of all Markov blanket prediction models determines a unique joint distribution, provided that the joint distribution is strictly positive everywhere. Second, for the undirected model classes that we consider----Markov and dependency networks---the connection between the conditional and the joint distributions is immediate. Third, one of the main performance metrics for a relational model is ``conditional log-likelihood'', which is defined as the average log-likelihood of the true value of a node $\node$ given its Markov blanket. Thus this prediction metric focuses on Markov blanket prediction. An advantage is being to apply regression ideas.
%
%For propositional data, the Bayes net Markov blanket classifier assigns as the probability that $\node = \value$ given its Markov blanket, the (normalized) product of the conditional probability of $\node$ given its parents, times the probabilities of the children of $\node$ given their parents. A straightforward way to adapt this classification model for relational data is to treat the Bayes net as a template, instantiate the Markov blanket nodes of the target node with the applicable constants, and then take the product of the associated probabilities. This is illustrated in Figure x, using the plate model notation for a Parametrized Bayes net developed by Poole. We provide two interpretations for this product Markov blanket prediction model. (1) Suppose that the Parametrized Bayes net is converted to Markov net using the conditional probabilities as clique potentials, as suggested by Domingos and Richardson. The MB prediction model for the resulting Markov net is the product one. (2) Suppose that we use product as a combining rule, that is, we combine the probabilities of a child node given its parents by the product of the conditional probabilities. The MB prediction model for the resulting Markov net is the product one. In both interpretations, the weights in the log-linear model are the logarithms of the conditional probabilities in the Parametrized Bayes net. This suggests using Bayes net parameter learning to learn the conditional probabilities, and then convert them to weights in a log-linear model. We compare this approach to the moralization method of Khosravi et al., which uses MLN learning to find the weights. As a heuristic for Bayes net parameter estimation, we use the observed database frequencies (with Laplace correction). This heuristic has several motivations.$
%
%Heuristic: use frequency estimates. Propositional case: Analytic for generative models. Works well for discriminative (see Domingos). If it works well for discriminative, should work well for 

\section{Background: Relational Graphical Models} We denote random variables by upper case letters such as $X_{i}, \Y_{j}$. With respect to a graphical model, we interchangeably refer to its nodes and its variables. We consider only graphical models with discrete random variables. We  use vector notation for lists of random variables and for lists of values assigned to them, e.g., $P(\X_{1} = \x_{1},\ldots,\X_{n} = \x_{n}) \equiv P(\set{X} = \set{x})$. 

\subsection{Graphical Models}
A Bayes net (BN) is a pair
$\langle{G,\bs{\theta}_G}\rangle$ where $\bs{\theta}_G$ is a set of parameter values that specify the  probability distributions of children conditional on instantiations of their parents, i.e. all conditional probabilities of the form
\[\theta_{ijk} \equiv P(\node_{i}=\nodevalue_{ik}|\Parents_{i}=\parents_{ij}),\] where $\nodevalue_{ik}$ is the $k$-th possible value of node $i$ and $\parents_{ij}$ is the $j$-th possible configuration of the parents of $\node_{i}$. The conditional probabilities are specified in a \textbf{conditional probability table} for variable $\node_{i}$ or CP-table. The Markov blanket of a BN node $\Y_{i}$ comprises the set of $\mbox{children}_{i}$, $\mbox{parents}_{i}$ and co-parents. The unnormalized \textbf{Markov blanket classification equation} is given by

\begin{equation} \label{eq:bn-mb}
\tilde{P}(\Y_{i} = \y|\set{X} = \set{x}) = P(\Y_{i} = \y|\parents_{i}) \cdot \prod_{\X_{j} \in \mbox{children}_{i}} P(\X_{j} = \y|\parents_{j}).
\end{equation}
where $\set{X}$ is the set of all nodes other than $\Y_{i}$.

A \textbf{Markov network} structure is an undirected graph. For each clique $\clique$ in the graph, a \textbf{clique potential function} $\potential_{\clique}$ specifies a nonnegative real number for each possible assignment of values to the clique. For an assignment of values to all nodes in the Markov net, the joint probability of the values is given by the product of the associated clique potentials, divided by a normalization constant.

A \textbf{dependency network} structure is a directed graph; cycles are allowed \cite{Heckerman2000,bib:jensen-chapter,Natarajan2012}. The parameters are conditional probabilities of each node, given its {\em Markov blanket} (not just the parents).  Dependency networks are like Markov networks in that conditional probabilistic independence corresponds to graph separation. They are like Bayes nets in that the parameters are conditional probabilities.

\subsection{Graphical Models for Relational Data} \label{sec:graph-relational}
We follow the original presentation of Parametrized Bayes Nets due to Poole \cite{Poole2003}. A \textbf{functor} is a function symbol or a predicate symbol. In this paper we discuss only  functors with a finite range of possible values. 
%A functor whose range is $\{\true,\false\}$ is a \textbf{predicate}, usually written with uppercase letters like $P,R$. 
A \textbf{parametrized random variable} or \textbf{functor node} is  of the form $\functor(\term_{1},\ldots,\term_{k}) = \functor(\set{X})$ where $\functor$ is a functor and each $\term_{i}$ is a first-order variable $\X_{i}$ or a constant $a_{i}$ of the appropriate type for the functor.\footnote{We use the term ``functor node'',  for brevity and to avoid confusion with the statistical sense of ``parametrized'', meaning that values have been assigned to parameters.} 
%We assume that the variables $X_{i}$ are distinct. 
If a functor node $\functor(\terms)$ contains no variable, it is \textbf{ground node}. An assignment to a ground node of the form $\functor(\terms) = a$, where $a$ is a constant in the range of $\functor$, is a \textbf{ground atom}. A \textbf{population} is a set of individuals, corresponding to a domain or type in logic. Each first-order variable $\X$ is associated with a population.
 %$\population_{\X}$ of size $|\population_{\X}|$. 
 An \textbf{instantiation} or \textbf{grounding} 
 %$\gamma$ 
 for a set of variables $\X_{1},\ldots,\X_{k}$ assigns to each variable $\X_{i}$ a constant 
 %$\gamma(\X_{i})$ 
 from the population of $\X_{i}$.
  %The functor formalism is rich enough to represent the constraints of an entity-relationship (E-R) schema \cite{Ullman1982} via the following translation: Entity sets correspond to populations, descriptive attributes to functions, relationship tables to predicates, and foreign key constraints to type constraints on the arguments of relationship predicates. 
%in the E-R format~\cite{Domingos2009} 
%and the ground atoms in functor notation. 
%The results in this paper extend to functors built with nested functors, aggregate functions \cite{Klug1982}, and quantifiers; for the sake of notational simplicity we do not consider more complex functors explicitly. 
 %corresponding functor formalism.  
% A \textbf{table join} of two or more tables contains the rows in the Cartesian products of the tables whose values match on common fields. In logical terms, a join corresponds to a conjunction \cite{Ullman1982}.


\begin{figure}[htbp]
\begin{center}
%\resizebox{0.5\textwidth}{!}{
\includegraphics{database}
%\includegraphics[width=1\textwidth]{database.png}
%}
\caption{A simple relational database instance.
%that are true in the database. 
\label{fig:db-tables}}
\end{center}
\end{figure}


%We assume that a database instance (interpretation) assigns a constant value to each gnode $\f(\set{a})$, which we denote by $
%[f(\set{a})]_{\D}$.
%%Thus a DB instance defines a truth value for each ground atom depending on whether the atom assigns the right function value to the ground functor term.
%The value of descriptive relationship attributes is well defined only for tuples that are linked by the relationship. For example, the value of $\it{grade}(\it{jack},\it{101})$ is not well defined in a university database if $\it{Registered}(\it{jack},\it{101})$  is false. In this case, we follow the approach of Schulte {\em et al.} \cite{Schulte2009c} and assign the descriptive attribute the special value $\bot$ for ``undefined''. Thus the atom $\it{grade}(\it{jack},\it{101}) = \bot$ is equivalent to the atom $\it{Registered}(\it{jack},\it{101}) = \false$. Fierens {\em et al.} \cite{Fierens2005} discuss other approaches to this issue. 


A \textbf{Parametrized} (Bayes, Markov, Dependency) Network is a (Bayes, Markov, Dependency) Network whose
nodes are functor nodes.  We usually omit the prefix ``Parametrized''. 
A \textbf{ground} graph is derived from a database and a network by instantiating the adjacencies in the Parametrized network with all possible groundings. Figure~\ref{fig:db-tables} shows a simple database instance. 
%A \textbf{ground} PBN $\ground{B}$ is a directed graph derived from $\B$ by instantiating the variables in the functor nodes in $\B$ with all possible constants. 
%Figure~\ref{fig:pbn} illustrates a Parametrized Bayes Net for the dataset in Figure \ref{fig:db-tables} and its grounding. 
A database instance specifies a unique value for each ground node; we denote such a joint assignment by $\set{V} = \set{v}$. We use the following notation. 
\begin{itemize}
\item$\family_{ijk}$ is the \textbf{family state} that expresses that functor node $f_{i}$ is assigned its $k$-th value, and the state of its parents is assigned its $j$-th value.
\item $\instances_{ijk}(\set{V} = \set{v})$ is the number of groundings of $\family_{ijk}$ that evaluate as true for a given complete assignment of values (database).
\end{itemize}


%A \textbf{level mapping} assigns to each functor $\f$ in a PBN $\B$ a nonnegative integer $\level(\f)$. $\B$ is \textbf{stratified} if there is a level mapping such that for every edge $\f(\set{\X}) \rightarrow \g(\set{\Y})$, we have $\level(\f) \leq  \level(\g)$. 

\begin{figure}[hbpt]
\begin{center}
%\resizebox{1\textwidth}{!}{
\includegraphics{graph-examples}
%\includegraphics[width=2 \textwidth]{combine1.jpg}
%}
\caption{%A simple relational database instance. 
Left: A Parametrized Bayes Net with some CP-table entries. $x = \{\it{hi}, \it{lo}\}$.  Right: A Parametrized Markov Net, obtained by moralization. 
 \label{fig:pbn}}
\end{center}
\end{figure}

Recursive dependencies (autocorrelations) are represented in a PBN by ``copies'' of the functors. Thus the structure $\it{gender}(\X) \rightarrow \it{gender}(\Y) \leftarrow \it{Friend}(\X,\Y)$ represents an association between the gender of a user and that of his/her friends. In this case we assume that the Bayes net is in main functor format \cite{Schulte2011b}: for each functor $\functor$, there is a main functor node that is the only node with $\functor$ with parents. In the example, $\it{gender}(\Y)$ is the main functor, and $\it{gender}(\X)$ is an auxilliary functor used only for representing the recursive dependency. 
%The intuition is that since different functor nodes with the same functor are statistically interchangeable, it suffices to select one for modelling the conditional distribution. 
While the existence of a main functor may seem like a strong assumption, Schulte {\em et al.} show that under a mild ordering condition on the BN structure, for every Parametrized BN $\B$ not in main functor format, there is an equivalent main functor Bayes net $\B'$ that has the same ground graph \cite{Schulte2011b}.
  
\paragraph{Model Conversions.} \label{sec:conversions}
Bayes nets can be converted to Markov nets through the standard \textbf{moralization} method: connect all spouses that share a common child, and make all edges in the resulting graph undirected \cite{Domingos2009}. Thus each family in the Bayes net becomes a clique in the moralized structure. For each state of each family clique, we define the clique potential in the Markov net to be the conditional probability of the child given its parents. 
%The resulting Markov net defines the same joint probability over assignments of values to the nodes as the original Bayes net. 
If $M(\B)$ is a Parametrized Markov net obtained from PBN $B$, the unnormalized likelihood function for the ground graph of $M(\B)$ \cite{Domingos2009} is given by

\begin{equation}\label{eq:mbn-likelihood}
    P_{\M(\B)}(\set{V} = \set{v}) = exp\left(\sum_{ijk} \instances_{ijk}(\set{V} = \set{v}) \cdot ln(\theta_{ijk})\right).
\end{equation}

%This is because each ground parent-child instance corresponds to a conditional probability $\theta_{ijk}$, viewed as a clique potential, so 
%Thus the likelihood is proportional to the product of all conditional probabilities. 
In terms of the Bayes net parameters, Equation~\eqref{eq:mbn-likelihood} is simply the product of all conditional probabilities defined by a ground child-parent instance.
%
Bayes nets can also be converted to dependency nets \cite{Heckerman2000}. First, for each node $\X_{i}$, add a directed edge $\X_{j} \rightarrow \X_{i}$ from each node $\X_{j}$ in the Markov blanket of $\X_{i}$. The resulting graph is the same as the moralized Bayes net structure but with bidrected rather than undirected adjacencies. The conditional probability parameters are  given by the Markov blanket equation~\eqref{eq:bn-mb}.
%A \textbf{Markov Logic Network} (MLN) is a finite set of 1st-order formulas or clauses $\{\formula_{i}\}$, where each formula $\formula_{i}$ is assigned a weight. 
%A Markov Logic Network can be viewed as a specification of a Markov network using logical syntax \cite{Domingos2009}. Given an MLN and a database $\D$, let $n_{i}(\D)$ be the number of groundings that satisfy $\formula_{i}$ in $\D$.
%%An MLN assigns a log-likelihood to a database according to the equation
%%
%%\begin{equation}\label{eq:log-linear}
%%ln(P(\D)) = \sum_{i} w_{i} n_{i}(\D) - ln(\Z)
%%\end{equation}
%%where $\Z$ is a normalization constant.
%%
%%Thus the log-likelihood is a weighted sum of the number of groundings for each clause. 
%
%Functor Markov Nets have a simple representation as Markov Logic Networks as follows. For each assignment of values to a clique of functor nodes, add a conjunctive formula to the MLN that specifies that assignment. The weight of this formula is the logarithm of the clique potential. For any Functor Markov net,  the MLN likelihood function defined by Equation~\eqref{eq:log-linear} for the corresponding MLN is exactly the Markov net likelihood defined by grounding the Functor Markov net. {\em Therefore we can use MLN inference to carry out inference for Functor Markov Nets.}

\section{Parameter Space: the $w_{i}$}

The characteristic feature of a Bayes net log-linear model is that a weight $w_{i}$ is a log-conditional probability of a child node values given an assignment of values to its parents. While in relational data, the restriction to conditional probabilities incurs a loss of expressive power, 
%, in that they cannot all probability distributions over relational structures that general log-linear models can represent. 
%For instance, in general rescaling weights and rescaling predictor variables defines equivalent log-linear models, but rescaling conditional log-probability violates the axioms of probability. Despite the loss of modelling power, 
we believe that conditional probability parameters are well motivated by the following advantages for interpretability and scalability.

%Log-linear models are a prominent model class in machine learning that subsumes both directed and undirected graphical models [cite Bishop]. They are widely used for relational data, both in discriminative and generative learning [cite]. In a relational log-linear model, the predictive features are defined by counts of relevant relational patterns defined by the model. For instance, to predict the intelligence of a student, the model may consider how many A grades they have received, how many B grades, etc. The log-linear Bayes net models that we examine uses the same type of predictors as a relational Markov random field. Therefore, log-linear inference with cyclic dependencies is handled in the same way in both directed and undirected models. However, in the presence of cyclic dependencies the resulting likelihood is not normalized and therefore inferences require a normalization constant. The normalization constant in a relational Bayes net model is like the normalization constant for a Markov net model (partition function); in contrast, a nonrelational Bayes net model does not require an explicit normalization constant, because acyclicity guarantees normalization [cite]. Within the space of log-linear models, Bayes nets are distinguished by the fact that their parameters are of a special form, namely conditional probabilities of a child node value given an assignment of values to its parents. Using parameters that are restricted to conditional probabilities, or simple functions of them (i.e., logarithms), has advantages for interpretability and scalability.

\emph{Interpretability.} The weight/clique potential parameters of undirected models are often difficult to interpret for users \cite{Pearl1988}. This is especially the case when weights are learned from data, which can reflect complex interactions between weights assigned to different local cliques. In contrast, a Bayes net parameter can be interpreted as a conditional probability, and reflects local statistics restricted to a parent-child constellation.

\emph{Scalability.} Fast Bayes net parameter estimates can be obtained by using the observed conditional frequencies, which are defined by

\begin{equation} \label{eq:frequencies}
\widehat{\theta}_{ijk} = \frac{n_{ijk}(\set{V} = \set{v})}{\sum_{k}n_{ijk}(\set{V} = \set{v})}.
\end{equation}
%show that the observed frequencies maximize two natural objective functions: the unnormalized generative log-likelihood, and the unnormalized pseudo-likelihood (product of conditional probabilities). Normalization is necessary for inference, but not related to how well the model fits the fact in a database.
%In a general log-linear model, maximum likelihood estimation without the normalization constant would be an undefined problem, but if parameters are constrained to be conditional probabilities, no degeneracy results. 
Using frequency estimates can be viewed as a type of {\em lifted learning}, by which we mean using only the sufficient statistics in a relational database rather than an iteration over ground facts. The computational cost scales well in both the size of the data and the number of parameters in the model. 
%The learning time gains over local search methods for Markov models are substantial: in our experiments on benchmark databases, the Bayes net parameters are computed in seconds, but in the worst-case database the Alchemy system requires 10 hours to optimize weights.
% rather than facts about specific individuals. 
%
%Sophisticated methods have been developed for optimizing weights for a relational Markov model, especially for Markov Logic Networks [cite]. While these methods find parameters that lead to accurate predictions, they are local search methods in a high-dimensional parameter space and can be a significant computational bottleneck. For instance, Khosravi et al report that MLN parameter optimization on benchmark databases can take hours and in some cases days [cite]. In contrast, the Bayes net parameters can be estimated as the observed conditional frequencies in the input relational database, with optional smoothing methods. The computational cost is only that required for event counts (sufficient statistics) in the database, and therefore scales well both in the size of the database {\em and} the number of parameters in the model. Using frequency estimates can be viewed as a type of lifted learning, which uses only the sufficient statistics in a relational database rather than an iteration over ground facts.
%
%
%We show in experiments that using the observed conditional frequencies leads to accurate predictions. Moreover, these 

Fast conditional probability estimates can be combined with general weight learning in at least two important ways. (1) Conditional probabilities can be used to calculate {\em initial weights} for a local search procedure. (2) Several problems require {\em repeated parameter estimation}, for instance structure learning, or missing data imputation (e.g., via the EM algorithm). In this case fast parameter estimates can be used to quickly approach a solution, for instance a good structure. 
%We discuss a theoretical justification for the frequency estimates below, after we specify the predictor variables $x_{i}$ in the Bayes net log-linear model. %Search can then be used to fine-tune the weights. 
%
%\item {\em Structure Learning} often requires repeated parameter estimation, where weights are found for a candidate structure to assess its likelihood, then a new candidate stucture is explored etc. If evaluating candidate structures requires parameter optimization, a fast parameter estimation procedure is essential for scalable learning. Slower optimization methods can be used after structure learning has converged on a final model.
%\item Other common learning problems involve repeated parameter estimation as well, for instance an {\em expectation-maximization} approach to dealing with unobserved variables or missing data.
%\end{itemize}
%
%One of the reasons for the widespread popularity of Bayes nets for nonrelational data is that parameters have a natural interpretation and high-quality estimates can be obtained quickly. 
%We believe that providing users with a relational model class that has similar advantages will encourage applications of statistical-relational learning: Users have the option to carry out an initial data exploration and deploy more complex methods if the results are promising.

%\paragraph{Counts vs. Frequencies} For nonrelational data, restricting the parameter space to conditional probabilities does not entail a loss of expressive power, in the sense that for a graph structure that is adequate for the target generating distribution (an I-map), conditional probability settings can be found that represent the target distribution. An important difference for relational data is that conditional probabilities are restricted in expressive power, in that they cannot all probability distributions over relational structures that general log-linear models can represent. For example, consider rescaling the predictors in a regression model [cite] such that $x'_{i} := x_{i}/k_{i}$. If there is no constraint on the weights, the resulting model is equivalent to one with the original predictors but rescaled weights $w_{i}/k_{i}$. Thus unrestricted the weights can, in effect, change the scale of a predictor, whereas if weights are constrained to be log-probabilities, rescaling typically violates this constraint. While restricting the parameter space to conditional probabilities has the advantages of interpretability and scalability that we discussed, it means that the quality of predictions becomes more sensitive to the form of the model and the scale of the predictors. Researchers have observed previously that relational regression models feature highly unbalanced feature counts [cite Domingos, me]. For example, the number of courses a student has taken may be in the 10s, whereas the number of their friends in a social network may be in the 100s. Moreover, a single attribute of a student (e.g., gender) will be instantiated only once for each student, so the count of this feature is just 1. The effect is that although attributes of the target individual tend to be more informative than attributes of related individuals, the counts associated with related individuals are much higher.



\section{Predictor Space: the $x_{i}$}
%We specify the model, discuss its interpretation in graphical terms, then parameter estimation.
%We begin with the regression equation.
A standard way to define a log-linear model for a Bayes net is to convert it to a Markov network via moralization, and then use the log-linear model defined by the Markov network \cite{Domingos2009,Khosravi2010}. 
%. Converting a 1st-order Bayes net to a 1st-order Markov net (or MLN) via moralization (Section\ref{sec:conversions}) was suggested by Domingos and Richardson [cite], and is also the Bayes net conversion method recommended by the Alchemy group [cite]. 
%A standard way  to define 
%We define the resulting log-linear regression equation as  is to take the product of all factors that are groundings of the Markov blanket equation~\eqref{eq:bn-mb}.
%Since the moralized Bayes net $M(\B)$ is a function of $\B$, Equation~\eqref{eq:mbn-likelihood} defines the log-linear model likelihood model for the moralized Bayes net. 
The regression equation for this model is as follows. 
Let $\Y = \functor(\set{a})$ be a target ground node instantiating functor node $\functor(\set{A})$.
%\footnote{[If there are several nodes with functor $\functor$, we assume that there is only one node with $\functor$ that has parents, and use that node. Schulte {\em et al.} prove that under a mild ordering condition on the BN structure, for every PBN $\B$ there is an equivalent PBN $\B'$ that satisfies the unique node condition and has the same ground graph \cite{Schulte2011b}.]}
The \textbf{regression graph} for $\Y$ is the partially ground PBN $B_{Y}$ that results by substituting $a_{i}$ for $A_{i}$ in functor node $\functor(\set{A})$ and its Markov blanket. This is illustrated in Figure~\ref{fig:regress}. If there is more than one functor node with $\functor$, we use the main functor node (Sec.~\ref{sec:graph-relational}).
%For an assignment $\set{X} = \set{x}$ of values to all ground nodes other than $Y$, let $\instances_{ijk}(\set{X}=\set{x};\Y=\y)$ be the number of instantiations in the partially ground model that assign value $y$ to $Y$, and whose family state is $\family_{ijk}$. If family $\family_{ijk}$ does not contain functor node $\functor$,  $\instances_{ijk}(\set{X}=\set{x};\Y=\y) = 0$. If the PBN contains more than one node with functor $\functor$, the instances are counted with respect to grounding the main functor node. 
Using the notation from Section~\ref{sec:graph-relational} {\em with the regression graph}, the regression equation for target node $\Y$ is given by

\begin{equation} \label{eq:regress-count}
\tilde{P}(\Y = \y|\set{X}=\set{x}) = exp\left(\sum_{ijk} \instances_{ijk}(\set{X}=\set{x},\Y=\y) \cdot ln(\theta_{ijk}).\right)
\end{equation}

Here the summation is over the Markov blanket of the target node in the regression graph, that is, the index $i$ ranges over the target node and its children. Including irrelevant predictors in the regression leads to bad predictions, so statistical-relational models restrict edges in the ground model to relevant predictors only \cite{Poole2003}. In our examples and experiments below, we take the relevance conditions to be the existence of a link, such that for two entities $a,b$, there is a directed edge from a ground node representing an attribute of $a$ to a ground attribute of $b$, only if a link exists between the two entities. Figure~\ref{fig:regress} illustrates the resulting computation for predicting the intelligence of Bob given the database instance of Figure~\ref{fig:db-tables}.

%It can be shown that the normalized predicted probability $P(\Y = \y|\set{X}=\set{x})$ is a logistic regression equation [cite Kersting]. 

\begin{figure}
\begin{center}
%\resizebox{1\textwidth}{!}{
\includegraphics[width=1 \textwidth]{regression-example}
%\includegraphics[width=2 \textwidth]{combine1.jpg}
%}
\caption{%A simple relational database instance. 
Left: The regression graph for target $\it{intelligence}(bob)$. Right: The computation of the unnormalized Markov blanket probability, for the count model (top) and the frequency model (bottom). The example uses the conditional probability $P(d(C) = x| i(S) = x, Registered(S,C) = true) = 60\%$. The frequency model assigns higher weight to the ranking of $bob$.
 \label{fig:regress}}
\end{center}
\end{figure}

A problem with the count regression equation~\eqref{eq:regress} is that Markov blanket components with many groundings have exponentially more influence. 
%In a typical scenario, a descriptive attribute of a target entity corresponds to a single factor, whereas the attributes of related entities correspond to many, as many as there are related entities. For example, in the prediction of intelligence in Figure~\ref{fig:regress}, the factors due to courses quickly overwhelm the influence of rank as the number of courses increases. 
To balance the scales of the predictor variables, we use the frequency of a factor rather than its count as a predictor. 
In terms of factor products, this corresponds to using the geometric mean rather than the simple product. 
The corresponding regression equation is as follows. 
%Consider again the regression graph $B_{\Y}$ where $\functor(\set{A})$ has been replaced by $\functor(\set{a}) = \Y$. 
Let $m_{i}$ denote the number of possible groundings of family formula $\family_{ijk}$ in the regression graph. 
Note that $m_{i}$ does not depend on $j$ or $k$. 
%Then the frequency of the family constellation $\family_{ijk}$ is given by dividing the number of satisfying instantiations by $m_{i}$. 
The frequency regression equation is then given by

\begin{equation} \label{eq:regress-frequency}
\tilde{P}(\Y = \y|\set{X}=\set{x}) = exp\left(\sum_{ijk} \frac{\instances_{ijk}(\set{X}=\set{x},\Y = \y)}{m_{i}} \cdot ln(\theta_{ijk})\right).
\end{equation}

Figure~\ref{fig:regress} illustrates the computation. 
%Notice that the relative weight of the single grounding factor due to the ranking of Bob increases. 
We next establish a random selection interpretation of the frequency model.

\subsection{Random Selection Interpretation.}
The frequency regression value can be interpreted as an expectation over random instantiations of the Markov blanket as follows.

\begin{enumerate}
\item Let $\X_{1},\ldots,\X_{k}$ be a list of {\em all} 1st-order variables that occur in the Markov blanket of target node $\Y$ in the regression graph for $\Y$.
\item Select an instance (constant) $a_{i}$ from the population of variable $\X_{i}$, for each $i=1,\ldots,k$; the selections are random, independent, and uniform. Replace each node in the Markov blanket with the corresponding ground node.
\item Using the values assigned to the ground nodes in the database, apply the Markov blanket equation~\eqref{eq:bn-mb} to compute the factor product for $\Y$; this defines a log-sum for the random instantiation $\set{a_{i}}$. The expected value of this log-sum is the \textbf{random regression} value $ln(\tilde{P}^{r}(\Y = \y|\set{X}=\set{x}))$. 
\end{enumerate}



%For the relevance version, treat each context sentence as having disjoint variables from the others (e.g. R(X,Y), R1(X1,Y1)). Then you get random selections from contexts.

\begin{table}[htdp]
\caption{Computing the random regression for target node $\it{intelligence}(bob)$. Each course selection defines an instantiation of the Markov blanket of the target node with two associated factors. The random regression value -1.07 is the log-average. Notice that $-1.07=ln(0.34)$, the log of frequency regression (Figure~\ref{fig:regress}).}
\begin{center}
\resizebox{1\textwidth}{!}{
  \begin{tabular}{@{} |c|c|c|c| @{}}
    \hline
  Grounding & Factor 1 & Factor 2 & Log-Product \\
    \hline \hline
   $C=100$ & \begin{tabular}{@{} l @{}}
$P(i(bob) = lo|r(bob) = lo)$ \\
= .7\\ \end{tabular} & 
\begin{tabular}{@{} l @{}}
    $P(i(bob) = lo|diff(100) = lo, $ \\ 
     $R(bob,100) = T) = .6$\\ 
  \end{tabular} & $ln(.7\times.6)$ = -0.87 \\\hline
    C=200 & \begin{tabular}{@{} l @{}}
$P(i(bob) = lo|r(bob) = lo)$ \\
= .7\\ \end{tabular} & \begin{tabular}{@{} l @{}}
    $P(i(bob) = lo|diff(200) = hi, $ \\ 
     $R(bob,200) = T) = .4$\\ 
  \end{tabular} & $ln(.7*.4)=-1.27$ \\ \hline
  &  & Average & -1.07 \\ \hline
  \end{tabular}
}
\end{center}
\label{table:random-regress}
\end{table}%

%Grounding & Factor 1 & Factor 2 & Log-Product \\
%C=100 & P(i(bob) = lo|r(bob) = lo) = .7 & P(i(bob) = lo|diff(100) = lo, R(bob,100) = T) = .6 & -0.87 \\
%C=200 & P(i(bob) = lo|r(bob) = lo) = .7 & P(i(bob) = lo|diff(200) = hi, R(bob,200) = T) =.4 & -1.27 \\
% &  & Average & -1.07 \\



Random regression can be modified for relevance restrictions by selecting only relevant predictors, as the example computation in Table~\ref{table:random-regress} illustrates. In the example, frequency regression and random regression return the same value. The next proposition establishes this identity in general. Thus the frequency regression equation can be viewed as a closed form for computing the random regression value. We omit the proof due to space constraints. We remark that this result applies to any graphical model based on a 1st-order template, not only Bayes nets.
%that avoids an enumeration of all possible groundings of the Markov blanket.

\begin{proposition} \label{prop:randomize}
The frequency regression value  for a target node (Equation~\eqref{eq:regress-frequency}) equals the random regression value. 
%; in symbols, $P^{r} = P^{\pseudo}$.
\end{proposition}


\subsection{Graphical Interpretations.} \label{sec:interpret} Because recursive dependencies lead to cycles in the grounding of the Parametrized Bayes net, the regression equations cannot be derived from a ground Bayes net in general. However, the conversion approach (Section~\ref{sec:related}) offers an interpretation in terms of a Markov net resp. dependency net derived from the Bayes net.

{\em Count Model.}
The count regression formula~\eqref{eq:regress-count} can be derived from the generative likelihood Equation~\ref{eq:mbn-likelihood} for Markov networks 
%and is the regression formula for the Bayes net converted to an MLN 
\cite{Domingos2009}. In terms of the ground moralized Markov net, Equation~\eqref{eq:regress-count} is the product of all clique potentials for every clique containing the target node, hence the correct (unnormalized) regression equation for this model. The empirical frequencies $\theta_{ijk}$ are the maxima of the unnormalized generative likelihood equation for the moralized Bayes net. Normalization is necessary for inference, but not related to how well the model fits the fact in a database. We omit the proof due to space constraints. 

\begin{proposition} \label{prop:maximize}
Let $\B$ be a parametrized Bayes net and let $\set{V} = \set{v}$ denote a set of observations for a relational data structure that specify values for the nodes in the ground Bayes net. The empirical conditional frequencies (Equation~\ref{eq:frequencies})
%
%\begin{equation*}
%\widehat{\theta}_{ijk} = \frac{n_{ijk}(\set{V} = \set{v})}{\sum_{k}n_{ijk}(\set{V} = \set{v})}
%\end{equation*}
maximize the unnormalized likelihood (Equation~\ref{eq:mbn-likelihood}) for the moralized Bayes net.
\end{proposition}


{\em Frequency Model.} Although the frequency equation~\eqref{eq:regress-frequency} is very similar to the count equation, it does not have an interpretation in terms of a Markov random field. The reason is that the scaling factor $m_{i}$ depends on the target node. For instance, if the target node is the intelligence of a student, the scaling factor is the number of courses the student has taken. If the target node is the difficulty of the course, it is the number of students in the course. This means that the model does not use a single factor/potential for all queries, but instead scales the factor depending on the query. However, Equation~\ref{eq:regress-frequency} gives the Markov blanket conditional probabilities for all ground nodes, so it defines a dependency network whose structure is obtained by converting the Bayes net graph to a dependency graph (Sec.~\ref{sec:conversions}).
%, where the Bayes net parameters provide a factored representation of the dependency network parameters. %

There does not seem to be a closed form solution for the frequency-model likelihood maxima. In the experiments below, we use the observed conditional frequencies as with count regression. One view of this choice is that the frequencies are a heuristic, as used by \cite{Grossman2004} for discriminative BN learning. 
%%In analogy, \cite{Grossman2004} showed that a Bayes net classifier for nonrelational data performs well using observed frequencies as heuristic parameter settings. 
A more sophisticated view is that since a Bayes net is a generative model, the generative likelihood equation~\eqref{eq:mbn-likelihood} is the proper objective for parameter optimization. 
%The goal of a generative model is to answer arbitrary queries, including queries about the joint probabilities of a set of nodes, and marginal queries about unconditional probabilities. On this view, 
The scaling factors are not included in the objective function for {\em learning}, but are added to the {\em inference} model after parameters have been learned.

\emph{Inference.} For the count model, Markov logic network inference algorithms can be used after moralization, as in \cite{Khosravi2010,Schulte2012a}. For the frequency model, Heckerman et al. \cite{Heckerman2000} show that applying Gibbs sampling to a dependency network defines a stationary joint distribution, hence can be used to answer general queries. Their ordered pseudo-Gibbs sampler has been lifted to the relational setting \cite{Neville2007}. A Gibbs sampler for the frequency model would be able to exploit the ordering constraints provided by the Bayes net structure. Since the form of the frequency model is very similar to that of the count model, an alternative is to adapt MLN inference methods developed for the count model.


%\paragraph{Generative Interpretation.} A log-linear generative model for a Parametrized Bayes net $\B$ can be defined by applying the moralization method to $\B$. This conversion method was suggested by Domingos and Richardson [cite], and is also the Bayes net conversion method recommended by the Alchemy group [cite]. In terms of the ground moralized Markov net, Equation~\eqref{eq:regress-count} is the product of all clique potentials for every clique containing the target node, hence the correct (unnormalized) regression equation for this model. 

%\paragraph{Parameter Estimation.}
%We show that the empirical conditional frequencies maximize two natural objective functions for parameter learning in the count model. The first is the the unnormalized likelihood (Equation~\ref{eq:mbn-likelihood}). Another
%widely used pseudo-likelihood measure for a Markov net is the product over all nodes, of the probability of the assigned node value conditional on the given values of its Markov blanket. Using the unnormalized Markov blanket factors leads to the following objective function:
%
%\begin{equation} \label{def:pseudo}
%P^{\pseudo} (\set{V} = \set{v}) = \prod_{\Y = \y(\set{v})} \tilde{P}(\Y = \y|\set{X} = \set{x})
%\end{equation}
%
%where $\y(\set{v})$ is the value of ground node $\Y$ in the data. The next proposition gives the closed form for the parameter setting that maximizes both objective functions. We omit the proof due to space constraints. %shows that both the unnormalized likelihood and the unnormalized pseudo-likelihood are maximized by using the emprical conditional frequencies.
%
%\begin{proposition} \label{prop:maximize}
%Let $\B$ be a parametrized Bayes net and let $\set{V} = \set{v}$ denote a set of observations for a relational data structure that specify values for the nodes in the ground Bayes net. The empirical conditional frequencies
%
%%\begin{equation*}
%%\widehat{\theta}_{ijk} = \frac{n_{ijk}(\set{V} = \set{v})}{\sum_{k}n_{ijk}(\set{V} = \set{v})}
%%\end{equation*}
%
%maximize both the unnormalized likelihood (Equation~\ref{eq:mbn-likelihood}) and the unnormalized pseudo-likelihood (Equation~\ref{def:pseudo}). 
%\end{proposition}

%\begin{proof}
%The unnormalized likelihood has the same form as the standard single-table Bayes net likelihood, so the standard argument for BN maximum likelihood estimation applies. To establish the claim for the pseudo-likelihood, we note that it can be written as $P^{\pseudo} (\set{V} = \set{v}) = exp(\sum_{ijk} \alpha_{i} \instances_{ijk}(\set{V} = \set{v}) \cdot ln(\theta_{ijk})$
%where $\alpha_{i}$ is the indegree of functor node $i$ + 1. This holds because a single conditional probability for a ground child-parent instance is multiplied $\alpha_{i}$ times in the pseudo-likelihood measure: once for evaluating the Markov blanket of the ground child, and once for evaluating the Markov blanket for each of its ground parents. Since $\alpha_{i}$ is independent of the CP-parameters, it does not affect the maximization analysis.
%\end{proof}
%
%\section{Log-linear Model with Frequencies}
%
%%\begin{proof}
%%Copy from SIAM (more or less).
%%\end{proof}
%
%\paragraph{Inference} In this paper we consider inference only with respect to predictions given the Markov blanket, which can be evaluated in closed-form. Heckerman et al. \cite{Heckerman2000} show that applying Gibbs sampling with the regression equations defines a stationary joint distribution, hence can be used to answer general queries. Their ordered pseudo-Gibbs sampler has been lifted to the relational setting \cite{Neville2007}. A Gibbs sampler for the frequency model would be able to exploit the ordering constraints provided by the Bayes net. Since the form of the frequency model is very similar to that of the count model, it should be straightforward to adapt MLN inference methods developed for the count model.
%
%
%\paragraph{Parameter Estimation.} The unnormalized pseudo-likelihood~\ref{def:pseudo} can be defined for frequency regression just as with count regression. However, there does not seem to be a closed form solution for the frequency pseudo-likelihood maxima. In the experiments below, we use the observed conditional frequencies as with count regression. One view of this choice is that the frequencies are a simple heuristic, as used by \cite{Grossman2004} for discriminative learning in propositional data. 
%%In analogy, \cite{Grossman2004} showed that a Bayes net classifier for nonrelational data performs well using observed frequencies as heuristic parameter settings. 
%A more sophisticated view is that since a Bayes net model is a generative model, the generative likelihood (Eq. ~\eqref{eq:mbn-likelihood}) is the proper objective for parameter optimization. The goal of a generative model is to answer arbitrary queries, including queries about the joint probabilities of a set of nodes, and marginal queries about unconditional probabilities. On this view, the scaling factors are not included in the objective function for {\em learning}, but are added to the {\em inference} model to improve inferences after parameters have been learned.
%%
%The generative version of random likelihood function \cite{Schulte2011} has a closed form very similar to the Markov likelihood~\eqref{eq:mbn-likelihood}, and is also maximized by the empirical frequencies. 
%%: consider  a random instantiation of all nodes in the Bayes net, and take the expected log-likelihood as a measure of data fit. 
%%Schulte provides a closed form for the expected value of the random log-likelihood, which is the likelihood equation~\ref{eq:mbn-likelihood} with counts replaced by frequencies. The empirical conditional frequencies maximize the generative random selection likelihood. 
%%Thus the generative application of the random instantiation idea supports the use of conditional frequencies, and the frequency regression model can be viewed as the discriminative counterpart of the random selection idea to provide a predictive equation.

\section{Evaluation}

We first discuss the datasets used, then the systems compared, finally the comparison metrics.
% and the comparison results.
%\subsection{Datasets}
We used %one synthetic and 
5 benchmark real-world databases.   
%The databases are fairly complex, so the experiments are computationally demanding, especially the Alchemy inference component, which needs to be applied to all groundings of all descriptive attributes to compute average predictive performance. The databases and their main characteristics are as follows. 
For more details please see the references in \cite{Khosravi2010} and on-line sources such as \cite{bib:jbnsite}.
%In this paper we report the average result over all subdatabases in this paper and leave the evaluation of how models should evolve based on the size of data to an extension of the work in a journal paper. 


%{\em University Database.} We manually created a small dataset, based on the schema given in Table~\ref{table:university-schema}.
%The dataset is small and is used as a toy example for testing purposes. There are three entity tables, Student, Course, Professor, and 2 relationship tables RA and Registered.
%The entity tables contain 38 students, 10 courses, and 6  Professors. The $\reg$ table has 92 rows and the $\it{RA}$ table has 25 rows. %This dataset is translated into 513 ground atoms.

{\em MovieLens Database.} This is a standard dataset from the UC Irvine machine learning repository. 
% \cite{Khosravi2010}.
%The schema for the dataset is shown in Table \ref{}.
%It contains two entity tables: $\it{User}$ and with 941 tuples and $\it{Item}$ with 1,682 tuples, and one relationship table $\it{Rated}$ with 80,000 ratings. The $\it{User}$ table has key field $\it{user\_id}$ and 3 descriptive attributes $\age, \it{gender}, \it{occupation}$. We discretized the attribute age into three bins with equal frequency. The table $\it{Item}$ represents information about the movies. It has 17 Boolean attributes that indicate the genres of a given movie. We performed a preliminary data analysis and omitted genres that have only weak correlations with the rating or user attributes, leaving a total of three genres.
%
%The full dataset contains 170,143 ground atoms and is too big for Alchemy to perform learning. We made small subsamples to make the experiments feasible. Subsampling 100 Users and 100 Items transforms to an Alchemy input file with 3,485 ground atoms. Structure learning with Alchemy takes around 30 min.
%Subsampling 300 Users and 300 Items transforms to an Alchemy input file with 27,134 ground atoms. Structure learning with Alchemy takes about 2 days to run.
%The full table with 100,000 ratings exceeded the memory limits of Tetrad, so we randomly picked 40\% of the ratings of the relationship table as input data.

{\em Mutagenesis Database.} This dataset is widely used in ILP research.
% \cite{Srinivasan1996}. %It contains 4 tables total to 15218 tuples. 
It contains information on Atoms, Molecules, and Bonds between them. We use the discretization of \cite{Khosravi2010}.
%
%Mutagenesis has two entity tables, $\it{Atom}$ with 3 descriptive attributes, and $\it{Mole}$, with 188 entries and 5 descriptive attributes, including two attributes that are discretized into ten values each (logp and lumo).
%% There are two relationships $\it{MoleAtom}$ indicating which atoms are parts of which molecules, and $\it{Bond}$ which relates two atoms and has 1 descriptive attribute. 
%The full dataset, with 35,973 ground atoms, crashed Alchemy with both structure  and parameter learning. A subsample with 5,017 ground atoms did not terminate for structure learning, but weight learning was feasible. The computational difficulties of Alchemy compared to the MovieLens dataset are  due to the high number of descriptive attributes.
%%another subsample with
%Representing a relationship between entities from the same table in a parametrized Bayes net requires using two or more variables associated with the same population (e.g., $\it{Bond}(\A_{1},\A_{2}))$.
%(Techreport 2009) describes a straightforward extension of Algorithm~\ref{alg:structure} for this case, which we applied to the Mutagenesis dataset.\footnote{Reference omitted for blind review.}
%We also tested our method on the Financial dataset with similar results, but omit a discussion due to space constraints.

{\em Hepatitis Database.} This data is a modified version of the PKDD02 Discovery Challenge database.
% \cite{Frank2007}. %, which includes removing tests with null values. 
The database contains information on the laboratory examinations of hepatitis B and C infected patients. 
%The examinations were realized between 1982 and 2001 on 771 patients. The data are organized in 7 tables (4 entity tables,  3 relationship tables and 16 descriptive attributes). They contain basic information about the patients, results of biopsy, information on interferon therapy, results of out-hospital examinations, results of in-hospital examinations. 


{\em Mondial Database.} 
%
%\textbf{Hassan: which version did you use? The full one from http://www.dbis.informatik.uni-goettingen.de/Mondial/mondial-ER.pdf or Bahareh's?} 
%
This dataset contains data from multiple geographical web data sources. We followed the modification of \cite{wangMondial}, and used a subset of the tables and features for fast inference. 
%Our dataset contains 4 entity tables, $\it{Country},\it{Continent},\it{Economy},\it{Government}$, where the latter three are related to Country by many-one relationships, and one relationship table $\it{Borders}$ that relates two countries.

%Table~\ref{table:datasetsize} lists the resulting full database  sizes in terms of total number of tuples and number of ground atoms, which is the input format for Alchemy. 
%\begin{table}[thbp] \centering
%%\scalebox{0.9}{
%\begin{tabular}[c]
%{|l|l|l|}\hline
% \textbf{Dataset} & \textbf{\#tuples} & \textbf{\#Ground atoms} \\\hline
%%University&171&513\\\hline
%Movielens &82623&170143\\\hline
%Mutagenesis &15218& 35973 \\\hline
%Hepatitis &12447&71597 \\\hline
%%Financial&&\\\hline
%Mondial & 814 & 3366\\\hline
%\end{tabular}
%%} % end scalebox
%\caption{Size of full datasets in total number of table tuples and ground atoms. Each descriptive attribute is represented as a separate function, so the number of ground atoms is larger than that of tuples.\label{table:datasetsize}}
%\end{table}

%\vspace{-10mm}

\emph{UW-CSE database.} This dataset lists facts about the Department of Computer Science and Engineering at the University of Washington (UW-CSE), such as entities (e.g., Student, Professor) and their relationships (i.e. AdvisedBy, Publication).
% \cite{Domingos2007}. 
%The total number of ground atoms is 4,106,841. The database contained a total of 3380 ground atoms. 
The dataset was obtained  by crawling pages in the department's Web site (www.cs.washington.edu). 
%Publications and AuthorOf relations were extracted from the BibServ database (www.bibserv.org). 


\subsection{Performance Metrics.}
We use 3 performance metrics: %measures: goo	ggg
Learning Time, Accuracy (ACC), and Conditional log likelihood (CLL). ACC and CLL have been used in previous studies of MLN learning  \cite{Kok2009,Khosravi2010}. The CLL of a ground atom in a database is given by the log of the regression equation; for a database we report the average CLL over all atoms in the test set. To define accuracy, we apply inference to predict the probability of an attribute value, and score the prediction as correct if the most probable value is the true one. For ACC and CLL the values we report are averages over all predicates that represent descriptive attributes.
%We do not use Area Under Curve(AUC) as it is mostly used for binary predicates.
%The AUC curves were computed by changing the CLL threshold above which a ground atom is predicted true (10 thresholds were used).
We do not use Area under Curve, as it mainly applies to binary values, and most of the attributes in our dataset are nonbinary. 
%Like previous studies, we used the MC-SAT inference algorithm \cite{Poon2006} to compute a probability estimate for each possible value of a descriptive attribute for a given object or tuple of objects.  %In principle, our 
We evaluate the learning methods using 5-fold cross-validation as follows. We formed 5 subdatabases for each by randomly selecting entities from each entity table and restricting the relationship tuples in each subdatabase to those that involve only the selected entities  \cite{Khosravi2010}. The models were trained on 4 of the 5 subdatabases, then tested on the remaining fold. We report the  average score over the 5 runs, one for each fold. 

\subsection{Comparison Systems.}
All experiments were done on a QUAD CPU Q6700 with a 2.66GHz CPU and 8GB of RAM. Our code and datasets are available on the world-wide web \cite{bib:jbnsite}. We applied the learn-and-join algorithm to learn a Bayes net structure for each database, which is the start of the art structure learning algorithm for Parametrized Bayes Nets \cite{Khosravi2010}. A limitation of the current learn-and-join algorithm is that it learns a generative model over attributes given link structure, so our evaluation considers only queries whose target are attributes, not links  \cite{Khosravi2010,Schulte2011b}. 

Parameter learning for general weights proceeds in two steps as in \cite{Khosravi2010}: (1)  Convert the Parametrized Bayes nets to Markov Logic Networks, using moralization, which adds a conjunctive clause for each family state $\family_{ijk}$. We declared attribute predicates as functional as recommended by the Alchemy Group \cite{bib:bayes-convert}.
%A \textbf{Markov Logic Network} (MLN) is a finite set of 1st-order formulas or clauses $\{\formula_{i}\}$, where each formula $\formula_{i}$ is assigned a weight. A Markov Logic Network can be viewed as a specification of a Markov network using logical syntax \cite{Domingos2009}. 
%Parametrized Markov Nets have a simple representation as Markov Logic Networks as follows. For each assignment of values to a clique of functor nodes, add a conjunctive formula to the MLN that specifies that assignment. The weight of this formula is the logarithm of the clique potential. By converting PMNs to MLNs, we can use MLN inference to carry out inference for PMNs.
(2) The moralized BN Equation~\ref{eq:mbn-likelihood} is the likelihood function for the MLN, but with general weights $w_{ijk}$ in place of $ln(\theta_{ijk})$. To learn the $w_{ijk}$, we applied the default weight training procedure \cite{Lowd2007} of the Alchemy package \cite{Kok2009a}. 

Inference is performed by evaluating the count resp. frequency regression equation. We employ exact inference rather than approximate inference (e.g., MC-SAT) to avoid conflating the impact of the inference model with the impact of the inference implementation. We conducted experiments with MC-SAT and the results were similar.
%
We compared the following four approaches.

%\textbf{MLN+ MLN}: We use Alchemy's default program (version x) for producing a parametrized Markov network.

\begin{description}

\item[MBN+Count] The Bayes net structure is converted to an MLN using moralization, weights learned using Alchemy \cite{Khosravi2010}.
%Weight learning is carried out with Alchemy, using the method of 
%%Kok and Domingos 
%\citet{Kok2005a}. 
%This learning method is called MBN for ``Moralized Bayes Net'' by Khosravi {\em et al.} \cite{Khosravi2010}. 
Inference uses count regression.

\item[MBN+Frequency] Same as the previous method, but using frequency regression for inference. 
%Alchemy optimizes weights for the count model; we include this method to assess the impact of frequency scaling for weights other than log-conditional probabilities.

\item[CP+Count]  Parametrizes the Bayes net with the empirical conditional probabilities and uses count regression.

\item[CP+Frequency]  Parametrizes the Bayes net with the empirical conditional probabilities and uses frequency regression.
\end{description}

\subsection{Results.}
All results are averages from 5-fold cross validation, over all attributes in the database.
Table~\ref{table:accuracy} compares the accuracy score of the methods. Table~\ref{table:cll} compares the log-likelihood score of the methods.  We first discuss the comparison of the frequency vs. count models, and then the comparison of the CP models with the Markov net models. 

\begin{table}[htdp]
\caption{Accuracy score of the Bayes net  parameters (cp+), which are conditional probabilities, with the Markov net parameters (mbn+), which are general weights. Accuracy is the percentage of correctly predicted values in the test data.}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
Method& UW & Mondial & MovieLens & Mutagenesis & Hepatitis \\\hline
mbn+count & 80.25\% $\pm$ 0.05 & 43.81\% $\pm$ 0.04 & 59.71\% $\pm$ 0.02 & 61.49\% $\pm$ 0.02 & 51.01\% $\pm$ 0.02 \\
mbn+freq & 80.25\% $\pm$ 0.05 & 43.81\% $\pm$ 0.04 & 58.76\% $\pm$ 0.02 & 60.89\% $\pm$ 0.03 & 50.94\% $\pm$ 0.02 \\
cp+count & 80.89\% $\pm$ 0.06 & \textbf{44.70\%} $\pm$ 0.04 & 61.93\% $\pm$ 0.02 & 66.95\% $\pm$ 0.03 & \textbf{55.12\%} $\pm$ 0.02 \\
cp+freq & \textbf{81.01\%} $\pm$ 0.06 & 44.59\% $\pm$ 0.04 & \textbf{65.14\%} $\pm$ 0.01 & \textbf{66.96\%} $\pm$ 0.03 & 54.79\% $\pm$ 0.02 \\\hline
\end{tabular}

\end{center}
\label{table:accuracy}
\end{table}%


\begin{table}[htdp]
\caption{Conditional log-likelihood comparison of the Bayes net  parameters (cp+) with the Markov net parameters (mbn+).}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
Method & UW & Mondial & MovieLens & Mutagenesis & Hepatitis \\\hline
mbn+count & -0.44 $\pm$ 0.07 & \textbf{-1.28} $\pm$ 0.07 & -0.79 $\pm$ 0.03 & -0.91 $\pm$ 0.09 & -1.18 $\pm$ 0.26 \\
mbn+freq & -0.43 $\pm$ 0.07 & \textbf{-1.28} $\pm$ 0.07 & -0.83 $\pm$ 0.03 & -0.93 $\pm$ 0.13 & -1.16 $\pm$ 0.21 \\
cp+count & -0.42 $\pm$ 0.05 & -1.36 $\pm$ 0.11 & -1.10 $\pm$ 0.16 & -0.77 $\pm$ 0.03 & -1.20 $\pm$ 0.07 \\
cp+freq & \textbf{-0.41} $\pm$ 0.04 & -1.34 $\pm$ 0.09 & \textbf{-0.71} $\pm$ 0.01 & \textbf{-0.73} $\pm$ 0.04 & \textbf{-1.07} $\pm$ 0.10 \\\hline
\end{tabular}
\end{center}
\label{table:cll}
\end{table}%


\noindent \textbf{Frequency vs. Count Model.}

\emph{Accuracy.}  
The count and frequency models are close, except for MovieLens, where the frequency method has a 3\% advantage. MovieLens is an especially unbalanced set because the number of ratings varies from movie to movie and user to user.  %\marginpar{More details?} 
Also, there are generally many more users rating a given movie than movies rated by a given user. 
%For the Markov net methods, both count and frequency models have almost the same scores, which is evidence that the learned parameters include scaling components. 

\emph{CLL.} {\em Using frequencies rather than counts improves the conditional log-likelihood score for the CP model}, substantially on MovieLens and Hepatitis (by 0.4 resp. 0.13 log-likelihood units). Whereas accuracy is a 0-1 loss function, CLL is continuous, so we expect the balancing of factors to have more impact. 

There is little difference between the Markov model with counts and frequencies. We hypothesize that this is because the optimized Markov model weights include a scaling component. Figure~\ref{fig:boxplots} examines the scaling components of the weights directly. Every dataset shows scaling effects except for UW, where all methods achieve the same CLL score. The scaling effects are especially strong for MovieLens, where the Bayes net frequency model outperforms the count model the most.


\begin{figure}[htbp]
\begin{center}
%\resizebox{0.5\textwidth}{!}{
\includegraphics[width=1\textwidth]{boxplots}
%}
\caption{Boxplots of the absolute weight sizes learned by the Markov Logic Network method. The median weight size is shown for the set of clauses with one 1st-order variable (left), and the set of clauses with two 1st-order variables (right). Smaller weights for 2-variable clauses balance their larger number of groundings against the smaller number for one-variable clauses.
\label{fig:boxplots}}
\end{center}
\end{figure}

\noindent \textbf{Bayes net vs. Markov net parameters.} 

\emph{Accuracy.} 
The conditional probability parameters have a slightly higher score than the Markov net methods, with the biggest differences on MovieLens (5\%) and Hepatitis (4\%). 

\emph{CLL.}
The frequency model scores substantially better than the Markov net models on Mutagenesis, Hepatitis and MovieLens (by 0.18, 0.11, 0.08 units) but worse on Mondial (0.06 difference). 
%Overall our experiments provide good evidence that the frequency model using Bayes net conditional probabilities is competitive with a general Markov net log-linear model.


%The next section presents synthetic-data experiments that focus on the scaling effect. 

\emph{Learning Times.}
%\subsubsection{Learning Times.}
Table~\ref{table:learn-times} shows runtime results for parameter learning. We see clear scalability advantages for the maximum likelihood conditional probability estimates: they take seconds to compute, whereas the local search method requires as much as 10 hours in the worst case (Hepatitis).

\begin{table}[htdp]
\caption{A comparison of runtime (seconds) required for parameter learning with a fixed Bayes net structure. The Bayes net methods use the observed conditional frequencies. The Markov net methods use Alchemy's default weight learning. Database sizes are specified by the number of tuples and the number of ground atoms. 
%For the Markov net methods, the number of model parameters determines runtime more strongly than datasize.
}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Dataset & Bayes Net (s) & Markov Net (s)&\#tuples & \#Ground atoms &\#Parameters \\\hline
UW & 2 & 5 &2099&3380 & 125 \\
Mondial & 3 & 90 & 814 & 3366 & %2470 
575\\
MovieLens & 8 & 10800 &82623&170143 &327\\
Mutagenesis & 3 & 14400 &15218& 35973 &880\\
Hepatitis & 3 & 36000 &12447&71597 & 793\\\hline
\end{tabular}
\end{center}
\label{table:learn-times}
\end{table}%

%hline Database & Parameters & Complement & FMT & C/FMT \\  \hline \hline
%Mondial & 1618 &157 & 7 &22 \\\hline 
%Hepatitis & 1987 & 18,246 & 77 & 237 \\\hline
%Financial & 10926 & 228,114 & 14,821 &15 \\\hline 
%MovieLens & 326 & 2,070 & 50 & 41 \\ \hline




%Figure~\ref{fig:accuracy} provides a graphical summary. 


%\begin{figure}[htbp]
%\begin{center}
%%\resizebox{0.5\textwidth}{!}{
%\includegraphics{chart-accuracy}
%%\includegraphics[width=1\textwidth]{database.png}
%%}
%\caption{Graphical Summary of Accuracy Results. Results for each method are ordered as shown.%that are true in the database. 
%\label{fig:accuracy}}
%\end{center}
%\end{figure}


%Figure~\ref{fig:cll} provides a graphical summary. 


%\begin{figure}[htbp]
%\begin{center}
%%\resizebox{0.5\textwidth}{!}{
%\includegraphics{chart-cll}
%%\includegraphics[width=1\textwidth]{database.png}
%%}
%\caption{Graphical Summary of CLL Results. Results for each method are ordered as shown.%that are true in the database. 
%\label{fig:cll}}
%\end{center}
%\end{figure}
 
\paragraph{Experimental Conclusions.} Together with our theoretical analysis, the empirical findings make a strong case for recommending the frequency model over the count model when the CP parameters are used. The performance of the CP frequency model compared to general log-linear weight learning is quite impressive across databases and attributes. 
%An application may feature a database where, with sufficient computational resources, weight optimization can exploit the greater flexibility of unconstrained weights. The Mondial database is an example in our experiments. Nonetheless, 
Therefore, log-linear models derived from Bayes nets appear to offer a good baseline model within the class of log-linear relational models.  One of the reasons for the widespread popularity of Bayes nets for nonrelational data is that parameters have a natural interpretation and high-quality estimates can be obtained quickly. 
We believe that providing users with a relational model class that has similar advantages will encourage applications of statistical-relational learning. Users have the option to carry out an initial data exploration and deploy more complex methods if the results are promising.


%\subsection{Synthetic Dataset}
%Scaling factors not so unbalanced. Averages over several predicates. Also has to do with attributes included in Markov blanket. Conduct experiment with synthetic data for the running example.

\section{Conclusion and Future Work} This paper considered log-linear inference models for applying Bayes nets to relational data. The characteristic feature of such models is that the weight parameters are log-conditional probabilities of parents given children. The predictor variables in previous relational log-linear models are instance counts of relational patterns. We provided theoretical considerations and empirical evidence that for conditional probability parameters, it is important to rescale the predictors to be instance frequencies. The frequency model can be interpreted as the expected log-linear regression value from a random instantiation of a node's Markov blanket. Using the maximum likelihood values as Bayes net parameters is much faster than optimizing weights using standard Markov Logic methods, typically seconds vs. hours. The predictive performance of log-conditional probability weights is competitive, on several datasets it was in fact superior. 

Parametrized Bayes nets have been extended with decision trees to obtain more compact models of the conditional distribution of a child node given its parents \cite{Khosravi2012}. This is a natural extension for testing the frequency model; we hypothesize that frequency scaling is even more important, because decision tree pruning leads to more variation in clause length. The powerful and effective technique of functional gradient boosting \cite{Khot2011} could be applied to learning tree models that augment a Bayes net; gradient boosting is well-suited to learning potential functions for log-linear models. 
In sum, among graphical relational models, log-linear models based on Bayes nets offer an attractive trade-off between expressiveness vs. interpretability and scalability.

\bibliographystyle{splncs}
\bibliography{master}

\end{document}